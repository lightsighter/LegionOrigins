
\section{Programming Interface}
\label{sec:interface}
The clients for our interface are higher-level language
compilers and runtimes such as Legion\cite{Legion12} as well as 
advanced systems programmers.  These clients expect total control
over the underlying hardware and transparent performance from an
implementation.  To support these demands we have designed our interface
to be as low-level as possible.  In many cases we have chosen to trade-off
ease of programmability for performance.  We eschewed any features that would
automatically be performed without being directed by the client.
Our goal in the design of this interface is to provide a set of low-level,
high-performance primitives that are asynchronous and composable.

Our interface is shown in Figure~\ref{fig:runtimeapi} and organizes functionality into objects.  We describe
these objects in six parts: {\em events} for composing operations 
(Section~\ref{subsec:events}), {\em processors} for parallel computation 
(\ref{subsec:procs}), {\em barriers} for producer/consumer coordination (\ref{subsec:barriers}),
{\em deferred locks} for more general synchronization
(\ref{subsec:locks}), {\em physical regions} for data layout and movement
(\ref{subsec:phyreg}), and a {\em machine} object for introspection of the underlying
hardware (\ref{subsec:machmodel}).  In all cases (except for the
machine object which is a static singleton), instances of the objects
are light-weight {\em handles} that provide a unique name for an underlying
implementation object.
Every handle is valid everywhere in the system, allowing handles to be freely copied,
passed as arguments to other tasks, or stored in the heap
without having to reason about the distributed nature of the system.
%%   Multiple copies of handles are permitted and handles can be 
%% passed by value.  Furthermore, every handle is valid everywhere in the system.  
%% This property allows a client to pass handles by value between computations 
%% without having to reason about the distributed nature of the system.
%We describe how we support this property in more detail in 
%Section~\ref{sec:impl}.

\lstset{
  captionpos=b,
  language=C++,
  basicstyle=\scriptsize,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  stepnumber=1,
  escapechar=\#,
  keepspaces=true,
  belowskip=-10pt,
  literate={<}{{$\langle$}}1 {>}{{$\rangle$}}1,
  %morekeywords={region,coloring,partition,spawn,disjoint,aliased},
  %deletekeywords=float,
}

\subsection{Events}
\label{subsec:events}
Events are the primary mechanism for describing dependences between operations in our system.
Lines 1-13 of Figure~\ref{fig:runtimeapi} show the interface for events.  An instance of the {\tt Event} type 
names a unique event in the system.  {\tt NO\_EVENT} (line 3)
is a special instance
of an event that by definition has always triggered.  The event interface
supports testing whether an event has triggered (line 5) and waiting on
an event to trigger (line 6), but the preferred
use of events is passing them as preconditions for other operations.  The client can use the
{\tt merge\_events} call (line 7) to create an aggregate event that triggers once every
event in a given set of events has triggered.  A code example using events follows in
Section~\ref{subsec:procs}.
%corresponding to the conjunction of a set of events.

In most cases events are created as the result of other
operations and the implementation is responsible for triggering these events.  Clients
can also create a {\tt UserEvent} (line 10) that is triggered explicitly by the client,
and may be used as precondition like any other event.

%Although user events and barriers are created and triggered differently,
%operations that express dependences on Events can transparently use
%UserEvents or Barriers as well.

%%   Users can also create another
%% type of event called a {\tt Barrier} that will require multiple arrivals before
%% the event is triggered.  The user can manage the number of arrivals that must
%% be seen as well as how many arrivals occur at a time.  Once a barrier's arrival
%% count goes to zero, the barrier will trigger.  Note that barriers in our interface
%% provide a superset of the functionality of traditional barriers since they can
%% be used in a blocking manner (via the {\tt wait} call), but are primarily used
%% asynchronously.  Lastly, since user events and barriers are sub-types of 
%% an event they can be used wherever an event is required.

\begin{figure}
\begin{lrbox}{\mylistingbox}
\begin{lstlisting}
class Event {
  const unsigned id, gen;
  static const Event NO_EVENT;

  bool has_triggered() const;
  void wait() const;
  static Event merge_events(const set<Event> &to_merge);
};

class UserEvent : public Event {
  static UserEvent create_user_event();
  void trigger() const;
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\
\vspace{-0.2cm}

\begin{lrbox}{\mylistingbox}
\begin{lstlisting}[firstnumber=14]
class Processor {
  const unsigned id;
  typedef unsigned TaskFuncID;
  typedef void (*TaskFuncPtr)(void *args,size_t arglen,Processor p);
  typedef map<TaskFuncID, TaskFuncPtr> TaskIDTable;

  enum Kind { CPU_PROC,GPU_PROC /* ... */ };
  Kind kind() const;

  Event spawn(TaskFuncID func_id,const void *args,size_t arglen, Event wait_on) const;
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\
\vspace{-0.2cm}

\begin{lrbox}{\mylistingbox}
\begin{lstlisting}[firstnumber=25]
class Barrier : public Event {
  static Barrier create_barrier(unsigned expected_arrivals);
  void alter_arrival_count(int delta) const;
  void arrive(unsigned count = 1, Event wait_for = NO_EVENT) const;
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\
\vspace{-0.2cm}

\begin{lrbox}{\mylistingbox}
\begin{lstlisting}[firstnumber=30]
class Lock {
  const unsigned id;
  Event lock(Event wait_on = NO_EVENT) const;
  void unlock(Event wait_on = NO_EVENT) const;

  static Lock create_lock(size_t payload_size = 0);
  void *payload_ptr();
  void destroy_lock();
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\
\vspace{-0.2cm}

\begin{lrbox}{\mylistingbox}
\begin{lstlisting}[firstnumber=39]
class Memory {
  const unsigned id;
  size_t size() const;
};

class PhysicalRegion {
  const unsigned id;
  static const PhysicalRegion NO_REGION;

  static PhysicalRegion create_region(size_t num_elmts, size_t elmt_size);
  void destroy_region() const;

  ptr_t alloc();
  void free(ptr_t p);

  RegionInstance create_instance(Memory memory) const;
  RegionInstance create_instance(Memory memory, ReductionOpID redopid) const;
  void destroy_instance(RegionInstance instance, Event wait_on = NO_EVENT) const;
};

class RegionInstance {
  const unsigned id;

  void *element_data_ptr(ptr_t p);
  Event copy_to(RegionInstance target, Event wait_on = NO_EVENT);
  Event reduce_to(RegionInstance target, ReductionOpID redopid, Event wait_on = NO_EVENT);
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\
\vspace{-0.2cm}

\begin{lrbox}{\mylistingbox}
\begin{lstlisting}[firstnumber=last]
class Machine {
  Machine(int *argc, char ***argv, const Processor::TaskIDTable &task_table);

  void run(Processor::TaskFuncID task_id, const void *args, size_t arglen);

  static Machine* get_machine(void);
  const set<Memory>& get_all_memories(void) const;
  const set<Processor>& get_all_processors(void) const;

  int get_proc_mem_affinity(vector<ProcMemAffinity> &result, ...);
  int get_mem_mem_affinity(vector<MemMemAffinity> &result, ...);
};
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\

\caption{Runtime Interface.\label{fig:runtimeapi}}
\vspace{-4mm}
\end{figure}

\subsection{Processors}
\label{subsec:procs}
Lines 14-24 of Figure~\ref{fig:runtimeapi} show the interface for {\tt Processor} objects which allow for
the creation of parallel computations.  Processors provide a way of naming 
every computational unit in the machine.  We describe how to discover processor handles
in Section~\ref{subsec:machmodel}.  In the current interface 
processors are either individual CPU cores or discrete GPUs, but this is easily extended
to include new processor types (line 20).   Processors support a single {\tt spawn}
method (line 23) that launches a new {\em task} on that processor.
Note that because the spawn operation
is invoked on a processor handle, a task can be launched on a 
processor from anywhere in the system.  
The spawn call takes an optional event that must trigger before the task begins and
always returns an event that triggers when the task completes.

Figure~\ref{fig:procevents} shows an example using 
the processor and event interface.  The {\tt diamond\_task} launches four
sub-tasks with a diamond dependence pattern.
The {\tt merge\_events} call
merges the events {\tt eb} and {\tt ec} to create the shared dependence
for the last sub-task.

\begin{figure}
\begin{lrbox}{\mylistingbox}
\begin{lstlisting}
void diamond_task(const void *args,size_t arglen,Processor p){
  Event ea = p.spawn(TASK_A,NULL,0);
  Event eb = p.spawn(TASK_B,NULL,0,ea);
  Event ec = p.spawn(TASK_C,NULL,0,ea);
  Event em = Event::merge_events(eb,ec);
  Event ed = p.spawn(TASK_D,NULL,0,em);
}
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}}\\

\centering
\scalebox{0.8}{
\begin{tikzpicture}
  %\path (1,3) node (t1) [shape=rectangle,draw] {run TASK\_A} -- node [right=1pt] {ea} +(0,-0.5) node[auto] {eax};
  \node (t1) at (-0.2,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\TASK\_A\end{tabular}};
  \draw (t1) to node[auto] {ea} (1.2,0);

  \node (t2) at (2.8,1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\TASK\_B\end{tabular}};
  \draw (t2) to node[auto] {eb} (4.2,1);
  \draw [->] (1.2,0) to (1.6,1) to (t2.west);

  \node (t3) at (2.8,-1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\TASK\_C\end{tabular}};
  \draw (t3) to node[auto] {ec} (4.2,-1);
  \draw [->] (1.2,0) to (1.6,-1) to (t3.west);

  \node (t4j) at (4.6,0) [shape=circle,inner sep=2pt,draw] {+};
  \draw (t4j) to node[auto] {em} (5.4,0);
  \draw [->] (4.2,1) to (t4j);
  \draw [->] (4.2,-1) to (t4j);

  \node (t4) at (6.4,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\TASK\_D\end{tabular}};
  \draw [->] (t4) to node[auto] {ed} (7.9,0);
  \draw [->] (5.4,0) to (t4.west);
  %
\end{tikzpicture}
}
%\vspace{-2mm}
\caption{Processor Example and Event Graph.\label{fig:procevents}}
\vspace{-4mm}
\end{figure}

\subsection{Barriers}
\label{subsec:barriers}
The common synchronization pattern in which multiple consumers are dependent on
a set of producers is supported by a {\tt Barrier} object (lines 25-29 of Figure~\ref{fig:runtimeapi}), which is similar
to a user event, but does not trigger until the expected number of {\tt arrive}
operations have occurred.  To better support
composability and asynchronous operation, our interface differs from barriers in other programming
models\cite{MPI} in three fundamental ways:
%The common synchronization pattern in which multiple consumers are dependent on a 
%(potentially different) set of producers is supported by a {\tt Barrier} object,
%which is similar to a {\tt UserEvent}, but does not trigger until the expected
%number of {\tt arrive} operations have occurred.  Barrier constructs exist in other
%programming models\cite{MPI}, but in addition to the decoupling of the waiters from
%the arrivers, our barrier interface has two further improvements to better support
%composability and asynchronous operation:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
\item The set of producers arriving at the barrier can be different from the set of consumers waiting on the barrier.
\item An arrival operation can be made dependent on another event, allowing a client to
express that an operation should be completed before the barrier can trigger.
\item The expected arrival count of a barrier can be dynamically modified, which we
have found useful in supporting nested parallelism in higher-level languages.
%allowing
%producer tasks to be subdivided as necessary, even after the barrier has been created.
\end{itemize}

Like user events, barriers are a sub-type of events and can therefore
be used transparently to express dependences wherever events are used.

\subsection{Deferred Locks}
\label{subsec:locks}

Events and barriers express ordering properties between operations, but 
%in many cases 
ordering is often too strict a requirement.  For many applications access to data need only be atomic and
not necessarily ordered.  Traditionally locks have been used to serialize access to
data.  However, lock implementations in existing parallel programming models require either blocking
or spinning, neither of which composes well with asynchronous operations.
{\em Deferred locks} are a new synchronization mechanism that allows for synchronization
without blocking or spinning.
%in an completely non-blocking environment.  

\begin{figure}
\begin{lrbox}{\mylistingbox}
\begin{lstlisting}
void lock_example(const void *args, size_t arglen, Processor p) {
  // Unpack lock, input events, target processors from arguments
  Lock mutex = Lock::create_lock(); 
  // Acquire lock, launch first subtask, release lock
  Event lock_event_1 = mutex.lock(prev_event_1);
  Event task_event_1 = proc1.spawn(SUB, args_1, sizeof(args_1), lock_event_1);
  mutex.unlock(task_event_1);
  // same for second subtask
  Event lock_event_2 = mutex.lock(prev_event_2);
  Event task_event_2 = proc2.spawn(SUB, args_2, sizeof(args_2), lock_event_2);
  mutex.unlock(task_event_2);
}
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}}\\

\centering
\scalebox{0.8}{
\begin{tikzpicture}
  \node (l1) at (2,1.95) [shape=rectangle,draw] {\begin{tabular}{c}{\bf lock}\\mutex\end{tabular}};
  \node (t1) at (4.5,1.95) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\SUB(args\_1)\end{tabular}};
  \node (u1) at (7,1.95) [shape=rectangle,draw] {\begin{tabular}{c}{\bf unlock}\\mutex\end{tabular}};

  \node[auto] (x1) at (-0.3,2.15) {prev\_event\_1};
  %\draw [->] (x1) to (l1.west);
  \draw [->] (0,1.95) to (l1.west);
  \draw [->] (l1.east) to (t1.west);
  \draw [->] (t1.east) to (u1.west);

  \node (l2) at (2,0.25) [shape=rectangle,draw] {\begin{tabular}{c}{\bf lock}\\mutex\end{tabular}};
  \node (t2) at (4.5,0.25) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\SUB(args\_2)\end{tabular}};
  \node (u2) at (7,0.25) [shape=rectangle,draw] {\begin{tabular}{c}{\bf unlock}\\mutex\end{tabular}};

  \node[auto] (x2) at (-0.3,0.45) {prev\_event\_2};
  \draw [->] (0,0.25) to (l2.west);
  \draw [->] (l2.east) to (t2.west);
  \draw [->] (t2.east) to (u2.west);

  \draw [->,dotted,thick] (0.5,2.5) to (l2.140);
  \draw [->,dotted,thick] (u2.40) to (l1.220);
  \draw [->,dotted,thick] (u1.320) to (8.5,-0.5);

  %% \draw (1.0,1.5) rectangle (6.2,2.4);
  %% \node (l1) at (3.6,1.95) {\begin{tabular}{c}{\bf lock}\\needed\end{tabular}};

  %% \draw (6.7,1.5) rectangle (8.6,2.4);
  %% \node (t1) at (7.65,1.95) {\begin{tabular}{c}{\bf run}\\SUB(args\_1)\end{tabular}};

  %% \draw (9.1,1.5) rectangle (10.5,2.4);
  %% \node (u1) at (9.8,1.95) {\begin{tabular}{c}{\bf unlock}\\needed\end{tabular}};

  %% \draw [->] (0.0,1.95) to (1.0,1.95);
  %% \draw [->] (6.2,1.95) to (6.7,1.95);
  %% \draw [->] (8.6,1.95) to (9.1,1.95);

  %% \draw (0.3,0.1) rectangle (1.7,1.0);
  %% \node (l2) at (1.0,0.55) {\begin{tabular}{c}{\bf lock}\\needed\end{tabular}};

  %% \draw (2.2,0.1) rectangle (4.1,1.0);
  %% \node (t2) at (3.15,0.55) {\begin{tabular}{c}{\bf run}\\SUB(args\_2)\end{tabular}};

  %% \draw (4.6,0.1) rectangle (6.0,1.0);
  %% \node (u2) at (5.3,0.55) {\begin{tabular}{c}{\bf unlock}\\needed\end{tabular}};

  %% \draw [->] (0.0,0.55) to (0.3,0.55);
  %% \draw [->] (1.7,0.55) to (2.2,0.55);
  %% \draw [->] (4.1,0.55) to (4.6,0.55);

  %% \draw [->,dotted,thick] (0.2,2.5) to (0.5,1.0);
  %% \draw [->,dotted,thick] (5.8,1.0) to (6.0,1.5);
  %% \draw [->,dotted,thick] (10.3,1.5) to (10.6,0.0);


  %\path (1,3) node (t1) [shape=rectangle,draw] {run TASK\_A} -- node [right=1pt] {ea} +(0,-0.5) node[auto] {eax};
  %% \node (t1) at (0,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf lock}\\needed\end{tabular}};
  %% \draw (t1) to node[auto] {lock\_event} (2.4,0);

  %% \node (t2) at (3.2,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\SUB\end{tabular}};
  %% \draw (t2) to node[auto] {task\_event} (5.4,0);
  %% \draw [->] (2.4,0) to (t2.west);

  %% \node (t3) at (6.4,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf unlock}\\needed\end{tabular}};
  %% \draw [->] (5.4,0) to (t3.west);

  %% \draw [->,dotted,thick] (-1,1) to (t1.135);
  %% \draw [->,dotted,thick] (t3.45) to (7.4,1);
\end{tikzpicture}
}
%\vspace{-2mm}
\caption{Lock Example and Event Graph.\label{fig:lockevents}}
\vspace{-4mm}
\end{figure}

The interface for deferred locks is shown in lines 30-38 of Figure~\ref{fig:runtimeapi}.
Unlike traditional blocking locks, the {\tt lock} method (line 32) doesn't
wait if the lock is in use but instead always returns immediately with an event that will be triggered
when the lock has been acquired.  Like a processor's spawn method, both the {\tt lock} 
and {\tt unlock} methods take an optional event parameter as a precondition (lines 32-33).

An important difference between deferred locks and blocking locks is that the processor
that requests the lock doesn't have to be the one that uses it.  A common
convention in writing code with deferred locks is to acquire locks on behalf of a task being
launched.  Figure~\ref{fig:lockevents} illustrates this with a simple example.  The {\tt lock\_example}
function needs to launch two subtasks on two other processors, each with exclusive access to some resource protected by a
lock named {\tt mutex} (created on line 3).  For each subtask, it first requests the lock on the subtask's behalf.  Using the
event returned by the lock request as a precondition, it then spawns the subtask.  The event dependence guarantees that the runtime will 
not start the subtask until the lock has been acquired.  The parent task also requests that the
runtime automatically release the lock after each subtask has completed by using the subtask's completion
event as a precondition for the unlock.  In the event dependency graph (Figure~\ref{fig:lockevents}),
solid arrows represent
the explicit events used in the code, while the dotted line represents the dynamic ordering that results
from the mutual exclusion guarantee of the lock.  (In this example, the second subtask is granted the lock first.)

When a lock is used to mediate access to a piece of data, the first thing a task will usually
do after being granted the lock is access the data.  To hide the latency of that
access as well, an additional feature of locks in our interface is the association of a lock with a
{\em payload}, a small (i.e. less than 4KB) 
piece of data that is moved with the lock and guaranteed to be coherent while 
the lock is held.  The size of the payload is specified when the lock is created (line 35 of Figure~\ref{fig:runtimeapi}) and
the pointer to the local copy of the payload can be obtained from the {\tt payload\_ptr} method
(line 36).

With deferred locks and barriers (see Section~\ref{subsec:events}) clients 
have the same synchronization primitives as
in threading and bulk-synchronous interfaces.  However, these operations can now 
be composed with other asynchronous operations which is not possible in any other interface
of which we are aware.

%Deferred locks provide a super-set of the functionality of blocking locks.  As their
%name suggests, deferred locks can use the event corresponding to their lock acquire
%operation to defer execution until the lock acquire has been granted.  Deferred
%locks can also be converted back into a blocking lock by immediately waiting on the event
%returned from a call to {\tt lock}.  In addition, deffered locks also provide {\tt mode} and
%and {\tt exclusive} parameters that allow the user to specify whether other requests can acquire
%the lock simultaneously.  A lock can only be acquired in one mode at a time.  The
%{\tt exclusive} parameter specifies whether other owners are permitted once the
%current lock request is granted.


\subsection{Physical Regions}
\label{subsec:phyreg}
%In distributed machines with discrete memories, data movement often consists of more
%than a simple copy operation.  Many applications perform operations and transformations on
%their data in conjunction with data movement for higher performance.  One common example is 
%that applications accumulate reduction operations in a buffer and then, as part of a copy operation,
%the reduction operations are applied to a destination buffer on the receiving side of the copy.  
%To support these kinds of conjoined operations in an asynchronous environment, our interface 
%must be aware of the structure of data so it can perform accompanying operations with 
%data movement operations.  To describe the structure of data our interface uses {\em physical regions.}
Current APIs for moving data in machines with discrete memories only support untyped
copies of a buffer of data and force the programmer to work with pointers in a variety of address
spaces.  This restriction requires programmers to manage both the layout of
data and the marshaling of data for communication.  {\em Physical regions} are a
mechanism for decoupling the placement of data from its layout.  Physical regions allow portability
of ``pointers'' as data is moved around the memory hierarchy, and enable multiple data
layouts, such as specialized layouts for buffering individual reduction operations.

%% A physical region is an allocation of data in a single memory in the memory hierarchy.  Physical
%% regions are grouped into {\em classes}, which are sets of physical regions sharing the
%% same names (i.e. addresses) for elements, but not necessarily sharing the same data layout.  This
%% allows pointers to elements within regions to be used, even if the regions are copied from one memory
%% to another.  Our
%% interface supports sub-classing, but the details are omitted due to space constraints.  The
%% interface allows copies between physical regions of
%% the same class (or between super- and sub-classes).  

%Note, clients can still pass arbitrary arrays
%of bits via the processor {\tt spawn} call.

% Say something about dynamically modifying the number of elements in a class

\begin{figure}
\begin{lrbox}{\mylistingbox}
\begin{lstlisting}
void reduction_ex(const void *args,size_t arglen,Processor p) {
  Memory m1,m2,m3; Processor p1,p2,p3;
  ... // Choose memory and processors, get redopid from args
  ReductionOpID redopid = ...;
  PhysicalRegion meta = PhysicalRegion::
                        create_region(num_elmts, elmt_size);
  Instance inst1 = meta.create_instance(m1, redop);
  Instance inst2 = meta.create_instance(m2, redop);
  Instance inst3 = meta.create_instance(m3);
  Event t1 = p1.spawn(REDUC, &inst1, sizeof(RegionInstance));
  Event t2 = p2.spawn(REDUC, &inst2, sizeof(RegionInstance));
  Event c1 = inst1.reduce_to(inst3, redopid, t1);
  Event c2 = inst2.reduce_to(inst3, redopid, t2);
  Event em = Event::merge_events(c1, c2);
  Event t3 = p3.spawn(USE, &inst3, sizeof(RegionInstance), em);
}
\end{lstlisting}
\end{lrbox}
\subfigure{\usebox{\mylistingbox}} \\

\subfigure{
\scalebox{0.8}{
\begin{tikzpicture}
  %\path (1,3) node (t1) [shape=rectangle,draw] {run TASK\_A} -- node [right=1pt] {ea} +(0,-0.5) node[auto] {eax};
  \node (t1) at (0,1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\REDUC(inst1)\end{tabular}};
  \draw (t1) to node[auto] {t1} (1.8,1);

  \node (t2) at (0,-1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\REDUC(inst2)\end{tabular}};
  \draw (t2) to node[auto] {t2} (1.8,-1);

  \node (t3) at (3.2,1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf copy-reduce}\\inst1 $\rightarrow$ inst3\end{tabular}};
  \draw (t3) to node[auto] {c1} (4.9,1);
  \draw [->] (1.8,1) to (t3.west);

  \node (t4) at (3.2,-1) [shape=rectangle,draw] {\begin{tabular}{c}{\bf copy-reduce}\\inst2 $\rightarrow$ inst3\end{tabular}};
  \draw (t4) to node[auto] {c2} (4.9,-1);
  \draw [->] (1.8,-1) to (t4.west);

  \node (t4j) at (5.3,0) [shape=circle,inner sep=2pt,draw] {+};
  \draw (t4j) to node[auto] {em} (6.1,0);
  \draw [->] (4.9,1) to (t4j);
  \draw [->] (4.9,-1) to (t4j);

  \node (t5) at (7.3,0) [shape=rectangle,draw] {\begin{tabular}{c}{\bf run}\\USE(inst3)\end{tabular}};
  \draw [->] (t5) to node[auto] {t3} (9,0);
  \draw [->] (6.1,0) to (t5.west);

\end{tikzpicture}
}
}
  %\vspace{-6mm}
  \caption{Reduction Example and Event Graph.\label{fig:reducevents}}
  \vspace{-4mm}
\end{figure}

Lines 39-61 of Figure~\ref{fig:runtimeapi} show a subset of the interface for physical regions.  
Each memory in the machine's memory hierarchy is named by a {\tt Memory} object (line 39).
The next object is a {\tt PhysicalRegion} object (line 44), which contains information about the
maximum number and size of the elements within the region (line 48) and handles dynamic allocation
within the region (lines 51-52), but does not contain any storage for the element data.  Instead,
element data is stored in one or more {\tt RegionInstance} objects (line 59), each of which can be
created in an arbitrary {\tt Memory} (line 54).  To maintain performance transparency, there is no
use of virtual memory - each {\tt Memory} is sized based on physical capacity and a new physical
instance can only be allocated if sufficient space remains.

A {\tt ptr\_t} value returned by the {\tt alloc}
method may be used to access the corresponding element in any of the instances of the region (line 62).
This
allows the creation of irregular data structures within physical regions.  The {\tt ptr\_t} values can
be safely stored in a region, and need not be changed when data is copied from one instance to another.
In common cases, this is as cheap as array-addressing calculations, so the overhead is minimal.
When more than one instance exists for a region, the client is responsible for maintaining coherence
of the data between instances of a region.  This is typically achieved through the use of instance-to-instance
copy operations (line 63) that use the producer task's completion event as a prerequisite and provide
their own completion event as a prerequisite for the consumer task.  When a copy operation is
the last use of a particular instance, the destruction of that instance can also be requested and made
dependent on the copy operation's event (line 56), allowing the runtime to reclaim the memory as soon as (but no
sooner than) possible.

If the only operation a task will perform on an instance is a reduction, a special reduction-only
instance can be created (line 55).  Rather than applying each reduction separately to the target instance,
the reduction operations can be accumulated in the reduction instance.  Using the {\tt reduce\_to} 
method (line 64), these accumulated reductions can be applied in bulk to another instance, which may be either
a normal instance or another reduction-only instance.  The use of reduction instances can result 
in much better performance, as we show in Section~\ref{subsec:reducmicro}, by reducing both
the total inter-node traffic and the overhead of that traffic while still allowing parallelism.
Furthermore, reduction-only instances are often smaller than normal instances, potentially improving
cache performance and helping address capacity issues for smaller memories (e.g. a GPU's device
memory).  For example, if reductions are being made to a single field in a larger structure, the 
reduction-only instance only needs to store that field and not the other fields that won't be 
modified by the reduction operation.

\texcomment{
If reductions are the only operations that will be performed on locations in a region instance,
a special reduction version can be created by providing the ID of a {\em reduction operator}
when the object is created (line 19).  Reduction-only region instances can 
increase the available parallelism and hide latency better, as we will show in 
section~\ref{subsec:reducmicro}.  The interface supports any reduction operation that
is commutative (i.e. multiple operations to the same location can be reordered without 
changing the final result) and is coded as an {\tt apply} method as shown on line 35.
Many commonly-used reduction operations are also {\em foldable} (e.g. $(l \text{ += } r_1) \text{ += } r_2$ can
be folded as $l \text{ += } r_1 + r_2$), and if the {\tt fold} operation and a {\em right identity}
(e.g. $0$ is a right identity for addition because $x + 0 = x$ for all $x$) are provided,
the runtime can use a {\em reduction fold instance}, described in section~\ref{subsec:reducimpl},
for even better performance.  Although the way they store data is very different,
reduction-only instances can be copied to other reduction-only instances (or normal 
instances) of the same class with the same asynchronous copy operations.  Copying between
instances with different layouts is only possible because physical regions provide 
the necessary information to understand the structure of the data.
}

%% Clients can also associate an operation as part of a physical region.  One example of
%% associating an operation with a physical region is shown on lines 13-14.  This instance
%% of the {\tt create\_instance} method
%% will create a physical region called a {\em reduction instance} that will be assoicated 
%% with the given {\tt ReductionOp} specified by the ID.  The interface for a {\tt ReductionOp} is shown on 
%% lines 26-33.  A reduction operation must have an {\tt apply} method, and can optionally 
%% implement two additional methods, {\tt init} and {\tt fold} that support additional 
%% optimizations described in Section~\ref{subsec:reducimpl}.  The two template parameters 
%% describe the base element type of the region ({\tt LHS}:left-hand-side) and the type of 
%% the argument to the reduction ({\tt RHS}:right-hand-side).  The reduction operation has 
%% total control over the choice of data layout for the reduction instance (not shown).
%% Reduction instances can be copied to other reduction instances with the same operation
%% or to regular physical regions of the same class.


An example using reduction-only instances is given in Figure~\ref{fig:reducevents} along with the resulting event graph.
Two reduction-only instances, {\tt inst1} and {\tt inst2}, are created in different memories to
be used with a reduction operation (lines 7-8).  (Reduction operations are registered at startup with integer IDs
as function pointers may not be portable across nodes.)  A normal instance {\tt inst3}
is also created in a different memory (line 9).  Two tasks are launched that will apply
reductions locally into their respective reduction instances (lines 10-11).  The resulting
events are then used to chain reduction copies back to {\tt inst3} after the tasks complete (lines 12-13).
The reduction operations accumulated in each of {\tt inst1} and {\tt inst2} are applied atomically
to {\tt inst3}.  The order in which the reduction operations are performed will be different from
the original order, but the requirement that reduction operations be commutative ensures the final
result is correct.  Finally, a merge on the events from the two reduction copies is 
performed and a final task using the results in {\tt inst3} is launched (lines 14-15).

%% SJT - redundant with first paragraph in this subsection?
%% Reductions and reduction instances illustrate only a single case where physical regions provide 
%% the structure necessary to conjoin an operation with data movement.  Physcal regions make our
%% interface easily extensible to arbitrary transformations and operations as a part of data movment.
%% While physical regions are a slightly higher-level construct than an annonymous
%% buffer of bits, we believe that the optimizations they enable are important in an asynchronous
%% environment.  Clients also still have access to an interface for controlling data layout through
%% the operations associated with physical regions which mitigates the loss in generality.

%{\em Physical regions} are the mechanism for reasoning about
%the placement and movement of data in the interface.  Physical regions are an 
%allocation of data in a single memory in the memory hierarchy.  Listing~\ref{lst:regionapi} 
%shows a subset of the physical region interface.  The first object in the interface
%is a {\tt PhysicalRegion} object (line 1).  Region meta-data objects provide the interface for the
%creation of sets of physical regions that possess the same number and type of elements (but not necessarily
%the same data layout).  The interface is only able
%to copy data between two physical regions that were created by the
%same region meta-data object.  A runtime error will be generated if a copy is attempted
%between two physical regions that were not created by the same meta-data object.

%An instance of a physical region is represented by a {\tt RegionInstance} object (line 17).
%When creating a physical region the programmer must specify the {\tt Memory}
%in which the physical region is going to be allocated (line 10).  Memories are described
%in more detail in Section~\ref{subsec:machmodel}.  Once the physical region is created it
%cannot be moved.   There is no coherence of data between physical regions created by the 
%same meta-data object.  It is the programmer's responsiblity to manage data coherence
%using copies between physical regions (line 22-23).  Just like other operations,
%copies can be predicated on an event and return an event corresponding to
%completion.  
%A copy between two physical regions can only be performed if copies
%are permitted between the two memories where the regions were created.  
%The legality of copies can be discovered using queries to the {\tt Machine} object
%described in Section~\ref{subsec:machmodel}.

%The data held inside a physical region is accessed by another kind of object called an
%{\em accessor} (not shown).  Accessors support read, write, and reduction operations
%to elements inside the physical region.  Accessors can be specialized for the case where
%all memory is known to be directly accessible to the executing processor, which allows 
%for direct references to elements.  In cases where not all memory is directly accessable, 
%accessors supply the level of indirection necessary to convert operations into remote 
%memory operations (RDMAs) if supported by the underlying hardware.  
%An example of this is described in more detail in Section~\ref{sec:impl}.

%One special (but common) case for applications is when reductions need to be performed.
%For reductions it is usually more efficient to store reduction
%operations in a local reduction buffer and then merge reduction buffers together at a later
%point in time.  To support this case the interface allows for the creation of a special kind
%of physical instance called a {\em reduction instance}.  The call to {\tt create\_instance} on
%lines 13-14 takes a {\em reduction operation} to be associated with a physical instance which
%tells the interface to create a reduction instance.  Reduction instances can only be accessed
%using a reduction of the given reduction type; any other accesses will result in a runtime
%error.  A reduction operation is a functor which must have at least one method of the type
%$(T_1 \rightarrow T_2 \rightarrow T_1)$ which takes a region element of type $T_1$ and a value
%to be reduced of type $T_2$ and creates and new region element.  Reduction operations can optionally 
%support two additional methods: a method that supplies an initial value of type $T_2$ and a 
%fold method of type $(T_2 \rightarrow T_2 \rightarrow T_2)$.  The presence of these methods
%enable additional optimizations for reductions described in Section~\ref{subsec:reducimpl}.

%Reduction instances can be copied using the same interface as regular physical instances.
%It is only legal to copy a reduction instance to a regular physical instance or another
%reduction instance of the same reduction operator.  Any attempt to copy from a regular physical 
%instance to a reduction instance will result in a runtime error.  A copy from a reduction 
%instance to a regular physical instance will apply all the reductions in the reduction 
%buffer to the physical instance.  If a copy is from one reduction instance to another, 
%the two reduction buffers will be merged.  The semantics of a merge are dependent upon 
%the underlying implementation of reduction buffers which is discussed in more detail 
%in Section~\ref{subsec:reducimpl}.




\subsection{Machine}
\label{subsec:machmodel}
The last component of the interface is the {\tt Machine} object, shown in lines 66-77 of Figure~\ref{fig:runtimeapi},
which provides initialization and run-time introspection
of the current machine.
The {\tt Machine} object is a singleton object that the client creates at the start of a
program, providing a task table that maps task IDs to function pointers,
and then invoking the {\tt run} method (line 69).
Any running task can get a pointer to the machine object with the
{\tt get\_machine} method (line 71) and use it to obtain lists of all the {\tt Memory} (line 72) or 
{\tt Processor} (line 73) handles in the system.  Using the affinity methods (lines 75-76),
the application can also determine which memories are accessible by each
processor and with what performance, as well as which pairs of memories support copy operations.
%are able to efficiently copy data between themselves.

%% in the machine.
%% Second, the machine object provides an interface for the client to 
%% discover properties of the underlying machine (lines 35-43).  The machine object maintains
%% sets of all the unique {\tt Processor} and {\tt Memory} handles in the system.  Processors
%% were described in Section~\ref{subsec:procs}.  For every memory in the system there also 
%% exists a {\tt Memory} handle with a unique ID.  Different memories often, but not always, 
%% imply a different address space.  Examples of different memories are described in
%% Section~\ref{sec:impl}.

%For
%example, in a large cluster each node would have a different {\tt Memory} handle
%corresponding to each node's DRAM memory.
%However, if the cluster supported multiple GPUs then there would be a different
%{\tt Memory} handle for each GPU's {\em zero-copy} memory which shares part of the 
%address space with the corresponding node memory.  Memories will be used for describing
%the placement of data described in Section~\ref{subsec:phyreg}.

%The machine interface 
%contains three different object types: {\tt Processor} (line 1), {\tt Memory} (line 17), and a 
%singleton {\tt Machine} object (line 22).  For every processor (i.e. CPU core or GPU) in 
%the system there exists a {\tt Processor} handle with a unique ID naming that processor.  
%{\tt Processor} objects support a single {\tt spawn} operation (line 13-14) that will
%launch a {\em task} on that processor.   

%% The machine object also maintains two relations between the set of processors
%% and the set of memories:
%% \begin{itemize}
%% \item Processor-Memory: for each processor-memory pair whether
%% the processor can directly access (read and write) the memory and 
%% latency and bandwidth properties if access is possible
%% \item Memory-Memory: for every pair of memories whether copies can be
%% performed and latency and bandwidth properties if copies are possible
%% \end{itemize}
%%
%% The client can query these two relations by the {\tt get\_*\_affinity} calls
%% (lines 40-43) to discover properties of the machine.  

%These calls populate a vector with affinity structures (not shown)
%that contain latency and bandwidth information.  The programmer can restrict the query
%by specifying a specific processor or memory, otherwise information is returned
%about all processors and memories in the system.  



