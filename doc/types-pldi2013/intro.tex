

\section{Introduction}
\label{sec:intro}

Machine architecture, particularly at the high
performance end of the spectrum, has recently undergone a revolution.  The
latest supercomputers are now composed of heterogeneous processors
and deep memory hierarchies.  Current programming systems for these
machines have elaborate features for describing parallelism, but 
few abstractions for describing the organization of data.  However, as 
distributed memory machines increase in complexity,
reasoning about the organization of program data will be imperative
for supporting the management of parallelism and the movement of data
through the memory hierarchy.

%Consequently, the burden of managing
%the correctness of parallel computations and movement of data
%through the memory hierarchy is placed on the programmer.

%However,
%these systems relegate to the programmer the responsibility for:
%\begin{itemize}
%\item the correctness of their parallel declarations
%\item the movement and placement of data in the memory hierarchy
%\end{itemize}
%Coupled with the complexity of these machines, this places a heavy
%burden on the programmer.  The reason that the programmer must bear
%this load is because current programming systems have few or no
%insights into properties of program data (e.g. locality and independence).  
%To enable more efficient programming of this class of machines we 
%need programming systems capable of reasoning both statically and dynamically
%about the structure of program data.

Previous work has explored two approaches for describing the organization of data
in distributed memory programs.
One class of work uses fully static analyses with no runtime overhead, but 
disallows potential data aliasing to make the analyis tractable, limiting expressivity.
Two recent examples, Sequoia \cite{Fatahalian06} and 
Deterministic Parallel Java (DPJ) \cite{Bocchino09}, each provide a 
mechanism to statically partition the heap.
% into a tree of collections of data.
The two designs are different in many aspects, but agree that there is
a single tree-shaped partitioning of data that must be checked statically 
(see Section~\ref{sec:related}).  The second approach permits potential data
aliasing, allowing greater expressivity, but uses fully dynamic analyses of
program execution to detect data aliasing and guarantee correct results.
This approach incurs significant runtime overhead and requires 
centralized control logic that is difficult to scale to distributed 
memory machines; examples include transactional memory \cite{Harris05} 
and thread-level speculation \cite{Steffan00}.

Our own experience writing high-performance applications
using the current industry standard mix of MPI, shared-memory
threads, and CUDA has taught us that a
single, static data partitioning is often insufficient.  In many cases, the
best way to partition data is often a function of the data
itself---i.e., the partitions must be dynamically computed.
Furthermore, applications often need multiple, simultaneous partitions of
the same data, each providing different views on to that data.  

In this work, we present the static and dynamic semantics of Legion \cite{Legion12},
a parallel programming model that supports multiple, dynamic data partitions and is
able to efficiently reason about potential aliasing.
Legion's {\em logical regions} are an abstraction capturing {\em locality} 
(data that will be used together, and therefore should be colocated) 
and {\em independence} (disjoint data that may be operated on in parallel).  
Logical regions have properties that allow programmers to communicate information
about the structure of program data to the Legion implementation:
\begin{itemize}
\item  logical regions are first-class values in Legion
and may be dynamically allocated and stored in data structures;

\item logical regions can be dynamically {\em partitioned} into {\em subregions}; 
the programmer can express arbitrary partitions of regions;

\item  a logical region may be dynamically partitioned in multiple different ways;
subregions from multiple partitions may include the same data, introducing 
aliasing\footnote{Thus
  the term {\em logical} regions: language-level regions
  name sets of data but do not imply a physical layout or placement in
  the memory hierarchy. A separate system of {\em physical regions} at
  runtime holds copies of data in the language-level logical regions
  \cite{Legion12}.}.

%\item {\em privilege} and {\em coherence} properties that
%describe how a logical region's data may be used by computations (explained below).
\end{itemize}

%Both dynamically computed partitions
%and simultaneous partitions of the same data introduce the possibility
%of {\em aliasing}---distinct regions may have overlapping data.

While data in Legion is organized in a hierarchy of logical regions,
computation is organized in a hierarchy of tasks.  Logical regions and tasks interact
through a system of {\em privileges} that specify which operations (read, write, or reduce)
a task (or any of its subtasks) may perform on each logical region argument.  
%Tasks can only access logical regions for which they 
%possess privileges.  
%Tasks acquire privileges from their calling task
%or by creating new logical regions.  A key restriction is that 
%a sub-task can only be passed a subset of the privileges of its parent task.  

%Through multiple partitions of a logical region, a given datum may
%belong to multiple different regions.  

To support additional parallelism in the presence of aliased data,
Legion provides {\em coherence} modes at the granularity of regions,
which allow relaxed constraints on task execution order, even
in case where tasks do access the same region.  The programmer may
specify that tasks conflicting on a region must execute serially (in
program order, the default), {\em atomically} with respect to each
other, or even {\em simultaneously} if the application has alternative
methods for ensuring determinism or can tolerate non-deterministic
behavior in that region.

%To guarantee the safety and correctness of programs, Legion relies on
%a combination of static and dynamic analysis based on the privilege
%and coherence systems.  
%To efficiently guarantee the safety and
%correctness of programs using first-class regions,
%privilege and coherence properties in the presence of arbitrary region
%aliasing, Legion relies on a combination of static and
%dynamic checks.  
The critical feature in Legion that makes static analysis tractable
and dynamic analysis efficient in the presence of potential aliasing is logical regions.
Alias analysis is a very hard static analysis problem, but is both easy and
inexpensive if done dynamically at the granulatiry of logical regions instead
of individual memory locations; similarly
checks on individual memory accesses are expensive if done dynamically, but
cheap if done statically within the context of local logical region privileges.
%A key theme that runs through the design is that 
%alias analysis is a very hard static analysis problem, but is both easy and
%inexpensive if done dynamically at the granularity of regions instead of individual
%heap locations, while checks that must be done on individual values are only
%cheap if done statically.  
Explicating and evaluating the design of Legion's
static and dynamic semantics and its implications are the topics of this paper,
which we present as a series of contributions:
\begin{itemize}
\item We present a type system for {\em Core Legion} programs that statically
  verifies the safety of individual pointers and region
  privileges at task call boundaries (Section~\ref{subsec:coretypes}).
%  A subtle issue arises in checking privileges when regions are
%  stored in and later retrieved from heap data structures.  The type
%  system provides enough static information that the Legion runtime
%  can leverage its dynamic information about region aliasing to
%  perform dynamic privilege checks guaranteeing safety.

\item We present a parallel operational semantics for Core Legion that
  may be of independent interest.  This semantics is compositional,
  hierarchical, and asynchronous, reflecting the way such programs
  actually execute on the hardware.  The semantics also
  models conflicting memory operations between tasks in cases of relaxed
  coherence, making explicit the boundary between deterministic and
  possible non-deterministic behavior (Section~\ref{subsec:opsemantics}).

\item We prove the soundness of Legion's static type and privilege system (Section~\ref{sec:soundness}).


\item We use soundness of the type system to prove the soundness of
  Legion's region coherence modes (Section~\ref{sec:coherence}).


\item Again using the soundness of the type system,  we show that if expressions $e_1$ and $e_2$ 
are {\em non-interfering} (can be executed in parallel), then subexpressions
$e_1'$ of $e_1$ and $e_2'$ of $e_2$ are also non-interfering (Section~\ref{sec:scheduling}).  
This result is the basis
for a hierarchical, distributed scheduler in the Legion implementation which is crucial for high performance
on the target class of machines.
%  any form of centralized scheduling would be a bottleneck because of the
%large communication latencies in the target class of machines.
%; on the target class of machines, any centralized
%scheduler would be a serious bottleneck.

\item We give experimental evidence that supports the Legion design choices.  On three
  real-world applications, we show that dynamic region pointer checks
  would be expensive, justifying checking this aspect of the type
  system statically.  We also show that the cost of region aliasing
  checks is low, showing that an expressive and dynamic
  language with aliasing is not incompatible with both high
  performance and safety (Section~\ref{sec:evaluation}).

\end{itemize}






%The crucial aspect of this result is that the tasks $t_1$ and $t_2$ only 
%need be non-interfering instead of non-aliased.  Thus, through a combination
%of static and dynamic analysis, our privilge type system and coherence system
%enable us to implement a hierarchical, distributed scheduling algorithm even
%in the presence of aliasing introduced by the need for dynamic descriptions of
%data via first-class logical regions.  
%All of this is possible only because
%our programming system supports an abstraction for understanding the structure
%of program data.



%The important difference between Legion and the more static approaches
%is that Legion allows region aliasing, but does so in a structured way that minimizes
%runtime overheads.  
%The key observation is that static alias analysis is hard, but dynamic 
%alias analysis is very easy and relatively cheap when done at the granularity 
%of regions instead of individual heap locations.  
%Thus, Legion falls between fully static systems, such as
%Sequoia and DPJ, and fully dynamic approaches such as transactional
%memory.  
%Legion still has a significant static component; in
%particular, the required privileges for function calls
%and region
%pointer dereferences are checked statically.  The dynamic alias checks
%for independence are done at the granularity of regions and one check 
%on a region
%often serves to prove the independence of many subsequent uses of that region.
%In contrast, because transactional memory has no mechanism
%for grouping heap data into larger collections, it must test the
%overlap between two sets of data on each individual memory location, which is
%significantly more expensive.



%which makes it difficult 
%to statically determine which computations can be run in parallel.  
%One approach is to outlaw aliasing which allows for a static analysis
%to discover all available parallelism and schedule it entirely at
%compile-time, avoiding any runtime overhead.  Languages such as
%Sequoia and DPJ take this approach and allow only a single static partition of any region 
%to eliminate region aliasing, at some cost in expressiveness.



%While many programming systems for
%these machines have intricate features for describing parallelism,
%few have any abstractions for capturing properties of a program's data (e.g.
%independence and locality).  Understanding the structure of a program's data
%is critical for allowing programming systems to validate/discover
%parallelism or support placement of data in the memory hierarchy.  
%The few programming systems that do support data abstraction 
%features\cite{Fatahalian06,Bocchino09} permit only
%statically analyzable abstractions which restrict expressivity.  We
%present a type system for Legion\cite{Legion12}, a programming model
%that permits both static and dynamic descriptions of the structure of 
%program data.  Despite the dual nature of Legion, our type system allows us 
%to statically prove several useful properties about the structure and usage 
%of data in Legion programs.  We show how to leverage these properties 
%to enable a scalable, hierarchical scheduling algorithm based on a 
%dynamic analysis of program data.

%complex hierarchies of many different
%kinds of computing technologies: networks of nodes at the top level,
%multiple chips per node, multiple cores within a chip, and, 
%recently, multiple accelerators (usually GPUs) per node, which 
%can themselves be further decomposed.  We present the operational and static
%semantics of Legion\cite{Legion12}, a programming model targeted at providing an
%appropriate level of abstraction for programming such machines, one
%that is both sufficiently high-level to be portable while still
%exposing aspects that are crucial to performance. 

%The primary abstraction for capturing the structure of data in Legion is {\em logical regions}. Logical
%regions express {\em locality} (data that will be used together, and therefore should be colocated) 
%and {\em independence} (disjoint data that may be operated on in parallel).
%Legion programs organize data into a forest of logical regions.  Logical regions can be
%recursively partitioned into sub-regions.  

%In Legion data is organized in a hierarchy of {\em regions}
%and subregions while computation is organized in a hierarchy of {\em
%tasks} and subtasks operating on regions.  Regions and tasks interact
%through a static system of {\em region privileges} specifying 
%operations a task may perform on a region argument (read,
%write, or reduce) and  {\em region coherence} annotations that
%express what other tasks may do concurrently with
%the region (exclusive, atomic, or simultaneous).  We prove the
%soundness of the privileges and coherence system and use these two
%results to prove a third result: if two {\em siblings} (tasks that
%have the same immediate parent task) $t_1$ and $t_2$ are 
%{\em non-interfering} (have no ordering requirements for correctness), 
%then the any descendant of $t_1$ in the task hierarchy is non-interfering
%with any descendant of $t_2$.  This theorem is the basis for correct distributed
%task scheduling in a Legion implementation, which is crucial for
%performance; a centralized scheduler would be a bottleneck
%because of the large communication latencies in the target
%class of machines.


% Putting this here so that we can get the code in a good place
\lstset{
  captionpos=b,
  language=Haskell,
  basicstyle=\scriptsize,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  stepnumber=1,
  escapechar=\#,
  keepspaces=true,
  belowskip=-10pt,
  literate={<}{{$\langle$}}1 {>}{{$\rangle$}}1,
  morekeywords={function,rr,int,float,bool,isnull,partition,as,downregion,upregion,reads,writes,rdwrs,reduces,read,write,reduce,using,unpack,pack,coloring,multicoloring,color,newcolor,atomic,simultaneous},
  deletekeywords={float,head,min,max}
}
\begin{lstlisting}[float={t},label={lst:circuit_ex},caption={Circuit Simulation}]
--                        <voltage,current,charge,capacitance>
type CircuitNode        = <float,float,float,float>
--                     < owned node, owned or ghost node, resistance, current>
type CircuitWire<rn,rg>  = <CircuitNode@rn, CircuitNode@(rn,rg),float,float>

type node_list<rl,rn>       = < CircuitNode@rn, node_list<rl,rn>@rl >
type wire_list<rl,rw,rn,rg>= < CircuitWire<rn,rg>@rw, wire_list<rl,rw,rn,rg>@rl >

type CircuitPiece<rl,rw,rn> = rr[rpw,rpn,rg]
                            < wire_list<rl,rpw,rpn,rg>@rl, node_list<rl,rpn>@rl >         
                            where rpn #$\le$# rn and rg #$\le$# rn and rpw #$\le$# rw and
                                  rn * rw and rl * rn and rl * rw

-- Simulation initialization and invocation
function simulate_circuit[rl,rw,rn] ( all_nodes : node_list<rl,rn>@rl, 
                            all_wires : wire_list<rl,rw,rn,rn>@rl, steps : int ), 
      reads(rn,rw,rl), writes(rn,rw,rl) : bool = 
  let pc : <coloring(rn),multicoloring(rn),coloring(rw)> 
            = color_circuit[rn,rw,rl](all_nodes,all_wires) in
  -- Disjoint partition for the owned nodes of each piece
  partition rn using pc.1 as rn0,rn1 in
  -- Aliased partition for ghost nodes of each piece
  partition rn using pc.2 as rg0,rg1 in
  -- Disjoint partition for the owned wires of each piece
  partition rw using pc.3 as rw0,rw1 in
  let lists0 : <wire_list<rl,rw0,rn0,rg0>@rl,node_list<rl,rn0>@rl> = 
        build_lists[rl,rw,rn,rw0,rn0,rg0](all_nodes,all_wires,pc.1,pc.2,pc.3,0) in
  let piece0 : CircuitPiece<rl,rw,rn> = 
        pack lists0 as CircuitPiece<rl,rw,rn>[rw0,rn0,rg0] in
  let lists1 : <wire_list<rl,rw1,rn1,rg1>@rl,node_list<rl,rn1>@rl> =
        build_lists[rl,rw,rn,rw1,rn1,rg1](all_nodes,all_wires,pc.1,pc.2,pc.3,1) in
  let piece1 : CircuitPiece<rl,rw,rn> = 
        pack lists1 as CircuitPiece<rl,rw,rn>[rw1,rn1,rg1] in
      execute_time_steps[rl,rw,rn](piece0,piece1,steps)

-- Time Step Loop
function execute_time_steps[rl,rw,rn] ( p0 : CircuitPiece<rl,rw,rn>, 
      p1 : CircuitPiece<rl,rw,rn>, steps : int ) , reads(rn,rw,rl), writes(rn,rw) : bool = 
  if steps #$<$# 1 then true else
  unpack p0 as piece0 : CircuitPiece<rl,rw,rn>[rw0,rn0,rg0] in 
  unpack p1 as piece1 : CircuitPiece<rl,rw,rn>[rw1,rn1,rg1] in
  let _ : bool = calc_new_currents[rl,rw0,rn0,rg0](piece0.1) in
  let _ : bool = calc_new_currents[rl,rw1,rn1,rg1](piece1.1) in
  let _ : bool = distribute_charge[rl,rw0,rn0,rg0](piece0.1) in
  let _ : bool = distribute_charge[rl,rw1,rn1,rg1](piece1.1) in
  let _ : bool = update_voltage[rl,rn0](piece0.2) in
  let _ : bool = update_voltage[rl,rn1](piece1.2) in
      execute_time_steps[rl,rw,rn](p0,p1,steps-1)

function color_circuit[rn,rw,rl] ( all_nodes : node_list<rl,rn>@rl, 
                               all_wires : wire_list<rl,rw,rn>@rl ), 
        reads(rn,rw,rl) : <coloring(rn), multicoloring(rn), coloring(rw)> =  
  -- Invoke programmer chosen coloring algorithm (e.g. METIS)
  -- return owned, ghost, wire colorings

-- Helper method
function build_lists[rl,rw,rn,rpw,rpn,rg] ( nodes : node_list<rl,rn>@rl, 
       wires : wire_list<rl,rw,rn>@rl, oc : coloring(rn), gc : multicoloring(rn), 
       wc : coloring(rw), c : int), reads(rn,rw,rl), writes(rl) 
       : < wire_list<rl,rpw,rpn,rg>@rl, node_list<rl,rpn>@rl > = 
  -- Construct lists of node and wire pointers for the given colorings
\end{lstlisting}

% Moving this to the circuit example section since it fits better there
% and some of this is redundant
%Legion's execution model is that by default, a program's semantics is
%its meaning as a sequential program, which we refer to as the {\em
%program order} execution.  While the
%implementation is not our focus here, if two functions use
%disjoint data (because all of the regions they use are disjoint), then
%Legion may execute them simultaneously.  Also like DPJ,
%Legion has a system of {\em privileges} ({\em read}, {\em write}, and
%{\em reduce}) on regions and an orthogonal system of region {\em
%coherence} modes ({\em excl}, {\em atomic}, and {\em simultaneous})
%that increase the number of situations in which functions can be
%executed in parallel.

%The rest of the paper is organized as follows.
%We begin in Section~\ref{sec:example} with an example program that illustrates
%a typical style of programming in Legion and motivates the need for multiple, dynamically computed partitions of
%a region that introduce aliased data.  We define Core Legion, a small language suitable 
%for our formal development, in Section~\ref{sec:legioncore}.
%Finally, Section~\ref{sec:related} discusses the most closely related work and
%Section~\ref{sect:conclusion} concludes.


  










