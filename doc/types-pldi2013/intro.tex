

\section{Introduction}
\label{sec:intro}

In the past decade machine architectures, particularly at the high
performance end of the spectrum, have undergone a revolution.  The
latest supercomputers are now composed of heterogeneous processors
and deep memory hierarchies.  Programming this class of machines is
increasingly difficult.  Current programming systems for these
machines have intricate features for describing parallelism, but contain
few abstractions for describing properties of program data
(e.g. independence).  Consequently, the burden of managing
the correctness of parallel computations and movement of data
through the memory hierarchy is placed on the programmer.

%However,
%these systems relegate to the programmer the responsibility for:
%\begin{itemize}
%\item the correctness of their parallel declarations
%\item the movement and placement of data in the memory hierarchy
%\end{itemize}
%Coupled with the complexity of these machines, this places a heavy
%burden on the programmer.  The reason that the programmer must bear
%this load is because current programming systems have few or no
%insights into properties of program data (e.g. locality and independence).  
%To enable more efficient programming of this class of machines we 
%need programming systems capable of reasoning both statically and dynamically
%about the structure of program data.

To combat this problem, researchers have previously explored programming
systems that are capable of reasoning about the structure of data in
parallel programs.  Traditionally, these attempts have taken one of two approaches.   
The first approach relies on fully static analyses that
have no runtime overhead, but limited expressivity.  Two recent examples, 
Sequoia \cite{Fatahalian06} and 
Deterministic Parallel Java (DPJ) \cite{Bocchino09}, each provide a 
mechanism to statically partition the heap recursively into a tree of collections 
of data.  The two designs are different in many aspects, but agree that there is
a single tree-shaped partitioning of data that must be fully checked statically 
(see Section~\ref{sec:related} for more discussion of related work).  The
second approach relies on a totally dynamic analysis of program data that doesn't
limit expressivity, but incurs runtime overhead.  Examples of this approach 
include transactional memory \cite{Harris05} and thread-level speculation \cite{Steffan00}.

These two approaches illustrate different mechanisms for dealing with
the problem of data aliasing in parallel programs.  The fully static approach disallows aliasing
by requiring a single static partitioning of data.  Our own experience with writing high-performance 
applications in both Sequoia and the current industry standard mix of MPI, shared-memory
threads, and CUDA, has taught us that for many practical situations a
single, static partitioning is insufficient.  In practice, the
best way to partition data is often a function of the data
itself---i.e., the partitions need to be dynamically computed.
Furthermore, it is useful to allow multiple, simultaneous partitions of
the same data, providing different views on to that data.  Alternatively, 
the fully dynamic approach enables arbitrary data aliasing, but requires 
centralized control logic to perform conflict detection between every pair 
of parallel computations. This results in runtime overhead and is 
not scalable on distributed-memory machines.

In this work, we present a type system and operational semantics for
Legion\cite{Legion12}.  Legion is a programming model that supports multiple,
dynamic partitions of program data which can result in aliasing.  We prove
the soundness of our type system and show that through a combination of
static and dynamic analysis we can implement it in a way that minimizes dynamic
runtime checks and results in low actual runtime overhead.  We then show 
that the soundness of our type system enables a scalable, hierarchical 
distributed scheduling algorithm that enables Legion programs to achieve
high performance on current supercomputers.  

Legion presents {\em logical regions} as an
abstraction for capturing {\em locality} (data that will be used together,
and therefore should be colocated) and {\em independence} (disjoint data
that may be operated on in parallel).  Logical regions have the following
properties that enable programmers to dynamically communicate information
about the structure of program data to Legion:
\begin{itemize}
\item  logical regions are first-class values in Legion
and may be dynamically allocated and stored in data structures;

\item logical regions can be dynamically {\em partitioned} into {\em subregions}; 
the programmer can express arbitrary partitions of regions;

\item  a logical region may be dynamically partitioned in multiple different ways.
\end{itemize}
Through multiple partitions
of a logical region, a given datum may belong to multiple different regions.  
For example, if a region $R$ is disjointly partitioned three distinct ways 
and each partition assigns every element in $R$ to a
subregion (partitions need not be total in Legion), then every element
in $R$ is included in three different subregions.  This is why we use
the term {\em logical} regions: language-level regions serve to name
sets of data but do not imply anything about physical layout or placement
in the memory hierarchy.\footnote{There
is a separate system of {\em physical regions} at runtime that hold
copies of the data in the language-level logical regions.  Details can
be found in \cite{Legion12}.}

While data in Legion is organized into a hierarchy of logical region forests, 
computation is organized into a hierarchy of tasks.  Logical regions and tasks interact
through a system of privileges specifying operations that a task
can perform on a logical region argument (read, write, or reduce).  
Tasks are only permitted to access logical regions for which they 
possess privileges.  Privileges are given to a task by their calling task
or by creating new logical regions.  The Legion programming model 
mandates a functional restriction: a sub-task can only be passed a subset
of the privileges possesed by its parent task.  

To enforce this restriction, our type system relies on a combination of static 
and dynamic analyses.  The key observation behind our type system is that dynamic
analysis is very easy and relatively cheap when done at the granularity of
regions instead of individual heap locations.  To that end, our type system supports
static verification of all pointer dereferences and passing of region privileges at 
task calls boundaries.  The Legion runtime then leverages its knowledge of logical
region aliasing to perform dynamic safety checks for the unpacking
and packing of logical regions to/from heap with regards to task privileges.  
These checks are then amortized over all the uses of the logical region 
which significantly reduces their cost.

To enable parallelism, Legion again relies on a combination of static and dynamic
analysis to prove that two tasks operate on independent sets of data.  If two tasks
cannot be proven independent statically, the Legion runtime performs dynamic dependence
tests to detect region aliasing.  Since these tests are performed at the granularity of
regions instead of individual heap locations, they are significantly less costly.
In addition to supporting parallel execution for independent tasks, Legion also enables
programmer specified parallel execution in the presence of aliasing.  For example, two tasks 
accessing aliased data may only need to be serialized
with respect to each other (e.g. run atomically) or are permitted to run
simultaneously with the programmer using separate synchronization mechanisms
to control access (e.g. locks).  To support these relaxed access modes
Legion provides {\em cohrence modes} on logical regions.  For each region
argument to a task, a programmer specifies the coherence in which other tasks
can access aliased data while the task is running.  We present an operational semantics
for coherence modes and show that it is sound.

Using our proofs of the soundess of the privilegs and coherence systems, we then 
prove a third result: if two {\em siblings} (tasks that
have the same immediate parent task) $t_1$ and $t_2$ are 
{\em non-interfering} (have no ordering requirements for correctness), 
then any descendant of $t_1$ in the task hierarchy is non-interfering
with any descendant of $t_2$.  This theorem is the basis for correct, distributed
task scheduling in a Legion implementation, which is crucial for
performance; a centralized scheduler would be a bottleneck
because of the large communication latencies in the target
class of machines.

%The crucial aspect of this result is that the tasks $t_1$ and $t_2$ only 
%need be non-interfering instead of non-aliased.  Thus, through a combination
%of static and dynamic analysis, our privilge type system and coherence system
%enable us to implement a hierarchical, distributed scheduling algorithm even
%in the presence of aliasing introduced by the need for dynamic descriptions of
%data via first-class logical regions.  
%All of this is possible only because
%our programming system supports an abstraction for understanding the structure
%of program data.



%The important difference between Legion and the more static approaches
%is that Legion allows region aliasing, but does so in a structured way that minimizes
%runtime overheads.  
%The key observation is that static alias analysis is hard, but dynamic 
%alias analysis is very easy and relatively cheap when done at the granularity 
%of regions instead of individual heap locations.  
%Thus, Legion falls between fully static systems, such as
%Sequoia and DPJ, and fully dynamic approaches such as transactional
%memory.  
%Legion still has a significant static component; in
%particular, the required privileges for function calls
%and region
%pointer dereferences are checked statically.  The dynamic alias checks
%for independence are done at the granularity of regions and one check 
%on a region
%often serves to prove the independence of many subsequent uses of that region.
%In contrast, because transactional memory has no mechanism
%for grouping heap data into larger collections, it must test the
%overlap between two sets of data on each individual memory location, which is
%significantly more expensive.



%which makes it difficult 
%to statically determine which computations can be run in parallel.  
%One approach is to outlaw aliasing which allows for a static analysis
%to discover all available parallelism and schedule it entirely at
%compile-time, avoiding any runtime overhead.  Languages such as
%Sequoia and DPJ take this approach and allow only a single static partition of any region 
%to eliminate region aliasing, at some cost in expressiveness.



%While many programming systems for
%these machines have intricate features for describing parallelism,
%few have any abstractions for capturing properties of a program's data (e.g.
%independence and locality).  Understanding the structure of a program's data
%is critical for allowing programming systems to validate/discover
%parallelism or support placement of data in the memory hierarchy.  
%The few programming systems that do support data abstraction 
%features\cite{Fatahalian06,Bocchino09} permit only
%statically analyzable abstractions which restrict expressivity.  We
%present a type system for Legion\cite{Legion12}, a programming model
%that permits both static and dynamic descriptions of the structure of 
%program data.  Despite the dual nature of Legion, our type system allows us 
%to statically prove several useful properties about the structure and usage 
%of data in Legion programs.  We show how to leverage these properties 
%to enable a scalable, hierarchical scheduling algorithm based on a 
%dynamic analysis of program data.

%complex hierarchies of many different
%kinds of computing technologies: networks of nodes at the top level,
%multiple chips per node, multiple cores within a chip, and, 
%recently, multiple accelerators (usually GPUs) per node, which 
%can themselves be further decomposed.  We present the operational and static
%semantics of Legion\cite{Legion12}, a programming model targeted at providing an
%appropriate level of abstraction for programming such machines, one
%that is both sufficiently high-level to be portable while still
%exposing aspects that are crucial to performance. 

%The primary abstraction for capturing the structure of data in Legion is {\em logical regions}. Logical
%regions express {\em locality} (data that will be used together, and therefore should be colocated) 
%and {\em independence} (disjoint data that may be operated on in parallel).
%Legion programs organize data into a forest of logical regions.  Logical regions can be
%recursively partitioned into sub-regions.  

%In Legion data is organized in a hierarchy of {\em regions}
%and subregions while computation is organized in a hierarchy of {\em
%tasks} and subtasks operating on regions.  Regions and tasks interact
%through a static system of {\em region privileges} specifying 
%operations a task may perform on a region argument (read,
%write, or reduce) and  {\em region coherence} annotations that
%express what other tasks may do concurrently with
%the region (exclusive, atomic, or simultaneous).  We prove the
%soundness of the privileges and coherence system and use these two
%results to prove a third result: if two {\em siblings} (tasks that
%have the same immediate parent task) $t_1$ and $t_2$ are 
%{\em non-interfering} (have no ordering requirements for correctness), 
%then the any descendant of $t_1$ in the task hierarchy is non-interfering
%with any descendant of $t_2$.  This theorem is the basis for correct distributed
%task scheduling in a Legion implementation, which is crucial for
%performance; a centralized scheduler would be a bottleneck
%because of the large communication latencies in the target
%class of machines.


% Putting this here so that we can get the code in a good place
\lstset{
  captionpos=b,
  language=Haskell,
  basicstyle=\scriptsize,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  stepnumber=1,
  escapechar=\#,
  keepspaces=true,
  literate={<}{{$\langle$}}1 {>}{{$\rangle$}}1,
  morekeywords={function,rr,int,float,bool,isnull,partition,as,downregion,upregion,reads,writes,rdwrs,reduces,read,write,reduce,using,unpack,pack,coloring,multicoloring,color,newcolor,atomic,simultaneous},
  deletekeywords={float,head,min,max}
}
\begin{lstlisting}[float={t},label={lst:circuit_ex},caption={Circuit Simulation}]
--                        <voltage,current,charge,capacitance>
type CircuitNode        = <float,float,float,float>
--                     < owned node, owned or ghost node, resistance, current>
type CircuitWire<rn,rg>  = <CircuitNode@rn, CircuitNode@(rn,rg),float,float>

type node_list<rl,rn>       = < CircuitNode@rn, node_list<rl,rn>@rl >
type wire_list<rl,rw,rn,rg>= < CircuitWire<rn,rg>@rw, wire_list<rl,rw,rn,rg>@rl >

type CircuitPiece<rl,rw,rn> = rr[rpw,rpn,rg]
                            < wire_list<rl,rpw,rpn,rg>@rl, node_list<rl,rpn>@rl >         
                            where rpn #$\le$# rn and rg #$\le$# rn and rpw #$\le$# rw and
                                  rn * rw and rl * rn and rl * rw

-- Simulation initialization and invocation
function simulate_circuit[rl,rw,rn] ( all_nodes : node_list<rl,rn>@rl, 
                            all_wires : wire_list<rl,rw,rn,rn>@rl, steps : int ), 
      reads(rn,rw,rl), writes(rn,rw,rl) : bool = 
  let pc : <coloring(rn),multicoloring(rn),coloring(rw)> 
            = color_circuit[rn,rw,rl](all_nodes,all_wires) in
  -- Disjoint partition for the owned nodes of each piece
  partition rn using pc.1 as rn0,rn1 in
  -- Aliased partition for ghost nodes of each piece
  partition rn using pc.2 as rg0,rg1 in
  -- Disjoint partition for the owned wires of each piece
  partition rw using pc.3 as rw0,rw1 in
  let lists0 : <wire_list<rl,rw0,rn0,rg0>@rl,node_list<rl,rn0>@rl> = 
        build_lists[rl,rw,rn,rw0,rn0,rg0](all_nodes,all_wires,pc.1,pc.2,pc.3,0) in
  let piece0 : CircuitPiece<rl,rw,rn> = 
        pack lists0 as CircuitPiece<rl,rw,rn>[rw0,rn0,rg0] in
  let lists1 : <wire_list<rl,rw1,rn1,rg1>@rl,node_list<rl,rn1>@rl> =
        build_lists[rl,rw,rn,rw1,rn1,rg1](all_nodes,all_wires,pc.1,pc.2,pc.3,1) in
  let piece1 : CircuitPiece<rl,rw,rn> = 
        pack lists1 as CircuitPiece<rl,rw,rn>[rw1,rn1,rg1] in
      execute_time_steps[rl,rw,rn](piece0,piece1,steps)

-- Time Step Loop
function execute_time_steps[rl,rw,rn] ( p0 : CircuitPiece<rl,rw,rn>, 
      p1 : CircuitPiece<rl,rw,rn>, steps : int ) , reads(rn,rw,rl), writes(rn,rw) : bool = 
  if steps #$<$# 1 then true else
  unpack p0 as piece0 : CircuitPiece<rl,rw,rn>[rw0,rn0,rg0] in 
  unpack p1 as piece1 : CircuitPiece<rl,rw,rn>[rw1,rn1,rg1] in
  let _ : bool = calc_new_currents[rl,rw0,rn0,rg0](piece0.1) in
  let _ : bool = calc_new_currents[rl,rw1,rn1,rg1](piece1.1) in
  let _ : bool = distribute_charge[rl,rw0,rn0,rg0](piece0.1) in
  let _ : bool = distribute_charge[rl,rw1,rn1,rg1](piece1.1) in
  let _ : bool = update_voltage[rl,rn0](piece0.2) in
  let _ : bool = update_voltage[rl,rn1](piece1.2) in
      execute_time_steps[rl,rw,rn](p0,p1,steps-1)

function color_circuit[rn,rw,rl] ( all_nodes : node_list<rl,rn>@rl, 
                               all_wires : wire_list<rl,rw,rn>@rl ), 
        reads(rn,rw,rl) : <coloring(rn), multicoloring(rn), coloring(rw)> =  
  -- Invoke programmer chosen coloring algorithm (e.g. METIS)
  -- return owned, ghost, wire colorings

-- Helper method
function build_lists[rl,rw,rn,rpw,rpn,rg] ( nodes : node_list<rl,rn>@rl, 
       wires : wire_list<rl,rw,rn>@rl, oc : coloring(rn), gc : multicoloring(rn), 
       wc : coloring(rw), c : int), reads(rn,rw,rl), writes(rl) 
       : < wire_list<rl,rpw,rpn,rg>@rl, node_list<rl,rpn>@rl > = 
  -- Construct lists of node and wire pointers for the given colorings
\end{lstlisting}

% Moving this to the circuit example section since it fits better there
% and some of this is redundant
%Legion's execution model is that by default, a program's semantics is
%its meaning as a sequential program, which we refer to as the {\em
%program order} execution.  While the
%implementation is not our focus here, if two functions use
%disjoint data (because all of the regions they use are disjoint), then
%Legion may execute them simultaneously.  Also like DPJ,
%Legion has a system of {\em privileges} ({\em read}, {\em write}, and
%{\em reduce}) on regions and an orthogonal system of region {\em
%coherence} modes ({\em excl}, {\em atomic}, and {\em simultaneous})
%that increase the number of situations in which functions can be
%executed in parallel.

The rest of the paper is organized as follows.
We begin in Section~\ref{sec:example} with an example program that illustrates
a typical style of programming in Legion and motivates the need for multiple, dynamically computed partitions of
a region that introduce aliased data.  We define Core Legion, a small language suitable 
for our formal development, in Section~\ref{sec:legioncore}.
The next four sections each state and prove one of our main results and contributions:
\begin{itemize}
\item We prove the soundness of Legion's privileges system (Section~\ref{sec:soundness}).

\item We use the soundness of privileges to prove the soundness of Legion's region coherence modes (Section~\ref{sec:coherence}).

\item We show that if expressions $e_1$ and $e_2$ are {\em non-interfering} (can be executed in parallel), then subexpressions
$e_1'$ of $e_1$ and $e_2'$ of $e_2$ are also non-interfering (Section~\ref{sec:scheduling}).  
%This result is the basis
%for a hierarchical, distributed scheduler in the Legion implementation which is crucial for high performance
%on the target class of machines.
%; on the target class of machines, any centralized
%scheduler would be a serious bottleneck.

\item We give experimental evidence for the Legion design.  On three real-world applications, we show that 
dynamic region pointer checks would be expensive, justifying checking this aspect of the type system statically.
We also show that the cost of checking for non-interference of region privileges is low, showing that a much more expressive
and dynamic language with aliasing is not incompatible with high performance (Section~\ref{sec:evaluation}).

\end{itemize}
Finally, Section~\ref{sec:related} discusses the most closely related work and
Section~\ref{sect:conclusion} concludes.


  










