
\section{Circuit Example}
\label{sec:example}

Listing~\ref{lst:circuit_ex} shows a circuit simulation written
in the core Legion language (see
Section~\ref{sec:legioncore}).  The input is a graph of circuit
elements and connecting wires.  To perform parallel
simulation the graph is partitioned into pieces, after which the
simulation is run for a number of time steps.  Each time step has
three phases: calculation of each wire's current, distribution of charge to
the nodes connected to each wire, and updating of node voltages.

The entry function is {\tt simulate\_circuit} (line 15), which accepts a 
linked list of nodes contained in region {\tt rn} and a linked list of 
wires contained in region {\tt rw}.  The linked list elements themselves
are in region {\tt rl}.
When {\tt simulate\_circuit} is invoked it must have both read and
write privileges for the regions {\tt rl}, {\tt rw}, and {\tt rn}
(line 17); the type system statically verifies that
{\tt simulate\_circuit} only accesses these regions and their subregions.

Partitioning of the graph is done using {\em colorings} built in the
{\tt color\_circuit} function (line 18-19).  A coloring maps region
elements to colors (Core Legion uses integers), with each color corresponding
 to a new subregion
to be created.  Note that it is impossible to pick a single static
partitioning scheme that would do well for all graphs.  Dynamic
partitioning using colorings provides the programmer the flexibility
to make partitioning decisions based on input data.

The {\tt color\_circuit} function is responsible for specifying how the
circuit is to be partitioned.  The body of this function is not shown as the
ideal algorithm for partitioning data will be application specific.  The 
only restriction imposed by the Legion programming model is that the result of
the algorithm be provided in the form of colorings.  For our circuit example,
there are three colorings.  The first maps each node to the graph piece that owns it.
The third maps each wire to a graph piece that
owns one of its nodes.  The second coloring captures, for each piece $p$,
the nodes in other pieces on the boundary of $p$, often referred to as
{\em ghost nodes}.  
%We describe how ghost node regions are used in
%conjunction with the owned nodes shortly.  
Note that 
ghost nodes may be on the boundary of multiple pieces and therefore be included in
multiple ghost regions.  For conciseness, a bit of syntactic sugar is used here the
form of a {\em multicoloring}, which allows the mapping of an element to multiple 
colors.  Although multicolorings are not part of the Core Legion described in 
Section~\ref{sec:legioncore}, they can be easily implemented using a separate 
coloring for each color in the multicoloring.

After creating the colorings, the application partitions the node and wire
regions into {\em subregions} according to the colorings (lines 21-25).  For
simplicity, our example only partitions the circuit into two pieces.
Line 21 uses coloring {\tt pc.1} to partition {\tt rn} into {\tt rn0}, and {\tt rn1}.
Subregion {\tt rn0} will contain all locations with a color of 0 in {\tt pc.1},
while {\tt rn1} will contain all locations with a color of 1.  This
partitioning uses a coloring and results in {\em disjoint} subpartitions.
Constraints are introduced into the static environment describing both the
disjointness of the subregions (i.e. {\tt rn0 $*$ rn1}) and the inclusion of
each subregion in the original region (e.g. {\tt rn0 $\leq$ rn}).  Line
23 is similar, but uses a multicoloring, so the subregions may be {\em aliased},
and only the inclusion constraints are introduced into the static environment.

The circuit simulation uses an {\em allocate-then-partition} style of
computation, where a large data structure is first allocated and then
partitioned.  Legion also supports a {\em partition-then-allocate} style,
where empty regions are first partitioned and later populated with
data.  Both are useful; e.g., Sequoia \cite{Fatahalian06} and DPJ
\cite{Bocchino09} support static forms of allocate-then-partition and
partition-then-allocate, respectively.  Legion supports both
approaches dynamically.

After partitioning the circuit into pieces, the application creates
instances of {\tt CircuitPiece} (defined on lines 9-12) for each piece
(lines 28-33).  {\tt CircuitPiece} is a {\em region relationship}, a
bounded existential type.  Region relationships allow the programmer
to {\em pack} a group of regions and pointers into those regions together
and remember properties
about them such as disjointness and subregion relationships (lines 29
and 33).  The type system verifies the properties hold statically when
packing; when region relationships are {\em unpacked} the same
properties are reintroduced into the static checking environment
(lines 40-41).  A key feature underlying the soundness of the Legion
type system is that privileges cannot be packed in a region
relationship---privileges belong to functions. When a function unpacks
a region relationship it must already hold privileges for the new
regions it finds in the packed value.  For example, on line 40 the
function {\tt execute\_time\_steps} has read/write privileges for
region {\tt rn0} because it has read/write privileges for {\tt rn} and
the {\tt CircuitPiece} region relationship constraints ensure $\tt rn0
\leq rn$ (line 12).

The {\tt execute\_time\_steps} function (lines 37-48)  
also illustrates the importance of using different partitions to give
multiple views onto the same logical region.  Both {\tt
  calc\_new\_currents} and {\tt distribute\_charge} 
use the owned and ghost regions of a piece, which are from different partitions. In
{\tt calc\_new\_currents} these regions only need read
privileges, allowing both instances of {\tt calc\_new\_currents} to be
run in parallel.  In {\tt distribute\_charge} the
privilege is for a reduction which can also be done in parallel
because of the atomic and commutative nature of reductions.  Finally,
the {\tt update\_voltage} function modifies only the owned regions, permitting
each piece to be updated in parallel.  No
single partition of the nodes
describes these data sharing patterns.

%This function also demonstrates the benefit of being able to dynamically
%(re-)discover disjointness.  Safely running both instances of 
%{\tt calc\_new\_currents} in parallel depends on knowing that {\tt rw0} and
%{\tt rw1} in {\tt execute\_time\_steps} are disjoint.  Similarly, parallel
%execution of the instances of {\tt update\_voltage} requires knowing that
%{\tt rn0} is disjoint from {\tt rn1}.  In both cases, this knowledge was
%statically available in {\tt simulate\_circuit} and could have been captured
%in a region relationship, but it would have resulted in much more complicated
%code.
%Instead, an inexpensive dynamic disjointness check is performed by the Legion
%runtime, and the safety of parallel execution follows from the soundness of
%the type system.
The {\tt execute\_time\_steps} function also demonstrates the benefit of dynamically
being able to discover disjointness.  To run the two instances of 
{\tt calc\_new\_currents} in parallel requires knowing that {\tt rw0}
and {\tt rw1} are disjoint.  There is a similar requirement for parallel
execution of the two {\tt update\_voltage} calls with {\tt rn0} and {\tt rn1}.  In
both cases, this knowledge was statically available in {\tt simulate\_circuit}
and could have been captured in a region relationship at the cost
of much more complicated code.  Instead, an inexpensive dynamic disjointness
check by the Legion runtime will (re)discover the parallelism.

Listing~\ref{lst:circuit_leaf} shows the leaf functions for each phase.
Each function iterates over the list of wires or
nodes for its piece.  Each function specifies the region privileges it
requires (lines 2,14,23).  
%These privileges
%are statically checked to match the operations performed inside of
%each function (e.g. read and write).  
In the case of {\em reduce} the
privilege must also specify which reduction function is used
(line 14).

In addition to privileges, functions can also specify {\em coherence}
on regions.  Coherence specifies what updates the function may
observe from other functions using aliased regions.  
If not otherwise specified, coherence defaults to {\em exclusive}, 
meaning the function must appear to execute in program order relative to
other function calls.  
Line 14 in Listing~\ref{lst:circuit_leaf} shows
an example of {\tt atomic}, a relaxed coherence mode requiring
that operations to {\tt rn} and {\tt rg} appear atomic relative
to other functions using regions which may alias.  The most relaxed coherence
mode is {\tt simult}; simultaneous coherence allows concurrent access 
to the region by all functions that are using the region
in a simultaneous mode.  The interaction between {\em tasks} (which are functions chosen to execute in parallel) using the same
region with different coherence
modes is formalized in Section~\ref{sec:coherence}.

% This is a description of how the listings should be formatted.
% It can go anywhere before the listings.
\lstset{
  captionpos=b,
  language=Haskell,
  basicstyle=\scriptsize,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  stepnumber=1,
  escapechar=\#,
  keepspaces=true,
  belowskip=-10pt,
  literate={<}{{$\langle$}}1 {>}{{$\rangle$}}1,
  morekeywords={function,rr,int,float,bool,isnull,partition,as,downregion,upregion,reads,writes,rdwrs,reduces,read,write,reduce,using,unpack,pack,coloring,multicoloring,color,newcolor,atomic,simultaneous},
  deletekeywords={float,head,min,max}
}

\begin{lstlisting}[float={t},label={lst:circuit_leaf},caption={Circuit Leaf Functions}]
function calc_new_currents[rl,rw,rn,rg] ( ptr_list : wire_list<rl,rw,rn,rg>@rl ), 
      reads(rl,rw,rn,rg), writes(rw) : bool =
  if isnull(ptr_list) then true else
  let wire_node : wire_list<rl,rw,rn,rg> = read(ptr_list) in
  let wire : CircuitWire<rn,rg> = read(wire_node.1) in
  let in_node : CircuitNode = read(wire.1) in
  let out_node : CircuitNode = read(wire.2) in
  let current : float = (in_node.1 - out_node.1) /  wire.3 in 
  let new_wire : CircuitWire<rn,rg> = <wire.1,wire.2,wire.3,current> in
  let _ : CircuitWire<rn,rg>@rw = write(wire_node.1, new_wire) in
      calc_new_currents[rl,rw,rn,rg](wire_node.2)

function distribute_charge[rl,rw,rn,rg] ( ptr_list : wire_list<rl,rw,rn,rg>@rl ), 
      reads(rl,rw,rn), reduces(reduce_charge,rn,rg), atomic(rn,rg) : bool =
  if isnull(ptr_list) then true else
  let wire_node : wire_list<rl,rw,rn,rg> = read(ptr_list) in
  let wire : CircuitWire<rn,rg> = read(wire_node.1) in
  let _ : CircuitNode@rn = reduce(reduce_charge, wire.1, wire.4) in
  let _ : CircuitNode@(rn,rg) = reduce(reduce_charge, wire.2, wire.4) in
      distribute_charge[rl,rw,rn,rg](wire_node.2)

function update_voltage[rl,rn] ( ptr_list : node_list<rl,rn>@rl ), 
      reads(rl,rn), writes(rn) : bool = 
  if isnull(ptr_list) then true else
  let node_node : node_list<rl,rn> = read(ptr_list) in
  let node : CircuitNode = read(node_node.1) in
  let voltage : float = (node.3/node.4) in
  let new_node : CircuitNode = <voltage,node.2,node.3,node.4> in
  let _ : CircuitNode@rn = write(node_node.1, new_node) in
      update_voltage[rl,rn](node_node.2)

-- Reduction function for distribute charge
function reduce_charge ( node : CircuitNode, current : float ) : CircuitNode =
    let new_charge : float = node.3 + current in
        < node.1,new_charge,node.3,node.4>
\end{lstlisting}


