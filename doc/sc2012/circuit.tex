\section{Example: Circuit Simulator}
\label{sec:ex}

We begin by describing an example program
written in the Legion programming model.
Listing~\ref{lst:code_ex} shows code for an electrical
circuit simulation, which takes a collection of
wires and nodes where wires meet.  
At each time step the simulation calculates 
currents, distributes charges, and updates voltages.

The key decisions in a Legion program are how data is grouped into
regions and how  regions are {\em partitioned} into {\em
subregions}.  
%
%FIXME
% the following sentence oversimplifies; it's one of the goals, and regions tell us
% useful things about dependent computations as well
%
The goal is to pick an organization that makes explicit
which computations are independent.  
A {\tt Circuit}
has two regions: a collection of nodes and a collection of wires (line
3 of Listing~\ref{lst:code_ex}).\footnote{Note that all pointers declare the region to which they point.  For
example, the definition of {\tt Wire} (line 2) is parametrized on the region
{\tt rn} to which the {\tt Node} pointers in fields {\tt in\_nodes}
and {\tt out\_nodes} point.}
An efficient parallel
implementation breaks this unstructured graph into pieces that can be
processed (mostly) independently. An appropriate
region organization makes explicit which nodes and wires are
involved in intra-piece computation and, where wires connect different pieces,
which are involved in inter-piece computation.

% FIXME   We add here a description of how the colors print when printed greyscale.
%
Figure~\ref{sfig:part_fig:pvs} shows how the nodes
in a small graph might be split into three pieces.  Blue (lighter) nodes,
attached by wires only to nodes in the same piece, are 
{\em private} to the piece.  Red (darker) nodes, on the boundary of a
piece, are {\em shared} with (connected to) other pieces.
In the simulation, computations on the
private nodes of different pieces are independent, while
computations on the shared nodes require communication.  To make
this explicit in the program, we partition the nodes region into
private and shared subregions (line 18).  To partition a region, we
provide a {\em coloring}, which is a relation between the elements of
a region and a set of colors.  
%A partition is an object which given a
%coloring and a region $r$ contains for each color $c$ a subregion of
%$r$ containing all the elements of $r$ colored $c$. 
For each color $c$ in the coloring, the partition contains a subregion $r$
of the region being partitioned, with $r$ consisting of the elements colored $c$.  Note that the
partition into shared and private nodes is disjoint because each node has one color.

The private and shared nodes are partitioned again into private and shared nodes for each circuit piece (lines 21
and 23);  both partitions are disjoint.  There is another useful partition of the shared nodes:
for a piece $i$, we will need the shared nodes that border $i$ in other pieces of the circuit.
This {\em ghost node} partition (line 25) has two interesting properties.  First, it is a second partition of the shared nodes:
we have two views on to the same collection of data.  Second, the ghost node partition is {\em aliased}, meaning the subregions
are not disjoint: a node may border several different circuit pieces and belong to more than one ghost node subregion (thus, {\tt node\_neighbor\_map} on line 25 assigns more than one color to some nodes).
The private, shared, and ghost node subregions
for the upper-left piece of the example graph are shown in
Figures~\ref{sfig:part_fig:p_i}, \ref{sfig:part_fig:s_i}, and
\ref{sfig:part_fig:g_i} respectively.  

Figure~\ref{sfig:part_fig:tree} shows the final hierarchy of node
partitions and subregions. The $*$ symbol indicates a partition is disjoint. This
{\em region tree} data structure plays an important role in scheduling
tasks for out-of-order execution (see Section~\ref{sec:soop}).
The organization of the wires is much simpler: a single disjoint partition
that assigns each wire to one piece (line 16).

%
% FIXME
% Sean wants to use the term task through this paragraph; however we haven't defined it yet.
% I think it is OK as is since the term is introduced at the end of the paragraph.
%
Line 9 declares the main simulator function, which specifies the regions it 
accesses and the privileges and coherence it requires of those regions.
The {\tt RWE} annotation specifies that
the regions {\tt c.r\_all\_nodes} and {\tt c.r\_all\_wires}
are accessed with read-write privileges and {\em exclusive} coherence (i.e., no other
task can access these two regions concurrently or be reordered around this
task if they use either region).  
%The simulation 
%reads and writes all nodes and wires, and it must be done
%with exclusive access to ensure correct results.  
Privileges specify what
the function can do with the regions; coherence specifies what other
functions can do with the regions concurrently.  Functions that
declare their accessed regions, privileges, and coherence are called {\em tasks}
and are the unit of parallel execution in Legion.

Lines 31-57 perform the actual simulation by making three
passes over the circuit for each time step.  Each pass loops over an
array of pieces (constructed on lines 27-30 from the partitions),
spawning a task for each piece.  There are no explicit requests for
parallel execution ({\tt spawn} simply indicates a task call and does
not mandate parallel execution) nor is there explicit synchronization 
between the passes.  Which tasks can be run in
parallel within a pass and the required inter-pass dependencies are
determined automatically by the Legion runtime based on the region
access annotations on the task declarations.  The tasks spawned on
lines 32-34 are {\em subtasks} of the main {\tt simulate\_circuit}
task. A subtask can only access regions (or subregions) that its parent task
could access; furthermore, the subtask can only have permissions on a
region compatible with the parent's permissions.  
%This
%requirement plays an important role in our task scheduling algorithm
%(see Section~\ref{sec:soop}).

The  {\tt calc\_new\_currents} task reads and writes the wires subregion 
and reads the private, shared, and ghost node subregions for its piece.
The {\tt distribute\_charge} task reads the piece's 
wires subregion and updates all nodes those wires touch.  However,
rather than using read/write privilege for the nodes (which would
serialize these tasks for correctness), the task
uses reorderable reduction operations and 
atomic rather than exclusive access. The final task 
{\tt update\_voltages} writes the shared and private nodes for a piece
and reads the results of the previous task's reductions.

  
%Since the wire subregions are known to be disjoint, the write sets of invocations of $calc\_new\_currents$ do not overlap, and can
%therefore be safely run in parallel.


%As long as the
%runtime can guarantee to apply the reductions from multiple subtasks safely, it
%can run the subtasks themselves in parallel.  Each invocation of 
%$distribute\_charge$ will be delayed until the corresponding invocation of 
%$calc\_new\_currents$ has completed due to the read-after-write dependency on
%the corresponding wire subregion.  However, despite the apparent 
%write-after-read anti-dependency on the ghost node regions, $distribute\_charge$
%tasks will generally not have to wait on the the completion of the other
%$calc\_new\_current$ tasks.  If there is sufficient memory available to make
%two copies of those nodes, the runtime can allow $distribute\_charge$ tasks to
%start calculating a new version of the nodes while older $calc\_new\_currents$
%tasks are still referring to the older version, all completely transparently to
%the application code.

%Again, the disjointness of the $p_i$ and $s_i$
%node subregions allows the runtime to safely run these tasks in parallel.  In
%this case, the runtime does wait for the completion of all the tasks in the 
%previous pass.  The read-after-write dependence on the $p_i$ is a guaranteed
%conflict, but there is also potential overlap between the $s_i$ subregions
%being reduced to in the previous pass and the $g_i$ subregions being accessed
%in this pass.  Although not every pair of $s_i$ and $g_j$ conflict, the
%runtime knows that they were created from two independent partitioning
%operations and guarantees correctness by conservatively assuming they might
%conflict.

%\subsection{Data Types}
%\label{subsec:datatypes}

%The two basic data types in the circuit simulation are {\tt Node}s and
%{\tt Wire}s declared on lines 1 and 2 of Listing~\ref{lst:code_ex}.
%As is standard in region-based systems, allocating a {\tt Node}, {\tt
%  Wire}, or any value in the heap requires naming the region
%in which the value is allocated.  For example, {\tt new Node@r} returns
%a reference to a new value of {\tt Node} type in region {\tt r}.  
Listing~\ref{lst:code_ex} illustrates one way of constructing partitioned
data structures in Legion: populate a region with data (the example
makes use of whatever data has been allocated by the caller of {\tt simulate\_circuit})
and then partition it into subregions.  One can also first partition an empty region into subregions
and then allocate data in the subregions. 
%We use both approaches in our
%applications (see Section~\ref{sec:exp}).


% which guarantees that the two endpoints of
%a wire are always in the same region.
%Regions are first-class values in Legion and can be stored in heap
%data structures.  For example, a {\tt Circuit} (defined on line 3)
%holds the regions for all nodes and wires of the circuit and a {\tt
%  CircuitPiece} (defined on line 4) hold pointers to the private,
%shared, and ghost node regions as well as the private wires regions of its piece of
%the circuit.




