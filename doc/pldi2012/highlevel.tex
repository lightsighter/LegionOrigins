
\section{High-Level Runtime} 
\label{sec:highlevel}
The high-level runtime is responsible for guaranteeing the semantics 
of programs written in the Legion programming model while at the same
time extracting high performance.  This goal is challenging because
the high-level runtime must be capable of operating on top of many 
different architectures abstracted by the low-level runtime.  This 
includes those with very large latencies for transferring data such
as distributed memory clusters.  To hide these latencies, the high
level runtime uses a deffered model of execution made possible by the
event model of the low-level runtime.  By deffering execution, the 
runtime can hide the latencies of data and task movement and acieve
good throughput and high performance.

However, a deffered execution model presents challenges to implementing
the semantics of the Legion programming model.  Deffered execution
does not necessarily imply that tasks will be executed in the order in
which they are scheduled, but only in the order in which low-level
event dependencies are created.  To orchestrate the execution of Legion
programs in this environment the high-level runtime is architected
in a manner similar to an out-of-order hardware processor.  There are
multiple stages to executing a task which allows the runtime to extract
as much task-level parallelism as possible from an application whithin
the constraints of the Legion programming model.  We now investigate
each of these stages in further detail.

\subsection{Dependence Analysis} 
\label{subsec:depanalysis}
The first step in the execution of a task is dependence analysis.  When
a task is registered with the high level runtime the task call includes
information about both the logical regions that the task will require
when it is executed as well as the read-write access and coherence properties
for each of the regions.  The runtime uses this information to perform
dependence detection with each of the previous tasks that have been registered
in the same parent task.  Dependence detection only needs to be performed at
the scope of an enclosing task because the semantics of the Legion programming
model ensure that all tasks can only use regions which are subregions of
regions that the parent task uses.  This implies that if a child task were
to depend on a task at a higher level of the task tree, then the
parent task would also have had a dependence.  

Lemma: If a task {\tt t} with ancestor task {\tt p} depends on task {\tt t'}
a sibling task of {\tt p}, then task {\tt p} must depend on task {\tt t'}.

This property of the Legion programming model enables the runtime to only 
have to perform dependence analysis between tasks which share the same 
parent task.  By restricting dependence analysis to tasks which share the same
parent task, the runtime can perform dependence analysis locally and not
have to be concerned with tasks being created in other parts of the machine.

When performing dependence analysis the runtime relies on its knowledge about
the relationships between logical regions and partitions.  We first describe
the the components to dependence analysis in sections \ref{subsec:regiontree},
\ref{subsec:cohdep}, and \ref{absinsts}.  We then describe the algorithm for performing 
dependence analysis on this data structures in section \ref{subsec:depdetect}.

\subsubsection{Region Trees}
\label{subsec:regiontree}
To detect dependences between the tasks, the runtime leverages its knowledge
about logical regions and their partitions.  To create a new logical region
or partition, the application must invoke the corresponding call in the
runtime.  As these calls occur, the runtime constructs a data strcture
called a {\em region tree}.  A region tree describes the relationship between
logical regions and partitions.  A region tree contains two types of nodes:
region nodes and partition nodes.  Every region tree is rooted with a region
node and alternates between region and partition nodes at each corresponding level.
A region node represents a specific logical region and tracks the set of
partitions of that logical region.  Similarly a partition node represents 
a specific partitioning of a logical region and keeps track of the logical regions 
which are subregions of the partition.  

\subsubsection{Dependencies From Coherence Properties}
\label{subsec:cohdep}
To detect dependencies between any two tasks using a logical region, we leverage
the information contained in the access and cohrence properties provided by the programmer
for how the region is to be used.  Dependency checks are performed pair-wise between regions
to determine whether one task depends on the other.  When checking for
dependencies we first examine the access property.  If both tasks are reads, then 
there is never a dependence.  If at least one of them is a write we use table
\ref{tab:depsdetect} to determine whether there is a dependence.

\begin{center}
\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
             & Exclusive & Atomic   & Simultaneous & Relaxed \\ \hline 
Exclusive    & Dependence & Dependence & Dependence & Dependence \\ \hline
Atomic       & Dependence & Same Instance & Contingent & Contingent \\ \hline
Simultaneous & Dependence & Contingent & Same Instance & None \\ \hline
Relaxed      & Dependence & Contingent & None & None \\ \hline
\end{tabular}
\caption{Dependence detection cases when at least one access is a write.\label{tab:depsdetect}}
\end{table}
\end{center}
In this table, {\em Dependence} indicates that there will always be a dependence between
regions while {\em None} indicates that there will never be a dependence regardless
of mapping decisions.  In the case of {\em Same Instance} there is a dependence contingent
upon a mapping decision.  If both tasks map to the same physical instance (described in 
more detail in section \ref{subsec:instmang}) then there is no dependence, which if
they map to different instances there is a dependence as the system will have to generate
a copy between the two tasks running.

For the case of {\tt Contingent} there are two special cases that we handle.
One special case is Write-After-Read (WAR) dependencies.  For the WAR case, if neither coherence property
is {\em Exclusive} then we can ensure that different physical instances
are used to avoid an anti-dependence and increase parallelism.  If either property is {\em Exclusive} then the semantics
of the programming model mandate that we detect a dependence and serialize the task.

Another special case occurs if there is an {\tt Atomic} writer and either {\tt Simultaneous}
or {\tt Relaxed} reader.  In this case it is acceptable for the tasks to run in
parallel as it is valid for the reader to see updates form the writer, and the
reader will not violate the atomic access semantics of the writer.

\subsubsection{Abstract Instances}
\label{subsec:absinsts}
For the cases where tasks do not depend, they are allowed to create different physical
instances of the same logical region.  The decision to do this however does not
happen until the mapping phase of a task execution.  However, we need to have a place
holder that is capable of representing all the possible valid physical instances
from which to choose even if those instances do not exist because the prior
tasks have not been mapped.  We call these place holders {\tt abstract instances}.

Abstract instances represent the set of possible valid physical instances which a
task will be capable of selecting from or adding to when it is mapped.  The value
of abstract instances is that they allow us to remember which physical instances
are valuable while continuing to perform dependence detection on newly registered
tasks.  In the analogy to out-of-order processors abstract instances allow us
to perform the same operation as register-renaming of instructions to avoid
anti-dependences and to remember actual dependences even though our dependence detection
analysis may run ahead of the actual execution of tasks.

\subsubsection{Dependence Detection}
\label{subsec:depdetect}
When a task is registered, the runtime
will perform dependence detection for every region that the task requests with
prior tasks that have been registered in the same parent task context.  Since
tasks are registered in program order we will always perform dependence detection
in program order which guarantees correctness even if tasks are executed out-of-order.  
Performing dependence detection in program order is also annalogous to how an out of order
processor works.  Out-of-order processors load instructions in order and only after
analyzing their instructions and register dependencies do they execute instructions
out of order.  The regions that a task requires are annalogous
to the registers listed in an instruction and the goal of the high level
runtime is to detect the same dependences and use them to enforce scheudling constraints
upon a task.  However, there is a catch to this analogy: in the Legion programming
model two different logical regions are not necessarily independent from each other.  In
order to do the dependence detection we rely on the forest of region tree data structures to be
able to detect dependences between tasks which require different logical regions.

To perform dependence detection, we first find the {\em root region} for the 
requested region.  The root region is the ancestor region for which the parent task
has privliges.  Note that the semantics of the Legion programming model state that
we must always be able to find such a root region or else an error is thrown.  

Once we have found the root region for the requested region, we then proceed to walk
down the region tree from the node corresponding to the root region to the node
for the requested region.  At each node, we determine if any potential dependences exist.
Determining whether dependencies exist involves checking information both about the current
node being visited as well as whether there are any potential dependences farther down
in the region tree.  We now describe the decision procedure performed at both region
nodes and partition nodes in the region trees.

For region nodes, we begin by checking if there are any tasks actively using the logical
region which we depend.  We determine this using the coherence dependence analysis described 
in section \ref{cohdep}.  For each dependence that is found, we record that the previous
task must have completed prior to the start of the task we are registering.  After performing
dependence analysis we then check if there is an open partition which may contain additional
dependencies.  If the open partition is the partition we intend to traverse then we
simply continue the traversal.  However, if the partition we want is not open then we must
first {\em close} up any open partition and then open the partition we are going to
traverse.  The act of closing a partition involves traversing the region tree and recording
all active tasks as dependences.  The reason for this is that we have no disjointness
information about tasks that use different partitions and therefore to be safe must register
all task touching subregions as dependences.  After closing any open partitions, we
can then open the partition on our path and continue the traversal.

When we reach our target logical region we again perform the coherence dependence analysis.
We also close up any open partitions as they might contain tasks which could contain
a dependence.  If our task was dependent on all of the previous active tasks in a region
we say that our task was a {\em dominator} of all the previous active tasks.  We can
then clear the list of active tasks and replace it with the current task.  This is safe since
any later tasks that depend on the current task will have a transitive dependence 
through the dominator task.  If our task is not a dominator task, then we simply append
it to the list of active tasks making it possible for future task registrations to detect
it as a dependence.

When traversing partition nodes we perform a similar, but less complicated analysis.  There
are two cases to consider for partition nodes: disjoint partitions and aliased partitions.
In the case of disjoint partitions, we simply open the subregion that we intend to traverse
and then traverse it.  This is correct because we know by definition that any two logical
subregions in a disjoint partition are disjoint and therefore a dependence can never exits
between them.

For aliased partitions we have to perform an additional dependence analysis.  At every
aliased partition node we maintain a list of tasks active in all of the subregions.  We
perform a coherence dependence analysis between every pair of tasks regardless of the subregion
they are accessing to see if a dependence could exist.  We do this regardless of the subregion
because we have no disjointness information in an aliased partition.  If there are any
potential dependences, we close up all the subregions and then open the subregion required.  If
no dependence was detected then we open the subregion we need, append the task to the list
of active tasks, and continue the traversal.  Allowing multiple open subregions in an aliased
partition is safe because we performed an analysis between all active tasks regardless of
whether the particular subregions were aliased or not.

\subsubsection{Implicit Dataflow Construction}
\label{subsec:dataflow}
In addition to computing the dependences between tasks, we also have to compute where
a task is permitted to acquire its data from when it goes to execute.
It is insufficient to simply record the producer task as there
may be multiple active tasks for a logical region, all of which may produce different physical
instances.  In addition, it is likely that the producer tasks have not been mapped
yet which makes it impossible to know the set of physical instances from which to pull data.

To solve this problem we create the notion of an {\em abstract instance}.  An abstract instance
is a place holder for the set of physical instances from which a task can choose to pull
data when it creates its own physical instances before executing.  For each region in
a task we record the source and destination abstract instances that correspond to the physical
instances from which a task will read its input data and then place its output data.
When the task is mapped as described in section \ref{subsec:mapping} we will update these
abstract instances with the actual physical instances chosen.

To determine the source and destination abstract instances of a region, we keep track of 
the previous abstract instances that were observed on the way to our target region node
when performing dependence analysis.  These abstract instances describe the set of physical
instances from which valid data for our task can be chosen when it is mapped.  If a region
subtree must be closed then all the abstract instances in that subtree are marked closed
so no additional tasks can register them as sources.  We can also chose to create a new
abstract instance to replace a currently valid abstract instance to avoid a WAR dependence.
By creating an abstract instance we will force the mapper to choose a different physical
instance from the existing ones when mapping is performed.

Abstract instances correspond to a dynamic construction of the dataflow graph.  Each abstract
instance is a node in the dataflow graph corresponding to a specific version of a logical 
region and its instantiations as physical regions.  Tasks are then edges between abstract
instances indicating operations on different versions of data.  By computing this information
during dependence detection, we can record it before tasks begin executing out of order allowing
us to guarantee program correctness in the presence of out of order task execution.  Continuing
the analogy with out-of-order processors, this implicit construction of the dataflow graph
is annalogous to register renaming which is done in order, but allows the processor to remove
WAR dependences and execute instructions in parallel that may have had false dependences.

\subsubsection{Putting It All Together: Circuit Example}
\label{subsec:cirdependence}
To illustrate concretely how dependence analysis works, we describe the steps for
performing dependence analysis for running circuit example.  When the {\tt simulate\_circuit}
task begins executing there are two region trees rooted at the logical
regions for {\tt all\_nodes} and {\tt all\_wires}.  For conciseness we will only
show the region tree for {\tt all\_nodes}.  At the start of the task, there exists
a single abstract instance at the root node representing the valid set of physical
instances from which sub tasks will be allowed to pull data.  The state of the
system at this point can be observed in figure \ref{fig:circdep1}.

When each of the {\tt compute\_new\_current} tasks are issued they must perform dependence
analysis for two node regions.  In the {\tt private\_node} case they will first open
up the {\tt p\_nodes\_pvs} partition and then open up the {\tt p\_pvt\_nodes} sub partition
for each of their subregions.  Since all of the private regions use a different subregion
of a disjoint partition for the private node accesses, no dependence is detected.  Similarly,
when all the dependence analysis is performed for the {\tt p\_ghost\_nodes} subregions,
each of the necessary partitions are opened.  There is no dependence with any of the private
nodes since all the ghost nodes traverse a different sub region of the disjoint partition
{\tt p\_nodes\_pvs}.  There is also no dependence between any tasks even though the
{\tt p\_ghost\_nodes} partition is aliased because all the ghost accesses are read-only.
The state of the region tree after dependence analysis has been performed for the
{\tt compute\_new\_current} nodes can be seen in figure \ref{fig:circdep2}.

In the second phase of the computation, all of the {\tt distribute\_charge} tasks
will be issued.  For these tasks they will first analyze traverse the region tree
for their subregions containing their private nodes.  The path to these regions are
already open, but when the dependence analysis arrives at the specific private node subregion it will
discover a dependence between the task and the {\tt compute\_new\_currents} task that operated
on that region previously.  This dependence will be recorded and the 
{\tt distribute\_charge} task will be made the new active task as it dominated the
previous task.  For the ghost node regions, the dependence analysis will discover that the
there is a dependence between each of the {\tt distribute\_charge} tasks and all of the
{\tt compute\_new\_current} tasks because the {\tt distribute\_charge} tasks are
performing writes and the partition is aliased.  However, the runtime will detect this as
anti-dependences which are resolved by forcing the {\tt distribute\_charge} tasks to
create new valid abstract instances rather than use the same abstract instances as
each of the {\tt compute\_new\_current} tasks.  There are no dependences detected
between each of the {\tt distribute\_charge} tasks on the ghost regions because
the {\tt Simultaneous} coherence property permits multiple tasks to be using the same
partition at the same time even if the subregions they are accessing are aliased.  The
state of the region tree after dependence analysis for {\tt distribute\_charge} can
be seen in figure \ref{fig:circdep3}.

For the final phase of the computation, all of the {\tt update\_voltage} tasks
are issued requiring their private node region and their shared node region.  Again
the private node paths are already open, but a WAR dependence is detected which
requires a new abstract instance to be created to avoid a dependence.  The
shared nodes present a more interesting case.  The first shared node to be registered
will detect a dependence with every prior task since it must open the shared partition
instead of the ghost partition.  A new abstract instance is created at the 
{\tt all\_shared} node and copies are recorded for each valid instance in the ghost partition
back to the new {\tt all\_shared} abstract instance.  After this is complete, the
ghost partition can be opened.  Each corresponding {\tt update\_voltage} task will record
a dependence on the copy operation to close ghost partition as it traverses the {\tt all\_shared}
node ensuring that there is no requirement on the ordering of {\tt update\_voltage}
tasks.  The state of the region tree after this phase can be seen in figure \ref{fig:circdep4}.

\subsection{Mapping}
\label{subsec:mapping}
After tasks have undergone dependence analysis, the tasks are placed on either a ready
queue to be mapped or waiting queue.  A task is placed on the ready queue if all the tasks with
which it registered a dependence have mapped.  If any of the task's dependences have
not mapped then the task is placed on the waiting queue.  The task also registers
itself with the task it depends on so that when the dependent task is mapped it will notify
the task on the waiting queue. Tasks are mapped based on the partial ordering created by 
their data dependences.  By mapping tasks based on data dependences we guarantee the 
semantics of the programming model, while still allowing tasks to execute out of order.

When the scheduler is invoked, it will pull a series of tasks off the ready queue, map them,
and then issue them to be executed by the low level runtime.  The process of mapping a task
consists of several different queries made by the runtime to an application level object
called a {\em mapper}.  A mapper is an object which has control over how both tasks and
data can be placed in the machine.  This idea is a dynamic instantiation of the idea
initially presented in \cite{Fatahalian06} of creating mapping files for specifying how
tasks and data were placed in a memory hierarchy.  Rather than rely on a static mapping
we opted for a dynamic interface that could make decisions both based on application specific
input data as well as the machine specifics both at start-up and throughout the execution
of an application.  The interface for a mapper is described in section \ref{subsec:mapinter}.

\subsubsection{Mapper Interface}
\label{subsec:mapinter}
The interface for a mapper is designed to give the programmer control over how data and tasks
are placed in the machine.  Every mapper must support the following function calls:

\begin{itemize}
\item {\tt rank\_initial\_region\_locations} - When a new logical region is created, the runtime
will ask the mapper to give a ranking of physical memories of where to place the inital instance.

\item {\tt rank\_initial\_partition\_locations} - When a new partition is created, the mapper
must provide a ranking of memory locations for each of the new sub partitions.

\item {\tt select\_initial\_processor} - Given a task to be mapped, select a processor to
send this task to for mapping.  

\item {\tt target\_task\_steal} - select a processor to attempt task stealing.  This is explained
in more detail in section \ref{subsec:steal}.  

\item {\tt permit\_task\_steal} - also covered in section \ref{subsec:steal}

\item {\tt map\_task\_region} - Given a specific task and the logical region to be mapped, select
a source physical instance for the data, and provide a ranking of memories in which to create
or reuse an existing physical instance.

\item {\tt rank\_copy\_targets} - Given a task and a set of valid instances, generate a set of
rankings for new memories in which to place new phsyical instances.  This operation is used to
manage the copy operations performed when closing a subtree as they allow the programmer to
eagerly migrate regions through the memory hierarchy as copy ups are occuring.

\item {\tt select\_copy\_source} - Given a set of currently valid source instances and destination
physical instance, select the instance from which a copy should be made.
\end{itemize}

These eight task calls give the programmer complete control over where all tasks are executed, and
where all data is placed within the memory hierarchy.  While this interface does give the programmer
complete control over data and task movement, it will never allow the programmer to affect the
correctness of the program.  The highlevel runtime still manages all the task dependences and the
ordering in which tasks are executed and copies are performed.  Therefore, regardless of choices
made by any mapper, the resulting program will always be executed correctly.

In order to make it easy for programmers to write programs, we provide an initial implementation 
of the mapper interface that uses simple heuristics to place task and data.  This makes it easy
for the programmer to get an initial program running, while allowing them to write their own
mapper to tune for application or machine specific scenarios at a later time.

Lastly, the runtime supports multiple mappers in use in the same application.  For each task 
call the programmer can choose a mapper to be used for the runtime calls made inside that
task.  This allows for the creation of library code which provides their own mappers indpendent
from any application level mappers that may be created.

\subsubsection{Task Stealing for Load Balance}
\label{subsec:steal}
In order to achieve load balancing in an application the runtime supports an interface
for task stealing.  Our task stealing algorithm is loosely based on the stealing algorithm
used by Cilk\ref{CILK95} with guidance from the mapper about how stealing should be
performed.  Tasks are initially placed on the ready queue on the processor in which they
were registered.  However, a task can be moved to a different processor either by a mapper
explicitly choosing to relocate it, or by a steal request from another processor.

When a processor notices that it has no work on its ready queue, it will inquire into each
mapper asking for a target processor to attempt a steal via the {\tt target\_task\_steal}
call in the mapper interface.  A mapper can either respond with
a target processor, or indicate that no steal should be attempted.  The runtime will then
send steal requests to each processor on behalf of the specific mapper.  

When a steal request is received at a processor it will determine the mapper(s) that generated
the list of steal requests.  For each mapper that issued a steal request, the runtime will
give to the local instance of that mapper a list of the tasks in the ready queue that that
specific mapper is responsible for mapping.  The mapper must then determine the set which
it is going to permit to be stolen if any.  Any tasks that the mapper marks as stolen are
sent back to the stealing processor.  By only allowing mappers to make steal requests about
the tasks they own, we are giving the mappers complete control over where tasks are mapped.
In addition, we are also giving the mappers control over how steal requests are made for
the case where the mapper has better knowledge about where additional work is than the 
runtime.  Finally, note that this mapping interface is sufficiently powerful to allow a user to
disable all stealing if desired, and to map tasks to explicit processors with no interference
from the runtime.

\subsection{Execution}
\label{subsec:execution}
Once task mapping is complete the high level runtime performs the necessary operations
to be issue the task to the low level runtime.  It first creates any new physical instances
for the regions which the task requires.  It then issues the necessary copies required
for the task to execute contignent on the tasks producing the source physical instances
have finished.  Then it issues the task itself to the low level processor contingent
on all the copies for the task haveing finished as well as the the set of tasks
on which the issuing task dependends having finished.  These dependences are all encoded
using event dependencies in the low level runtime.  Once all the necessary events
have triggered the task will be run.

Once a task starts running, the runtime is asked to create the region accessors for
the task.  The runtime investigates whether or not fast accessors can be created for
each of the regions that the task requires.  Fast accessors are described in more
detail in section \ref{sec:fastaccess}.  If fast accessors can be created for every
region in a task, then the runtime chooses to execute a task instantiated with fast
region accessor methods.  However, if any region can not be specialized into a fast
accessor the runtime defaults to a task instantiation that relies on {\tt get} and
{\tt put} operations to access regions.

The runtime will also return to the executing task a unique identifier describing the
context in which the task is executing.  All calls into the runtime require the task
to pass this context identifier which is what permits the runtime to know the context
for incoming calls.  

\subsection{Task Completion}
\label{subsec:cleanup}
We define a task to have {\em completed} once the actual code for the task has completed.
We also define a task to have {\tt terminated} once all of the operations for a task
have finished.  We can only consider a task to be terminated once all of the regions that
the task accessed reflect all the changes made by a task and its child tasks.  However,
even after a task has completed it is possible for child tasks of that task
not to have been executed or even mapped.  We describe how we handle this scenario in
section \ref{subsec:restore}.  We also describe how physical instances are garbage
collected in seciont \ref{subsec:garbage}.

The runtime needs to know the mappings for
the child tasks as it must issue restoring copy operations so that the set of regions
that the task operated on are up to date with any data generated by child tasks.  

\subsubsection{Restoring Copy Operations}
\label{subsec:restore}
An underlying assumption of the mapping process is that the resulting regions that
a task access will represent the data produced by that task.  However, this may not
be true if a task has child tasks which also modify subregions of the parent task.
In order to handle this issue the runtime must compute the necessary copy operations
from child task physical instances back to the parent task's physical instances.  To
compute these copy operations, the runtime first will wait until all the child tasks
have been mapped.  The runtime then issues a close operation to the each of the parent
task's region trees.  These close operations record all of the necessary copies to
restore the parent task's regions as well as the child tasks on which these copies
are contingent.  We call these copy operations at the end of a task {\em restoring copies}.
The runtime then issues these copies with the required dependences
and marks the task terminated once the restoring copy operations are complete.

\subsubsection{Garbage Collecting Physical Instances}
\label{subsec:garbage}
In addition to cleaning up task execution, the runtime must also deal with the problem
of garbage collecting physical instances of regions.  Physical instances must be
garbage collected since they are not visible to the application and would present
a correctness problem if deleting instances was an option for the mapper.  
Garbage collecting physical instances is a challenging problem
due to both the distributed environment of the machine and the deffered execution
model.  To solve this problem we employ a two-level reference counting scheme.  We
reference count both abstract instances and as well as physical instances.  

An abstract instance maintains a count of the number of tasks that have referenced 
that abstract instance, but have not yet been mapped.  An abstract instance also tracks
whether or not it is open or closed.  An abstract instance is open if it is still
a valid abstract instance in a region tree.  An abstract instance is marked closed
once it is no longer a valid abstract instance and can therefore no longer be used
by any new tasks that are registered.

When a task is mapped it notifies an abstract instance that it has been used and can
decrement its reference count.  The task mapping also notifies the abstract instance
of the specific physical instance that was used and whether or not this use invalidates
previous physical instances of the abstract instance.  The abstract instance maintains
a reference count for each physical instance of it that has been made as well as
the set that are currently valid physical instances to be used for mapping.  When a 
task terminates, it notifies the abstract instances that it used to decrement the
reference counts for the task's specific physical instances.

There are two conditions under which a physical instance can be garbage collected.  The
first condition is when a physical instance is not a valid physical instance of
an abstract instance and its reference count is zero.  This condition occurs once no
more tasks are using a physical instance and it can longer be mapped by any task.  The
second condition is when an abstract instance is both closed and has a reference count
of zero as well as the physical instance's reference count being zero.  This handles the
case when no more tasks can be set to use an abstract instance, there are no more tasks
to map to an abstract instance, and there are no current tasks using the physical
instance.

While reference counting normally doesn't allow for garbage collection of all data structures
it will always work in this case because both our tasks and our regions are organized as 
trees which ensures that it is impossible to create cyclic region references in our system.


