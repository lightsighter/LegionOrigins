
\section{High-Level Runtime} 
\label{sec:highlevel}
In this Section we describe a high-level runtime system
that is designed to implement the Legion programming
and execution models while remaining machine agnostic for portability.  
The high-level runtime system operates on top of the 
low-level runtime which hides machine specific details
and is described in detail in Section \ref{sec:lowlevel}.

The biggest challenge to implementing the high-level
runtime is to avoid the latencies associated with distributed
memory systems and deep memory hierarchies.  To aid
in solving this problem, the high-level runtime employs two useful 
scheduling techniques.  The first technique was proved
in Section \ref{sec:exec}: all scheduling decisions for a
task can be made locally in the context in which the
task was created.  The second technique is a {\em deferred execution model} 
that allows the runtime to hide the latencies associated
with moving data through deep memory hierarchies.  A
deffered execution model decouples the issuing of operations
from when they are actually performed.  This model is supported
by features in the low-level runtime that are discussed in Section \ref{sec:lowlevel}.

%The high-level runtime is responsible for guaranteeing the semantics 
%of programs written in the Legion programming model while at the same
%time extracting high performance.  This goal is challenging because
%the high-level runtime must be capable of operating on top of many 
%different architectures abstracted by the low-level runtime.  This 
%includes those with very large latencies for transferring data such
%as distributed memory clusters.  To hide these latencies, the high
%level runtime uses a deffered model of execution made possible by the
%event model of the low-level runtime.  By deffering execution, the 
%runtime can hide the latencies of data and task movement and acieve
%good throughput and high performance.

A deffered execution model however presents challenges to implementing
the semantics of the Legion programming model.  Deffered execution
does not necessarily imply that tasks will be executed in the order in
which they are scheduled, but only in the order in which dependences
are expressed.  To orchestrate the execution of Legion
programs in this environment the high-level runtime is architected
in a manner similar to an out-of-order hardware processor, but at
a coarser granularity with tasks corresponding to instructions and
regions corresponding to registers.  There are multiple stages to 
executing a task which allows the runtime to extract
as much task-level parallelism as possible from an application whithin
the constraints of the Legion programming model.  To illustrate
the implementation of the high-level runtime we now describe the
details of how each stage of task execution is performed.

\subsection{Dependence Analysis} 
\label{subsec:depanalysis}
The first step in the execution of a task is dependence analysis.  When
a task is registered with the high-level runtime the task call includes
information about both the logical regions that the task will require
when it is executed as well as the read-write privileges and coherence properties
for each of the regions.  The runtime uses this information to perform
dependence detection with each of the previous tasks that have been registered
in the same context.  Dependence detection only needs to be performed at
the scope of an enclosing context as was proved in Section \ref{sec:exec}. 

We initially describe how dependences are detected from logical regions
in Section \ref{subsec:cohdep}.  We then describe the algorithm that
is used to perform dependence detection in Section \ref{subsec:depdetect}.
Section \ref{subsec:dataflow} describes how data is versioned correctly
during deffered execution.  For concreteness we describe how dependence
analysis is performed for the circuit simulation from Section \ref{sec:ex}.
%When performing dependence analysis the runtime relies on its knowledge about
%the relationships between logical regions and partitions.  We first describe
%the the components to dependence analysis in sections \ref{subsec:regiontree},
%\ref{subsec:cohdep}, and \ref{absinsts}.  We then describe the algorithm for performing 
%dependence analysis on this data structures in section \ref{subsec:depdetect}.
%\subsubsection{Region Trees}
%\label{subsec:regiontree}
%To detect dependences between the tasks, the runtime leverages its knowledge
%about logical regions and their partitions.  To create a new logical region
%or partition, the application must invoke the corresponding call in the
%runtime.  As these calls occur, the runtime constructs a data strcture
%called a {\em region tree}.  A region tree describes the relationship between
%logical regions and partitions.  A region tree contains two types of nodes:
%region nodes and partition nodes.  Every region tree is rooted with a region
%node and alternates between region and partition nodes at each corresponding level.
%A region node represents a specific logical region and tracks the set of
%partitions of that logical region.  Similarly a partition node represents 
%a specific partitioning of a logical region and keeps track of the logical regions 
%which are subregions of the partition.  
\subsubsection{Dependencies From Coherence and Privileges}
\label{subsec:cohdep}
To detect dependencies between any two tasks using a logical region, we leverage
the information contained in the privileges and cohrence properties provided by the
task call.  Dependency checks are performed pair-wise between regions
to determine whether one task depends on the other.  When checking for
dependencies we first examine the region privileges.  If both tasks have reads privileges, then 
there is never a dependence.  If at least one task has write privileges we use table
\ref{tab:depsdetect} to determine whether there is a dependence.
\begin{center}
\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
             & Exclusive & Atomic   & Simultaneous & Relaxed \\ \hline 
Exclusive    & Dep & Dep & Dep & Dep \\ \hline
Atomic       & Dep & Same & Cont & Cont \\ \hline
Simultaneous & Dep & Cont & Same & None \\ \hline
Relaxed      & Dep & Cont & None & None \\ \hline
\end{tabular}
\caption{Dependence detection cases when at least one access has a write privilege. {\tt Dep}
indicates a dependence is detected.  {\tt Same} indicates a dependence is detected
only if the task uses different instances.  {\tt Cont} means that the dependence
is contingent upon the permissions.  {\tt None} means there will never
be a conflict.\label{tab:depsdetect}}
\end{table}
\end{center}
%In this table, {\em Dependence} indicates that there will always be a dependence between
%regions while {\em None} indicates that there will never be a dependence regardless
%of mapping decisions.  In the case of {\em Same Instance} there is a dependence contingent
%upon a mapping decision.  If both tasks map to the same physical instance (described in 
%more detail in section \ref{subsec:instmang}) then there is no dependence, which if
%they map to different instances there is a dependence as the system will have to generate
%a copy between the two tasks running.
For the case of {\tt Contingent} there are two special cases that we handle.
One special case is Write-After-Read (WAR) dependencies.  For the WAR case, if neither coherence property
is {\em Exclusive} then we can ensure that different physical instances
are used to avoid an anti-dependence and increase parallelism.  If either property is {\em Exclusive} then the semantics
of the programming model mandate that we detect a dependence and serialize the task.

Another special case occurs if there is an {\tt Atomic} writer and either {\tt Simultaneous}
or {\tt Relaxed} reader.  In this case it is acceptable for the tasks to run in
parallel as it is valid for the reader to see updates form the writer, and the
reader will not violate the atomic access semantics of the writer.

%\subsubsection{Abstract Instances}
%\label{subsec:absinsts}
%For the cases where tasks do not depend, they are allowed to create different physical
%instances of the same logical region.  The decision to do this however does not
%happen until the mapping phase of a task execution.  However, we need to have a place
%holder that is capable of representing all the possible valid physical instances
%from which to choose even if those instances do not exist because the prior
%tasks have not been mapped.  We call these place holders {\tt abstract instances}.

%Abstract instances represent the set of possible valid physical instances which a
%task will be capable of selecting from or adding to when it is mapped.  The value
%of abstract instances is that they allow us to remember which physical instances
%are valuable while continuing to perform dependence detection on newly registered
%tasks.  In the analogy to out-of-order processors abstract instances allow us
%to perform the same operation as register-renaming of instructions to avoid
%anti-dependences and to remember actual dependences even though our dependence detection
%analysis may run ahead of the actual execution of tasks.

\subsubsection{Dependence Detection}
\label{subsec:depdetect}
When a task is registered, the runtime
will perform dependence detection for every region that the task requests with
prior tasks that have been registered in the same context.  Since
tasks are registered in program order we will always perform dependence detection
in program order which guarantees correctness even if tasks are executed out-of-order.  
Performing dependence detection in program order is annalogous to how an out of order
processor works.  Out-of-order processors load instructions in order and only after
analyzing their instructions and register dependencies do they execute instructions
out of order.  The regions that a task requires are annalogous
to the registers listed in an instruction and the goal of the high-level
runtime is to detect the same dependences and use them to enforce scheudling constraints
upon a task.  However, there is a catch to this analogy: in the Legion programming
model two different logical regions are not necessarily independent from each other.  In
order to perform dependence detection we rely on knowledge of the forest of regions to be
able to detect dependences between tasks which require different logical regions.

To perform dependence detection, we first find the {\em root region} for the 
requested region.  The root region is the ancestor region for which the parent task
has privliges.  Once we have found the root region for the requested region, we then proceed to walk
down the region tree from the root region to the node
for the requested region.  At each node, we determine if any potential dependences exist.
Determining whether dependencies exist involves checking information both about the current
node being visited as well as whether there are any potential dependences farther down
in the region tree.  

When traversing a region node, we begin by checking if there are any tasks actively using
the node.  We determine this using the pair-wise dependence analysis described 
in Section \ref{subsec:cohdep} to detect dependences between the region we are looking for and the
region represented by the current node.  For each dependence that is found, we record that the previous
task must have completed prior to the start of the task we are registering.  

After performing
dependence analysis we then check if there is an open partition which may contain additional
dependencies.  If the open partition is the partition we intend to traverse then we
simply continue the traversal.  However, if the partition we want is not open then we must
first {\em close} up any open partition.   The act of closing a partition 
involves traversing the region tree and recording
all active tasks as dependences.  The reason for closing a partition is that we have no disjointness
information about tasks that use different partitions and therefore to be safe must register
all task touching subregions as dependences.  After closing any open partitions, we
can then open the partition on our path and continue the traversal. 

When we reach our target logical region we again perform the pair-wise dependence analysis.
We also close up any open partitions as they might contain tasks which could contain
a dependence.  If our task was dependent on all of the previous active tasks in a region
we say that our task was a {\em dominator} of all the previous active tasks.  We can
then clear the list of active tasks and replace it with the current task.  This is safe since
any later tasks that depend on the current task will have a transitive dependence 
through the dominator task.  If our task is not a dominator task, then we simply append
it to the list of active tasks making it possible for future task registrations to detect
it as a dependence.

When traversing partition nodes we perform a similar, but less expensive analysis.  There
are two cases to consider for partition nodes: disjoint partitions and aliased partitions.
In the case of disjoint partitions, we simply open the subregion that we intend to traverse
and then traverse it.  This is correct because we know by definition that any two 
subregions in a disjoint partition are disjoint and therefore a dependence can never exits
between them.

For aliased partitions we have to perform an additional dependence analysis.  At every
aliased partition node we maintain a list of tasks active in all of the subregions.  We
perform a pair-wise dependence analysis between every pair of tasks regardless of the subregion
they are accessing to see if a dependence could exist.  We do this regardless of the subregion
because we have no disjointness information in an aliased partition.  If there are any
potential dependences, we close up all the subregions and then open the subregion required.  If
no dependence was detected then we open the subregion we need, append the task to the list
of active tasks, and continue the traversal.  Allowing multiple open subregions in an aliased
partition is safe because we performed an analysis between all active tasks regardless of
whether the particular subregions were aliased or not.

\subsubsection{Managing Data Versions}
\label{subsec:dataflow}
One aspect of the deffered execution model is that it allows dependence analysis to run
ahead of the actual execution of tasks.  As a result the runtime must maintain information
about which {\em physical instances} of a logical region a task is allowed to use when executing.
A physical instance is a copy of the data in a logical region.  There can be multiple
physical instances of a logical region.  It is the responsibility of the high-level runtime
to ensure tasks use instances corresponding to the appropriate version of data.

%In addition to computing the dependences between tasks, we also have to compute the version
%of data that a task is permitted to use when it goes to execute since the computation.
%It is insufficient to simply record the producer task as there
%may be multiple active tasks for a logical region, all of which may produce different physical
%instances.  In addition, it is likely that the producer tasks have not been mapped
%yet which makes it impossible to know the set of physical instances from which to pull data.

In an out-of-order processor this problem is solved by register-renaming which maps logical
registers to a larger set of physical registers.  Our solution employs a similar technique
that maps logical regions to a larger set of {\em abstract instances}.  An abstract instance
is a place holder for the set of physical instances which a task will reference.  We call these
abstract instances because we may not know the actual physical instances until a task's predecessors
are mapped as described in Section \ref{subsec:mapping}.  For each region in
a task we record the source and destination abstract instances that correspond to the physical
instances from which a task will read its input data and then place its output data.
%When the task is mapped as described in section \ref{subsec:mapping} we will update these
%abstract instances with the actual physical instances chosen.

To determine the source and destination abstract instances of a region, we keep track of 
the previous valid abstract instances that were observed while traversing the region tree
during dependence analysis.  Every region node in the region tree that has active tasks
has a valid abstract instance.  Every time we close a region tree we close all the valid
abstract instances.  When a node is reopened it must then create a new abstract instance
to reflect that new physical instances must be created.
%If a region
%subtree must be closed then all the abstract instances in that subtree are marked closed
%so no additional tasks can register them as sources.  We can also chose to create a new
%abstract instance to replace a currently valid abstract instance to avoid a WAR dependence.
%By creating an abstract instance we will force the mapper to choose a different physical
%instance from the existing ones when mapping is performed.

Abstract instances correspond to a dynamic construction of the dataflow graph.  Each abstract
instance is a node in the dataflow graph corresponding to a specific version of a logical 
region and its instantiations as physical regions.  Tasks are then edges between abstract
instances indicating operations on different versions of data.  By computing this information
during dependence detection, we can record it before tasks begin executing out of order allowing
us to guarantee program correctness.  
%Continuing
%the analogy with out-of-order processors, this implicit construction of the dataflow graph
%is annalogous to register renaming which is done in order, but allows the processor to remove
%WAR dependences and execute instructions in parallel that may have had false dependences.

\subsubsection{Circuit Execution Example}
\label{subsec:cirdependence}
To illustrate concretely how dependence analysis works, we describe the steps for
performing dependence analysis for circuit example from Section \ref{sec:ex}.  
For conciseness we will only show the node region tree.  At the start of the task, there exists
a single abstract instance at the root node representing the valid set of physical
instances from the parent task. 

When each of the {\tt compute\_new\_current} tasks are issued they must perform dependence
analysis for two node regions.  In the {\tt private\_node} case they will first open
up the {\tt p\_nodes\_pvs} partition and then open up the {\tt p\_pvt\_nodes} sub partition
for each of their subregions.  We use a bold circle on an edge to indicate that it has been
opened.  Since all of the private regions use a different subregion
of a disjoint partition for the private node accesses, no dependence is detected.  

Similarly, when the dependence analysis is performed for the {\tt p\_ghost\_nodes} subregions,
each of the necessary partitions are opened.  There is no dependence with any of the private
nodes since all the ghost nodes traverse a different subregion of the disjoint partition
{\tt p\_nodes\_pvs}.  There is also no dependence between any tasks even though the
{\tt p\_ghost\_nodes} partition is aliased because all the ghost accesses are read-only.
The state of the region tree after dependence analysis has been performed for the first
{\tt compute\_new\_current} task can be seen in figure~\ref{sfig:mapping_fig:cnc}.

In the second phase of the computation, all of the {\tt distribute\_charge} tasks
will be issued.  For these tasks they will first traverse the region tree
for the regions containing their private nodes.  The path to these regions are
already open, but when the dependence analysis arrives at the specific private node subregion it will
discover a dependence between the task and the {\tt compute\_new\_currents} task that operated
on that region previously.  This dependence will be recorded and the 
{\tt distribute\_charge} task will be made the new active task as it dominated the
previous task.  

For the ghost node regions, the dependence analysis will discover that the
there is a dependence between each of the {\tt distribute\_charge} tasks and all of the
{\tt compute\_new\_current} tasks because the {\tt distribute\_charge} tasks are
performing writes and the partition is aliased.  However, the runtime will detect this as
anti-dependences which are resolved by forcing the {\tt distribute\_charge} tasks to
create new valid abstract instances rather than use the same abstract instances as
each of the {\tt compute\_new\_current} tasks.  There are no dependences detected
between each of the {\tt distribute\_charge} tasks on the ghost regions because
the {\tt Simultaneous} coherence property permits multiple tasks to be using the same
partition at the same time even if the subregions they are accessing are aliased.  The
state of the region tree after dependence analysis for first {\tt distribute\_charge} task can
be seen in figure~\ref{sfig:mapping_fig:dc}.

For the final phase of the computation, all of the {\tt update\_voltage} tasks
are issued requiring their private node region and their shared node region.  Again
the private node paths are already open, but a WAR dependence is detected which
requires a new abstract instance to be created to avoid a dependence.  The
shared nodes present a more interesting case.  The first shared node to be registered
will detect a dependence with every prior task since it must open the shared partition
instead of the ghost partition.  A new abstract instance is created at the 
{\tt all\_shared} node and copies are recorded for each valid instance in the ghost partition
back to the new {\tt all\_shared} abstract instance.  After this is complete, the
ghost partition can be opened.  Each corresponding {\tt update\_voltage} task will record
a dependence on the copy operation to close ghost partition as it traverses the {\tt all\_shared}
node ensuring that there is no requirement on the ordering of {\tt update\_voltage}
tasks.  The state of the region tree after the first update voltage task
has been registered can be seen in figure \ref{sfig:mapping_fig:volt}.

\subsection{Mapping}
\label{subsec:mapping}
After tasks have undergone dependence analysis, the tasks are placed on either a ready
queue to be mapped or waiting queue.  A task is placed on the ready queue if all the tasks with
which it registered a dependence have mapped.  If any of the task's dependences have
not mapped then the task is placed on the waiting queue.  The task also registers
itself with the task on which it depends so that when that task is mapped it will notify
the task on the waiting queue. Tasks are mapped based on the partial ordering created by 
their data dependences.  By mapping tasks based on data dependences we guarantee the 
semantics of the programming model, while still allowing tasks to execute out of order.

When the scheduler is invoked, it will pull a series of tasks off the ready queue, map them,
and then issue them to be executed by the low level runtime.  The process of mapping a task
consists of several different queries made by the runtime to an application level object
called a {\em mapper}.  A mapper is an object which has control over how both tasks and
data can be placed in the machine.  This idea is a dynamic instantiation of the idea
initially presented in \cite{Fatahalian06} of creating mapping files for specifying how
tasks and data are placed in a memory hierarchy.  Rather than rely on a static mapping
we opted for a dynamic interface that could make decisions both based on application specific
input data as well as the machine specifics both at start-up and throughout the execution
of an application.  %The interface for a mapper is described in Section \ref{subsec:mapinter}.


\subsubsection{Mapper Interface}
The mapper interface is a collection of eight function calls that allow the runtime to query
the application for information about how to map tasks are regions.  Mapper calls are made
from the runtime to ask where physical instances of regions should be place in the memory hierarchy,
which physical instances should be the source for copy operations, on which processors
tasks should be run, and how task stealing should be performed.
%\label{subsec:mapinter}
%The interface for a mapper is designed to give the programmer control over how data and tasks
%are placed in the machine.  Every mapper must support the following function calls:

%\begin{itemize}
%\item {\tt rank\_initial\_region\_locations} - When a new logical region is created, the runtime
%will ask the mapper to give a ranking of physical memories of where to place the inital instance.

%\item {\tt rank\_initial\_partition\_locations} - When a new partition is created, the mapper
%must provide a ranking of memory locations for each of the new sub partitions.

%\item {\tt select\_initial\_processor} - Given a task to be mapped, select a processor to
%send this task to for mapping.  

%\item {\tt target\_task\_steal} - select a processor to attempt task stealing.  This is explained
%in more detail in Section \ref{subsec:steal}.  

%\item {\tt permit\_task\_steal} - also covered in Section \ref{subsec:steal}

%\item {\tt map\_task\_region} - Given a specific task and the logical region to be mapped, select
%a source physical instance for the data, and provide a ranking of memories in which to create
%or reuse an existing physical instance.

%\item {\tt rank\_copy\_targets} - Given a task and a set of valid instances, generate a set of
%rankings for new memories in which to place new phsyical instances.  This operation is used to
%manage the copy operations performed when closing a subtree as they allow the programmer to
%eagerly migrate regions through the memory hierarchy as copy ups are occuring.

%\item {\tt select\_copy\_source} - Given a set of currently valid source instances and destination
%physical instance, select the instance from which a copy should be made.
%\end{itemize}

These eight task calls give the programmer complete control over where all tasks are executed, and
where all data is placed within the memory hierarchy.  While this interface does give the programmer
complete control over data and task movement, it will never allow the programmer to affect the
correctness of the program.  The high-level runtime still manages all the task dependences and the
ordering in which tasks are executed and copies are performed.  Therefore, regardless of choices
made by any mapper, the resulting program will always be executed correctly.

In order to make it easy for programmers to write programs, we provide an initial implementation 
of the mapper interface that uses simple heuristics to place tasks and data.  For example, when
performing task stealing, the default mapper only encourages stealing tasks in groups that use
the same logical regions, which improves locality by moving tasks that use the same regions
together.  The default mapper implementation makes it easy for the programmer to get an 
initial program running.  As the programmer tunes the application, he can write mappers that
specialize for both application specific and/or machine specific parameters.

The high-level runtime also supports the use of multiple mappers in the same application.  For each task 
call the programmer can choose a mapper to be used for the runtime calls made inside that
task.  This allows for the creation of library code which provides its own mappers indpendent
from any application level mappers.

\subsubsection{Task Stealing for Load Balance}
\label{subsec:steal}
In order to achieve load balancing in an application the runtime supports an interface
for task stealing.  Our task stealing algorithm is loosely based on the stealing algorithm
used by Cilk \cite{CILK95} with guidance from the mapper about how stealing should be
performed.  Tasks are initially placed on the ready queue on the processor in which they
were registered.  However, a task can be moved to a different processor either by a mapper
explicitly choosing to relocate it, or by a steal request from another processor's mappers.

When a processor notices that it has no work on its ready queue, it will inquire into each
mapper asking for a target processor to attempt a steal.  A mapper can either respond with
a target processor, or indicate that no steal should be attempted.  The runtime will then
send steal requests to each processor on behalf of each specific mapper.  

When a steal request is received at a processor it will determine the mapper(s) that generated
the list of steal requests.  For each mapper that issued a steal request, the runtime will
give to the local instance of that mapper a list of the tasks in the ready queue that that
specific mapper is responsible for mapping.  The mapper must then determine the set which
it is going to permit to be stolen if any.  Any tasks that the mapper marks as stolen are
sent back to the stealing processor.  By only allowing mappers to make steal requests about
the tasks they own, we are giving the mappers complete control over where tasks are mapped.
In addition, we are also giving the mappers control over how steal requests are made for
the case where the mapper has better knowledge about where additional work is than the 
runtime.  Note that this mapping interface is sufficiently powerful to allow a user to
disable all stealing if desired, and to map tasks to explicit processors with no interference
from the runtime.

\subsection{Execution}
\label{subsec:execution}
Once task mapping is complete the high-level runtime performs the necessary operations
to issue the task to the low level runtime.  It first creates any new physical instances
for the regions which the task requires.  It then issues the necessary copies required
for the task to execute contignent on the tasks producing the source physical instances
have finished.  Then it issues the task itself to the low level processor contingent
on all the copies for the task haveing finished as well as the the set of tasks
on which the issuing task dependends having finished.  These dependences are all encoded
using event dependencies in the low level runtime.  Events are described
in detail in Section \ref{subsec:events}.  Continuing the analogy of an out-of-order
processor, the execution phase is equivalent to issuing an instruction to an
execution unit, and having the execution unit defer the instruction's execution until all its
operands are ready.  The low-level runtime contains processors that act as execution
units that will run a task once all of the events required for the task to run
have occurred. 

%Once a task starts running, the runtime is asked to create the region accessors for
%the task.  The runtime investigates whether or not fast accessors can be created for
%each of the regions that the task requires.  Fast accessors are described in more
%detail in Section \ref{sec:fastaccess}.  If fast accessors can be created for every
%region in a task, then the runtime chooses to execute a task instantiated with fast
%region accessor methods.  However, if any region can not be specialized into a fast
%accessor the runtime defaults to a task instantiation that relies on {\tt get} and
%{\tt put} operations to access regions.

The runtime also assigns a unique identifier describing the context
in which the task is executing.  All calls into the runtime require the task
to pass this context identifier which is what permits the runtime to know the context
for incoming calls for creating/destroying regions and launching sub-tasks.  

\subsection{Task Completion}
\label{subsec:cleanup}
We define a task to have {\tt completed} once all of the task's regions reflect
the updated state of the task and all its sub-tasks.  This reflects an underlying assumption 
of the mapping process that the regions for which a task has priviliges will represent the data produced 
by that task.  However, this is not inherently true that a physical instance of a task reflects
this state after the task finishes executing.  If a task has child tasks these tasks may
still be executing when the parent task finishes the execution phase.  

In order to handle this scenario the runtime must compute the necessary copy operations
from child task physical instances back to the parent task's physical instances.  To
compute these copy operations, the runtime must first wait until all the child tasks
have been mapped.  The runtime then issues a close operation to the each of the parent
task's region trees.  These close operations record all of the necessary copies to
restore the parent task's regions.  We call these copy operations at the end of 
a task {\em restoring copies}.
The runtime then issues these copies with the required dependences
and marks the task completed once the restoring copy operations are complete.
This is similar to the write-back phase of an out-of-order processor where store instructions cannot be
considered complete until the store effects the state of memory.

The last part of the completetion phase is garbage collecting physical instances.  Physical
instances are not a visible part of the programming model and therefore cannot be managed
directly by the programmer.  Having the mapper manage them would introduce correctness
issues.  We use a two-level reference counting scheme on both abstract instances and physical
instances to know when it is safe to reclaim physical instances.  Due to space constraints
we omit the details of this algorithm. 

%Even after a task has completed it is possible for child tasks of that task not to have been 
%executed or even mapped.  This is similar
%to the write-back phase in an out-of-order processor where store instructions can only
%be considered terminated when they finish effecting the state of memory.   

%The runtime needs to know the mappings for
%the child tasks as it must issue restoring copy operations so that the set of regions
%that the task operated on are up to date with any data generated by child tasks.  

%\subsubsection{Restoring Copy Operations}
%\label{subsec:restore}
%\subsubsection{Garbage Collecting Physical Instances}
%\label{subsec:garbage}
%In addition to cleaning up task execution, the runtime must also deal with the problem
%of garbage collecting physical instances of regions.  Physical instances must be
%garbage collected since they are not visible to the application and would present
%a correctness problem if deleting instances was an option for the mapper.  
%Garbage collecting physical instances is a challenging problem
%due to both the distributed environment of the machine and the deffered execution
%model.  To solve this problem we employ a two-level reference counting scheme.  We
%reference count both abstract instances and as well as physical instances.  

%An abstract instance maintains a count of the number of tasks that have referenced 
%that abstract instance, but have not yet been mapped.  An abstract instance also tracks
%whether or not it is open or closed.  An abstract instance is open if it is still
%a valid abstract instance in a region tree.  An abstract instance is marked closed
%once it is no longer a valid abstract instance and can therefore no longer be used
%by any new tasks that are registered.

%When a task is mapped it notifies an abstract instance that it has been used and can
%decrement its reference count.  The task mapping also notifies the abstract instance
%of the specific physical instance that was used and whether or not this use invalidates
%previous physical instances of the abstract instance.  The abstract instance maintains
%a reference count for each physical instance of it that has been made as well as
%the set that are currently valid physical instances to be used for mapping.  When a 
%task terminates, it notifies the abstract instances that it used to decrement the
%reference counts for the task's specific physical instances.

%There are two conditions under which a physical instance can be garbage collected.  The
%first condition is when a physical instance is not a valid physical instance of
%an abstract instance and its reference count is zero.  This condition occurs once no
%more tasks are using a physical instance and it can longer be mapped by any task.  The
%second condition is when an abstract instance is both closed and has a reference count
%of zero as well as the physical instance's reference count being zero.  This handles the
%case when no more tasks can be set to use an abstract instance, there are no more tasks
%to map to an abstract instance, and there are no current tasks using the physical
%instance.

While reference counting normally doesn't allow for garbage collection of all data structures
it will always work in this case because both our tasks and our regions are organized as 
trees which ensures that it is impossible to create cyclic region references in our system.


