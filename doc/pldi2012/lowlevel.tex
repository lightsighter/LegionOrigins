\section{Low-Level Runtime}
\label{sec:lowlevel}

The low-level runtime is a machine-independent layer that provides portability
for our Legion implementation.
%The low-level runtime hides idiosyncracies of the underlying machine while
%at the same time not straying 
%too far from the native capabilities of the underlying hardware.  
The low-level runtime abstracts the machine as a base set of objects:
events, deferred locks, processors, memories, and regions.

%The target machines for Legion include both clusters and GPU-equipped nodes,
%and neither of which are able to efficiently provide a uniformly addressable
%and accessible memory space across the system.  The low-level runtime does
%not hide this fact from the high-level runtime, but rather relies on the
%high-level runtime to manage data movement.  Consequently all objects in
%the low-level runtime are named by handles 
%which can be copied by value and moved around the machine.

%The low-level runtime provides a deferred execution model rather
%than the more common immediate model where operations occur as soon
%as requested.  Requests made of the low-level 
%runtime usually return to the caller immediately, with the requested action
%performing asynchronously to the caller's thread of execution.  This is done
%for two different, but equally important, reasons.  First, it matches the
%standard GPU execution model.  GPUs tend to have very deep pipelines, and
%maximum performance cannot be achieved if each command has to be run to
%completion before the next command can be started.  Second, in clusters, the
%latency of inter-node communication can be significant, and a caller that
%waited for every request of a remote node to return before continuing on would
%also be similarly underutilizing the computational resources of the system.

%Although it does not show up explicitly in the interface, the low-level runtime
%was also architected to permit a distributed and hierarchical implementation.
%Critical resources can be replicated or migrated between nodes to reduce
%both average latencies and the amount of traffic that use the scarce inter-node
%communication bandwidth.



\subsection{Low-Level Abstractions}
\label{subsec:lowobjects}
\subsubsection{Events}
\label{subsec:events}
%The low-level runtime supports a deffered execution model to hide
%the latencies associated with communicating in a distributed memory
%system.  
The basic building block of deferred execution 
is the {\em event}.  An event is a placeholder for the completion of a deferred
operation.  When an asynchronous call is made to any of the low-level runtime objects, an event is 
immediately returned to the caller.  The caller may query whether the event has {\em triggered},
which happens when the operation corresponding to the event completes.  The common use case for events is
to provide them as prerequisites to subsequent requests made of the runtime. 
These dependences cause the later requests to be automatically deferred until the previous
events have triggered.  By using events as dependences, the caller can
set up complicated, but coordinated chains of operations to be executed without
having to wait for the low-level runtime to execute them.
%These requests will
%also return events, and the caller can set up a complicated, but coordinated,
%set of operations to be performed by the low-level runtime with no babysitting
%required of the original caller.

With nearly every single runtime operation creating a new event and referring
to potentially any number of past events, both the time and space overhead of
implementing events is a serious concern.  The time overhead is addressed by a
mechanism that allows a node to create a new event without needing to
communicate with any other node.  The event's handle identifies which node
created it, allowing another node to lazily access it only when and if it 
is asked to perform an operation on it.  Checking to see if an event has
occurred is done via a subscription mechanism.  Any number of threads or
deferred operations on one node can query or wait on an event with a single
message sent to the originating node.  The requesting node tracks the multiple
local requestors and upon receiving the trigger message from the originating
node, distributes that message to all the local parties that are interested.

The potential space concern with events is addressed by a {\em generation scheme}
that allows events to be reclaimed and reused without tracking references.  The
naive implementation of an event cannot reclaim the storage used by an event
until the event has triggered and all waiters on the event have been
notified.  
%Some of these waiters may take quite a while to query the event.
%Additionaly, we must somehow track how many queries to expect to know
%when they've all been performed.  
Our events include a
{\em generation count} that tracks how many times the event has been triggered,
which allows an event to ve reused with a new generation as soon as the previous generation has triggered.
Holders of an older (handle,generation) pair are able to query the newer
incarnation of the event and determine from the generation mismatch that their
event must have already triggered.  With this technique, there is no need to
count references to the event at all. 
% An event can only be reused until the
%generation counter reaches its maximum value, the current implementation
%supports $2^{32}-1$ generations, and should that not be sufficient, extending
%it to $2^{64}-1$ is trivial.

\subsubsection{Deferred Locks}
\label{subsec:defferedlocks}
Events provide a deterministic ordering between operations, 
but the more relaxed coherence do not require strict ordering.  The traditional tool for this job is
the mutex (a.k.a. lock), and it is from the common reader/writer lock that
the low-level runtime's {\em deferred lock} is derived.  The key visible 
difference between a mutex and a deferred lock is that the request does not
block the caller's execution.  Many libraries offer nonblocking locks, but
they achieve this by returning without the lock if it is unavailable.  If
a deferred lock is currently unavailable, the request is guaranteed to be
granted at some time in the future (with no further action required of the 
caller).  That time in the future is described with an event.  Like all other
events, it can be given to other threads, or even stored
in a data structure in memory to be extracted by an arbitrary thread at a 
later time.  This dissociation of a held lock from the thread that
requested it is another important difference between the deferred lock and
mutexes.

The deferred nature of a deferred lock also allows a distributed implementation
of locks.  Similar to events, a deferred lock can be created on any node
without communication with the other nodes.  That node becomes
the initial home for the lock, but the deferred lock (unlike events) can 
migrate to other nodes or even be shared between nodes, allowing repeated
lock requests from a single node to be kept local to that node after a single
message is sent to the current owner.  (Should the ownership of the lock change
while the message is in flight, the previous owner forwards the request to
the new owner.  The highly unlikely, but possible, case in which multiple 
forwardings are required is bounded at the number of nodes in the machine.)
The ability to migrate locks between nodes allows the performance of a
heavily-contended cluster-level lock to approach that of a single-node lock.

The final interesting feature of the deferred lock is that it can be associated
with an object in memory and that object will automatically be kept coherent
across the cluster.  This is similar to MESI coherence in a distributed cache,
but is done per-object rather than on cache lines.  This is currently only 
used for object within the runtime itself, but we are exploring ways to expose
it at the Legion application level.

\subsubsection{Processors}
\label{subsec:processors}
Hardware units capable of performing computation are captured as generic
processors in the low-level runtime.  Like other runtime objects, a processor
has a globally unique name and can be referred to anywhere in the system.
Processors come in different \emph{kinds} (e.g. X86 CPU core, CUDA-capable GPU),
but they all support the same
interface and execution model.  A request to spawn a task on a given processor
may come from anywhere in the system.  The request names which task should
be spawned and provides input arguments and zero or more events.  Once those
events have occurred, the task is placed into a queue of ready tasks, which
the processor works through in the order in which they were enqueued.  A
processor will generally run a task to completion before taking another, but
will try to run another task (resources permitting) if the current task needs
to wait for an event to occur.  Once a task is placed on a processor's queue,
it may not be moved or deleted.  Load-balancing of tasks is expected to be done
by the high-level runtime.  The primary purpose of the queue for the low-level
Processor is to be able to hide the latency of starting new tasks.

\subsubsection{Memories}
\label{subsec:memories}
Application-visible storage in the low-level runtime is described in terms of
memory objects.  A memory object has a size and describes its affinity to 
processors (and other memories) in terms of relative bandwidth and latency.
The low-level runtime allows for the allocation and freeing of region instances
and allocators.  It also supports bulk transfers from one memory to another.
These bulk transfers can often be implemented using dedicated DMA (or RDMA)
hardware, freeing up the processors to perform more interesting
computation. 

\subsubsection{Regions}
\label{subsec:regionmeta}
Application-level regions are broken into four inter-related pieces in the
low-level runtime:
\begin{description}

\item[MetaData] The metadata for a logical region captures attributes of a
logical region that are common across all instances.  These attributes include
the size of the logical region, the element type, and the allocation mask,
which tracks which portions of the logical region are actually in use.  
%Region
%MetaDatas are created independently of any memory objects and are accessible
%(via a globally unique name) to all nodes in the machine.

\item[Allocator] The dynamic memory allocation capability of a region instance
is provided via an allocator object.  An allocator is associated with a
region metadata and must be created in a particular memory.  Once created, it
may be used to allocate and free elements of the region by any task on any
processor that has non-zero affinity with the memory.

\item[Instance] Each physical instance of a region stores some version of all
the elements in a region.  As with allocators, a physical instance is
associated with a particular region metadata and spends its entire lifetime in
the memory object in which it was created.  
%When multiple physical instances
%exist for the same region, the low-level runtime does nothing to keep them
%copies and set up the right inter-operation dependencies to provide the level
%of coherence requested by the application.  Although a physical instance 
%stores the contents of a region, it does not directly offer access to those
%contents.

\item[Accessor] Access to the contents of a physical region instance are 
handled by accessor objects.  These are lightweight objects that allow a given
task to access a given physical region instance, regardless of the
implementation of that instance or the underlying memory.  In the most common
case, read and write access is done through a single array dereference, but the
accessor interface allows seamless use of other types of memory as well (e.g.
memory that's accessed via RMDA put and get calls).

\end{description}


\subsection{Low-Level Implementations}
\label{subsec:lowimpl}
The primary goal of the low-level runtime is to provide an
abstraction that enables efficient execution of Legion programs
on a wide array of machines.  To support this claim we describe
two implementation of the low-level runtime: one based on POSIX
threads for a shared memory SMP and one based on GASNet and CUDA
for distributed memory clusters with multiple GPUs.
%The magic of an
%apparently-unified address space is left to the high-level runtime, and the
%interface to the low-level runtime is done in terms of handles rather than 
%pointers for runtime objects and relative offsets rather than absolute 
%pointers for resource allocations.

%\subsection{Initialization}

%On startup, the low-level runtime allocates all the resources it needs from
%the operating system and set up the communication channels between nodes.
%Each processor in the system then automatically runs the high-level runtime's
%initialization task, which will usually spawn more tasks.  These tasks are
%started, and whenever any processor's task queue is empty , the high-level
%runtime's scheduler task is automatically called to explore the possibility
%of moving tasks around for load balancing.

\subsubsection{SMP Implementation}
\label{subsec:smpimpl}
The SMP implementation of the low-level runs on any SMP machine that supports 
POSIX threads.  A single memory object is created for the shared memory, and 
a processor object is created for
each CPU core in the system.  Events are 
mutex-protected data structures that maintain a list of callbacks to call when
the corresponding event is triggered.  Locks are similar, except that the list
is of pending lock requests to attempt when the previous lock is released.

\subsubsection{GPU Cluster Implementation}
\label{subsec:clusterimpl}
The cluster implementation of the low-level runtime is implemented on a mix
of POSIX threads for intra-node threading and communication, GASnet for 
inter-node communication, and CUDA for access to resources of the GPUs in 
each node.  Locks and events have roughly the same implementation as the SMP
runtime for handling multiple local requestors, but use GASnet active messages
to coordinate lock ownership and event subscription and triggering between
nodes.  In addition to the processor objects for each CPU core, a single
processor object is created for each GPU in the system.  It would be preferable
to expose the GPU's computational resources at a finer granularity, but
current GPU hardware doesn't directly allow this flexibility and recent attempts
at implementing such control in a software layer on top of CUDA have suffered
from significant overhead.  The memory hierarchy is expanded to include a 
{\em global} memory that is striped across all nodes and accessible via GASnet
RDMA calls, one process local memory per cluster node, and two memories per GPU: one for the GPU's framebuffer that is
accessible only to that GPU and by bulk DMA operations, and one for the pinned
{\em zero copy} system memory that is accessible to the GPU and all the CPU cores
in a single system.
