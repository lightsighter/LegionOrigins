\section{Example: Circuit Simulator}
\label{sec:ex}

%\include{code_ex}

In this section we give an informal overview of Legion through an example.
Listing~\ref{lst:code_ex} shows Legion code for an electrical
circuit simulation, which illustrates the core features of the programming model.


Lines 1-16 define data structures.  A circuit consists of wires and
nodes where wires meet.  {\tt Node}s have fields for the simulation
(e.g. charge, capacitance) and are linked in a circular list.
Pointers in data structures (e.g. the {\tt next} fields in the list)
point to a particular region given by the standard {\tt Type@region}
syntax.  The program relies on different lists being in different
regions, so the {\tt Node} definition is parameterized on a region
{\tt rn}.  The {\tt next} field of a $\tt Node \langle rn \rangle$ has
type $\tt Node \langle rn \rangle \mbox{\tt @} rn$, which implies that
all nodes in the list are in region {\tt rn}.

{\tt Wire}s are similar, except that a wire refers to two nodes to
which it is electrically connected.  The two {\tt Node}s are in region {\tt rn},
but they may point to {\tt Node}s in a different region {\tt rn2}.  This is a common pattern in Legion code, and
arises when one needs to place one constraint on the objects directly
referenced, and a second (presumably weaker) constraint on objects
that are indirectly reached through the direct pointers.

Lines 4-9 declare {\tt Circuit}s, which is our first example of a
\emph{region relationship}.  A region relationship is just a structure
with named fields, except that any region fields can be used in the
type declarations.  Thus the region {\tt r\_all\_nodes} contains {\tt
Node}s that point only into region {\tt r\_all\_nodes}, guaranteeing that
all {\tt Node}s are in region {\tt r\_all\_nodes}.  (Note that regions are
declared with the single data type they can contain.)  Simiarly, the
type of the wire region's element not only guarantees that every wire
in the circuit is in the wire region, but also guarantees that every
node referred to by any wire is in the circuit's node region.

The last definition is for the pieces into which the circuit is partitioned.
The original node and wire regions are specified as parameters, and the
four kinds of subregions that are created are made fields of the
{\tt CircuitPiece}.  This declaration also illustrates {\em subregion constraints}
placed on the three region fields stating the region fields  must be subregions of the 
node and wire regions provided as parameters.  Since the simulation code will need to be able to
iterate over the nodes owned by a {\tt CircuitPiece}, the parameterization of the {\tt rn\_pvt} (private nodes),
{\tt rn\_shr} (shared nodes), and {\tt first\_node} fields constrains the linked list of
nodes to reside entirely in those two regions.  The linked list of wires is
similarly constrained, and the wire's node pointers are constrained to fall
into one of the three subregions defined in the {\tt CircuitPiece}.  Finally,
the use of {\tt rn} for {\tt rn\_ghost}'s parameter and the second parameter
in the wire type places no constraints on which nodes a ghost node might point
to.

Line 17 declares the main simulator function, which accepts a \emph{Circuit}
to be simulated.  In addition to specifying the return type and the types and
names of the formal parameters, it must also specify which regions of memory
it will access, how it will access them (e.g. read-only, read-write, or with
reductions), and what coherence guarantees are needed (e.g. exclusive access).
The regions listed in these access declarations may be anything in the static
scope of the function definition, the formal parameters, or fields of the formal
parameters.  In our example, the simulation of the circuit will need to read
and write all of the nodes and all of the wires in the circuit, and it must
be done with exclusive access to ensure a correct result.

%\include{part_fig}

Lines 19-27 are responsible for partitioning the circuit into \emph{MAX\_PIECES}
pieces that can be worked on in parallel.  In order to partition a region, we
need to provide a \emph{coloring}, which is a relation over the valid elements
in a region and a set of ``colors''.  Our current implementation uses small
integers or booleans for the names of colors, but any enumerable set would be
fine.  The coloring relation need not be total - it can leave out some elements entirely.  And although it is not required to be injective (it can map a single
element to multiple colors), the fact that injective mappings will always
result in disjoint subregions makes them strongly encouraged (when possible).

Our example uses four colorings to help create the partitions of the circuit.
A graph partitioning algorithm
(not shown) is run on to divide the nodes of the graph into \emph{MAX\_PIECES}
subsets.  (These subsets are hopefully reasonably compact, but the correctness
of the simulation is not dependent on that.)  The thick lines in \ref{sfig:part_fig:pvs}
show a how the nodes in a small graph might be partitioned into 3 subsets.
Once the subsets are known, the \emph{node\_owner\_map} is created by assigning
each node a color corresponding to which subset it was placed in.  The 
\emph{wire\_owner\_map} assigns each wire to the same subset as its ``in\_node''.
The \emph{node\_nghbr\_map} maps a node to color(s) of all wires that 
connect to it.  Finally, the \emph{node\_sharing\_map} is derived from the
\emph{node\_nghbr\_map}, with a node colored ``true'' if any colors other than 
its own were used, and ``false'' if the only wires that connect to a node are
in the same piece.
Figure~\ref{fig:part_fig} shows how the partitions are defined.

Line 19 uses the \emph{wire\_owner\_map} to partition the wires region of the
circuit into a subregion for each piece, but the partitioning of the nodes is
more complicated due to the sharing that is necessary between the pieces.
First, the \emph{node\_sharing\_map} is used to create two subregions: $p\_nodes\_pvs[false]$ contains all the nodes that are private to some piece (i.e. will
never be needed for the computations in any other piece), while $p\_nodes\_pvs[true]$ contains nodes that will be accessed by multiple pieces' computations.
Each of these subregions is then partitioned using the $node\_owner\_map$ to
create the subregions owned by each computation.  These are the $p_i$ and $s_i$
subregions, shown in figures \ref{sfig:part_fig:p_i} and \ref{sfig:part_fig:s_i}.  Finally, the $g_i$ subregions (shown in figure~\ref{sfig:part_fig:g_i}) are
created using the $node\_nghbr\_map$ to 
create subregions that include the ``ghost'' nodes needed to perform each
piece's calculations.

With the partitioning operations completed, the various subregions are
recorded in an array of $CircuitPiece$ structures.  The combination of the
choice of parameters on line 29 with the constraints from lines 11-13 allow 
this array to hold pieces of the circuit provided to the $simulate\_circuit$
function, but no others.

Lines 31-32 use a \emph{simultaneous assignment} operation to fill in multiple
fields of a $CircuitPiece$ at once.  Because a region relationship can have
fields with types that refer its own fields' values, it is often not possible
to assign fields one at a time without violating the type checking rules.
The simultaneous assignment operator asks the type checker only to make sure
that the fields in the structure would have the right (self-referential) types
after all the fields have been changed.

Lines 34-38 form the bulk of the actual simulation, performing three passes
over the circuit on each iteration.  For each pass, a for loop is used to 
spawn a task for each piece of the circuit.  There are no explicit requests for
parallel execution, nor is there explicit synchronization required between the
passes.  As we will see, bBoth the fact that the pieces can be run in parallel
for a single 
pass and the the required inter-task dependencies are determined automatically
by the runtime based on the region access annotations on the task declarations.

The declarations for the three subtasks are shown on lines 41-50.  The 
$calc\_new\_currents$ task declares that it will read and write the wires
for that piece, and will need to read both the nodes in its piece and any
node that is adjacent to the piece (i.e. any node that one of the wires in the
piece might connect to).  Since the wire subregions are known to be disjoint,
the write sets of invocations of $calc\_new\_currents$ do not overlap, and can
therefore be safely run in parallel.

The $distribute\_charge$ subtask turns things around, reading the piece's 
wires and updating all the nodes that those wires connect to.  However,
rather than requesting the ability to read and write the nodes (which would
require serialization of these tasks for correctness), the task declares that
it will use reorderable reduction operations and that the coherence requirement
can be reduced to atomicity rather than exclusive access.  As long as the
runtime can guarantee to apply the reductions from multiple subtasks safely, it
can run the subtasks themselves in parallel.  Each invocation of 
$distribute\_charge$ will be delayed until the corresponding invocation of 
$calc\_new\_currents$ has completed due to the read-after-write dependency on
the corresponding wire subregion.  However, despite the apparent 
write-after-read anti-dependency on the ghost node regions, $distribute\_charge$
tasks will generally not have to wait on the the completion of the other
$calc\_new\_current$ tasks.  If there is sufficient memory available to make
two copies of those nodes, the runtime can allow $distribute\_charge$ tasks to
start calculating a new version of the nodes while older $calc\_new\_currents$
tasks are still referring to the older version, all completely transparently to
the application code.

The final task is $update\_voltages$, in which each piece is able to update
voltage information on the nodes in that piece based on the reductions that
happened in the previous task.  Again, the disjointness of the $p_i$ and $s_i$
node subregions allows the runtime to safely run these tasks in parallel.  In
this case, the runtime does wait for the completion of all the tasks in the 
previous pass.  The read-after-write dependence on the $p_i$ is a guaranteed
conflict, but there is also potential overlap between the $s_i$ subregions
being reduced to in the previous pass and the $g_i$ subregions being accessed
in this pass.  Although not every pair of $s_i$ and $g_j$ conflict, the
runtime knows that they were created from two independent partitioning
operations and guarantees correctness by conservatively assuming they might
conflict.

%\include{code_ex}
%\include{part_fig}
