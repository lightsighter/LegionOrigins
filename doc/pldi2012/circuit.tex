\section{Example: Circuit Simulator}

%\include{code_ex}

Listing~\ref{lst:code_ex} shows an example of Legion code for an electrical
circuit simulation.  Although it does not use all the capabilities of Legion
(or all the aspects of the language), it covers the core set.  Legion code is
derived from C, but different in several important ways.

Lines 1-16 
define the data structures used in the rest of the code.  A {\tt Circuit}
is an irregular data structure that describes the wires in the circuit and
the 'nodes' (where the wires meet).  The basic nodes and wires are defined
first.  Nodes have a few fields for the simulation (e.g. charge, capacitance),
and are hooked up in a circular linked list to allow the simulation to be
able to iterate over all of them.  Pointers in data structures (e.g. the
``next'' fields in this linked list) must be constrained
to a particular region, so Legion uses a {\tt Type@region} syntax rather than
C's unconstrained {\tt Type~*}.  The example relies on the ability to
constrain different linked lists of nodes differently, so rather than using
the name of some static region the {\tt Node} definition is parameterized on
a region {\tt rn}.  The use of the same parameter on the type pointed to by
the next pointer means that every single node in the linked list is constrained
to reside in the specified region.

The definition of a {\tt Wire} is similar, except that a wire must refer to
the two nodes to which it is electrically connected.  The pointers to the nodes
must be constrained, and the type that the pointer points to requires a region
as well, but unlike in the linked list case, two different regions are used.
This is a common occurrance in Legion code, and arises when one needs to place
one constraint on the objects directly referenced, and a second (presumably
weaker) constraint on objects that are indirectly referencable through the
direct pointers.

With the definition of nodes and wires out of the way, lines 4-9 declare how
they are formed into a {\tt Circuit}, and is our first example of a
\emph{region relationship}.  At the language level, a region relationship is
similar to a structure, except that it is allowed to contain references to
dynamically allocated regions as fields, and the types of the fields of a region
relationship are allowed to refer to those region fields.  (This will be 
discussed in additional detail below.)  It is also allowed
to place constraints on those region fields, which we'll see in the next
declaration.  In this example, a circuit is defined
by two regions, one for its nodes and one for its wires.  The element type of
the node region (and of the {\tt first\_node} pointer) guarantees that every
node in the circuit reside in this region.  Simiarly, the type of the wire
region's element (and of the {\tt first\_wire} pointer) not only guarantees
that every wire in the circuit is in the wire region, but also guarantees that
every node referred to by any wire is in the circuit's node region.

The last definition is for the pieces that the circuit is partitioned into.
The original node and wire regions are specified as parameters, and the
four kinds of subregions that are created below are made fields of the
{\tt CircuitPiece}.  Constraints are placed on all four of these fields that
they must be subregions of the original node and wire regions that were
provided as parameters.  Since the simulation code will need to be able to
iterate over the nodes owned by a piece, the parameterization of the {\tt rn\_pvt},
{\tt rn\_shr}, and {\tt first\_node} fields constrains the linked list of
nodes to reside entirely in those two regions.  The linked list of wires is
similarly constrained, and the wire's node pointers are constrained to fall
into one of the three subregions defined in the {\tt CircuitPiece}.  Finally,
the use of {\tt rn} for {\tt rn\_ghost}'s parameter and the second parameter
in the wire type places no constraints on which nodes a ghost node might point
to.

Line 17 declares the main simulator function, which accepts a \emph{Circuit}
to be simulated.  In addition to specifying the return type and the types and
names of the formal parameters, it must also specify which regions of memory
it will access, how it will access them (e.g. read-only, read-write, or with
reductions), and what coherence guarantees are needed (e.g. exclusive access).
The regions listed in these access declarations may be anything in the static
scope of the function definition, the formal parameters, or fields of the formal
parameters.  In our example, the simulation of the circuit will need to read
and write all of the nodes and all of the wires in the circuit, and it must
be done with exclusive access to ensure a correct result.

%\include{part_fig}

Lines 19-27 are responsible for partitioning the circuit into \emph{MAX\_PIECES}
pieces that can be worked on in parallel.  In order to partition a region, we
need to provide a \emph{coloring}, which is a relation over the valid elements
in a region and a set of ``colors''.  Our current implementation uses small
integers or booleans for the names of colors, but any enumerable set would be
fine.  The coloring relation need not be total - it can leave out some elements entirely.  And although it is not required to be injective (it can map a single
element to multiple colors), the fact that injective mappings will always
result in disjoint subregions makes them strongly encouraged (when possible).

Our example uses four colorings to help create the partitions of the circuit.
A graph partitioning algorithm
(not shown) is run on to divide the nodes of the graph into \emph{MAX\_PIECES}
subsets.  (These subsets are hopefully reasonably compact, but the correctness
of the simulation is not dependent on that.)  The thick lines in \ref{sfig:part_fig:pvs}
show a how the nodes in a small graph might be partitioned into 3 subsets.
Once the subsets are known, the \emph{node\_owner\_map} is created by assigning
each node a color corresponding to which subset it was placed in.  The 
\emph{wire\_owner\_map} assigns each wire to the same subset as its ``in\_node''.
The \emph{node\_nghbr\_map} maps a node to color(s) of all wires that 
connect to it.  Finally, the \emph{node\_sharing\_map} is derived from the
\emph{node\_nghbr\_map}, with a node colored ``true'' if any colors other than 
its own were used, and ``false'' if the only wires that connect to a node are
in the same piece.
Figure~\ref{fig:part_fig} shows how the partitions are defined.

Line 19 uses the \emph{wire\_owner\_map} to partition the wires region of the
circuit into a subregion for each piece, but the partitioning of the nodes is
more complicated due to the sharing that is necessary between the pieces.
First, the \emph{node\_sharing\_map} is used to create two subregions: $p\_nodes\_pvs[false]$ contains all the nodes that are private to some piece (i.e. will
never be needed for the computations in any other piece), while $p\_nodes\_pvs[true]$ contains nodes that will be accessed by multiple pieces' computations.
Each of these subregions is then partitioned using the $node\_owner\_map$ to
create the subregions owned by each computation.  These are the $p_i$ and $s_i$
subregions, shown in figures \ref{sfig:part_fig:p_i} and \ref{sfig:part_fig:s_i}.  Finally, the $g_i$ subregions (shown in figure~\ref{sfig:part_fig:g_i}) are
created using the $node\_nghbr\_map$ to 
create subregions that include the ``ghost'' nodes needed to perform each
piece's calculations.

With the partitioning operations completed, the various subregions are
recorded in an array of $CircuitPiece$ structures.  The combination of the
choice of parameters on line 29 with the constraints from lines 11-13 allow 
this array to hold pieces of the circuit provided to the $simulate\_circuit$
function, but no others.

Lines 31-32 use a \emph{simultaneous assignment} operation to fill in multiple
fields of a $CircuitPiece$ at once.  Because a region relationship can have
fields with types that refer its own fields' values, it is often not possible
to assign fields one at a time without violating the type checking rules.
The simultaneous assignment operator asks the type checker only to make sure
that the fields in the structure would have the right (self-referential) types
after all the fields have been changed.

Lines 34-38 form the bulk of the actual simulation, performing three passes
over the circuit on each iteration.  For each pass, a for loop is used to 
spawn a task for each piece of the circuit.  There are no explicit requests for
parallel execution, nor is there explicit synchronization required between the
passes.  As we will see, bBoth the fact that the pieces can be run in parallel
for a single 
pass and the the required inter-task dependencies are determined automatically
by the runtime based on the region access annotations on the task declarations.

The declarations for the three subtasks are shown on lines 41-50.  The 
$calc\_new\_currents$ task declares that it will read and write the wires
for that piece, and will need to read both the nodes in its piece and any
node that is adjacent to the piece (i.e. any node that one of the wires in the
piece might connect to).  Since the wire subregions are known to be disjoint,
the write sets of invocations of $calc\_new\_currents$ do not overlap, and can
therefore be safely run in parallel.

The $distribute\_charge$ subtask turns things around, reading the piece's 
wires and updating all the nodes that those wires connect to.  However,
rather than requesting the ability to read and write the nodes (which would
require serialization of these tasks for correctness), the task declares that
it will use reorderable reduction operations and that the coherence requirement
can be reduced to atomicity rather than exclusive access.  As long as the
runtime can guarantee to apply the reductions from multiple subtasks safely, it
can run the subtasks themselves in parallel.  Each invocation of 
$distribute\_charge$ will be delayed until the corresponding invocation of 
$calc\_new\_currents$ has completed due to the read-after-write dependency on
the corresponding wire subregion.  However, despite the apparent 
write-after-read anti-dependency on the ghost node regions, $distribute\_charge$
tasks will generally not have to wait on the the completion of the other
$calc\_new\_current$ tasks.  If there is sufficient memory available to make
two copies of those nodes, the runtime can allow $distribute\_charge$ tasks to
start calculating a new version of the nodes while older $calc\_new\_currents$
tasks are still referring to the older version, all completely transparently to
the application code.

The final task is $update\_voltages$, in which each piece is able to update
voltage information on the nodes in that piece based on the reductions that
happened in the previous task.  Again, the disjointness of the $p_i$ and $s_i$
node subregions allows the runtime to safely run these tasks in parallel.  In
this case, the runtime does wait for the completion of all the tasks in the 
previous pass.  The read-after-write dependence on the $p_i$ is a guaranteed
conflict, but there is also potential overlap between the $s_i$ subregions
being reduced to in the previous pass and the $g_i$ subregions being accessed
in this pass.  Although not every pair of $s_i$ and $g_j$ conflict, the
runtime knows that they were created from two independent partitioning
operations and guarantees correctness by conservatively assuming they might
conflict.

%\include{code_ex}
%\include{part_fig}
