
\section{Execution Model}
\label{sec:exec}

%In this section we present the Legion execution model and show that
%it is correct in that it is guaranteed to preserve the sequential
%execution semantics of programs.  The correctness argument is
%instructive in that the properties that ensure correctness are also
%exploited in the implementation of the Legion high-level runtime
%system to achieve high performance and highly distributed scheduling
%of parallel tasks.

In this section we present the Legion parallel execution model and
show that the sequential execution semantics of a program can be
preserved while making only local scheduling decisions, provided we
know the set of regions accessed by a task at scheduling time---the
property that the type system described in Section~\ref{sec:type}
guarantees.
To make our arguments precise we consider an extremely restricted core language
that nevertheless illustrates the main idea.
Programs are sets of functions, one of which is a distinguished entry point:
{ \small
\[
\begin{array}{rcl}
F & := & \mbox{\tt def}\ f(r_1,\ldots,r_n) =   s_1; \ldots s_n; \\
s & := & f(r_1,\ldots,r_n) \\
&| & \deref r = x \\
& | & x = \deref r \\
&| & (r_1, r_2)_{p} = \mbox{\tt partition}(r)
%&| & \mbox{\tt close}\ p 
\end{array}
\]
}
There are four kinds of names: functions $f$, regions $r$, 
variables $x$, and partitions $p$.  A function body is a list of statements.
The core language has no permissions or coherence annotations;
in this section we simply use exclusive access.
The statement $f(r_1,\ldots,r_n)$ calls a function, which
is executed for its effects on the region arguments.  The statement $x
= \deref r$ reads a value from region $r$ into
$x$, and the statement $\deref r = x$ stores the value of $x$
in region $r$.  

The statement $(r_1, r_2)_{p} = \mbox{\tt partition}(r)$ creates
regions $r_1$ and $r_2$ partitioning the parent region $r$.  For
simplicity we assume partitions result in disjoint subregions.  The
partition has a name $p$.  We extend the subregion
relation $\rleq$ to include partitions, so $r_1 \rleq p$, $r_2 \rleq
p$, and $p \rleq r$.  The $\rleq$ relation defines a {\em region
forest} with regions at the roots, and alternating levels where
partitions are always children of regions and regions are always
children of partitions.  If we ignore partitions, $\rleq$ is just the
subregion relation.  Including partitions gives information about
disjointness.  Let $\lca{r}{r'}$ be the
least-common ancestor of $r$ and $r'$ in the region forest.  If
$\lca{r}{r'} = p$ then $r$ and $r'$ are disjoint, as $r$ and $r'$ must
be in distinct subregions of the partition $p$.  If $\lca{r}{r'} = r''$ then either one of $r$ or $r'$
is a subregion of the other, or they lie in two different partitions of
$r''$ and disjointness cannot be guaranteed.  (In the full
language, the test for disjointness has one more case for
partitions with non-disjoint subregions.)


%which is used to {\em close} the partition in the $\mbox{\tt close}\
%p$ statement Intuitively, closing a partition reconciles modifications
%to the subregions with the {\em parent} region: $r$ is updated to
%incorporate any changes to $r_1$ and $r_2$.  In our source language
%{\tt close} is not available to the programmer, but is invoked by the
%language implementation to reconcile distributed copies of regions
%whenever necessary, and it is impossible for the program to observe
%discrepencies between a region and its subregions. This toy language,
%then, is closer to the level of ourintermediate representation, which
%is most appropriate for discussing our runtime system.

To make the following development as simple as possible, we assume every
function except the entry point is called in exactly one place and that the formal
parameters are named identically to the actual parameters in the
call.  Variable and partition names are chosen to be distinct from all
other names in the program.  These restrictions allow us to avoid
dealing with variable renaming and also mean that each runtime call is
uniquely named by its function.



The sets of regions read and written by statements are:
{\small
\[
\begin{array}{rcl}
\rread{x = \deref r} & = & \{ r \} \\
\rwrite{\deref r = x} & = & \{ r \} \\
%\rwrite{(r_1, r_2)_{p} = \mbox{\tt partition}(r)} & = & \{ r_1,r_2,r \} \\[.15in]
\rwrite{f(r_1,\ldots,r_n)} & = & \bigcup_{1 \leq i \leq n}{\rwrite{s_i}} \\
\multicolumn{3}{l}{\ \ \ \ \mbox{where\ {\tt def}}\ f(r_1,\ldots,r_n): =   s_1; \ldots s_n;} \\
\rread{f(r_1,\ldots,r_n)} & = & \bigcup_{1 \leq i \leq n}{\rread{s_i}} \\
\multicolumn{3}{l}{\ \ \ \ \mbox{where\ {\tt def}}\ f(r_1,\ldots,r_n): =   s_1; \ldots s_n;} \\
\end{array}
\]
}
We say two statements $s_1$ and $s_2$ are {\em independent} if
\[
\begin{array}{l}
   (\forall r_1 \in \rread{s_1} \cup \rwrite{s_1}.\forall r_2 \in \rwrite{s_2}. \exists p. \lca{r_1}{r_2} = p) \wedge \\
   (\forall r_2 \in \rread{s_2} \cup \rwrite{s_2}.\forall r_1 \in \rwrite{s_1}. \exists p. \lca{r_1}{r_2} = p)
\end{array}
\]
The definition captures the usual idea that statements are independent if whenever one of them is a write they do not access the same memory location, where the notion of ``location'' is generalized to our setting with regions and partitions.  
If $s_1$ and $s_2$ are independent, then $s_1; s_2 \equiv s_2; s_1$, meaning the two sequences are semantically
equivalent (have the same net effect on regions when executed).  We omit the simple proof of this claim.


The {\em sequential execution order} $S$ is the usual call-by-value
sequence of region read and write statements performed by the program.  Let
$S'$ be a permutation of $S$ where the order of dependent (i.e., not
independent) statements is preserved.  It is easy to prove that
$S \equiv S'$ by a sequence of swaps of adjacent independent
statements.  
Preserving dependencies between statement reads and writes captures
dataflow parallelism, but in the large
distributed memory machines we target instruction-level dataflow
is both too fine-grain and incurs too much communication,
as we must compute a global dependence graph across the entire
program.  

In Legion we use a different notion of what can be executed
in parallel that works at the granularity of functions and also
requires only local analysis within a function body.  We first need a few
additional definitions.  Define $\context{s}$ to be the function call
statement that invokes the function in which statement $s$ occurs
(here we assume some way of distinguishing identical statements that
occur in different functions).  Let $\interval{f}$ be the set of
read and write statements executed by $f$ and all the
functions $f$ transitively calls; we define $\interval{s} = \{ s \}$
for any statement $s$ other than a function call.  Finally we say
statements $s_1$ and $s_2$ are {\em siblings} if they occur in the
same function body.

Consider two statements $s_1$ and $s_2$ where $s_2$ {\em depends on}
$s_1$ (i.e., $s_1$ and $s_2$ are dependent and $s_2$ follows $s_1$ in
the sequential execution order).  Let $f$ be the function that is the
least common ancestor of $s_1$ and $s_2$ in the call tree of the
program---the unique function $f$ with the smallest interval such that
$s_1,s_2 \in \interval{f}$.  Then one of the following is true: (1) $s_1$ and $s_2$ are siblings in $f$;
(2) $s_1$ occurs in $f$, there is a function call $f_2(\ldots)$ in $f$ such that $s_2 \in \interval{f_2}$, and $f_2(\ldots)$
is dependent on $s_1$;
(3) $s_2$ occurs in $f$, there is a function call $f_1(\ldots)$ in $f$ such that $s_1 \in \interval{f_1}$, and
$s_2$ is dependent on $f_1(\ldots)$; or
(4) there are two distinct function call statements $f_1(\ldots)$ and $f_2(\ldots)$ in the body of $f$ such
that $s_1 \in \interval{f_1}$ and $s_2 \in \interval{f_2}$ and $f_2(\ldots)$ is dependent on $f_1(\ldots)$.
The proof of this statement follows from the observation that if $s \in \interval{f}$, then either $s$ is in the function body of $f$ or $f$ contains a function call to $g$ and $s \in \interval{g}$ and $\rread{s} \subseteq \rread{g}$
and $\rwrite{s} \subseteq \rwrite{g}$.

In other words, any dependence between arbitrary statements
in different functions can be identified at a coarser granularity as a
dependence between two statements (one or both of which may be a function call) within the same function body.  
The following lemma gives a class of statement orderings based on these observations.
\begin{lemma}
\rm
\label{lem:scheduling}
Let $S$ be the sequential execution order of region reads and writes.  Let $S'$ be any permutation of $S$ such that
for any sibling statements $s_1$ and $s_2$ such that $s_2$ depends on $s_1$, all statements in $\interval{s_1}$ precede
all statements in $\interval{s_2}$.  Then $S \equiv S'$.
\end{lemma}
\begin{proof}
It suffices to show that the order of any two dependent statements $s_1$ and $s_2$ is the same in $S$ and $S'$.
From the discussion above, we know that if $s_2$ depends on $s_1$ then there are two sibling statements $s_1'$ and $s_2'$
such that $s_1 \in \interval{s_1'}$, $s_2 \in \interval{s_2'}$, and $s_2'$ depends
on $s_1'$.
\end{proof}
The advantage of Lemma~\ref{lem:scheduling} is that given
the region read and write sets for function calls, the decision about
whether to execute two tasks in parallel can be made completely locally for the statements in a
single function body at a time, independent of any decisions for
any other functions.  
%This observation eliminates any global
%computation to determine what statements can execute in parallel, and
%furthermore allows scheduling decisions theselves to be parallelized, as the statements
%in different functions can be scheduled separately.  As a practical matter
%our Legion implementation only parallelizes function calls---the other
%statements within a function body are executed in sequential
%order.
Consider just the first two loops with subtasks in the example in Listing~\ref{lst:code_ex}.
The calls to {\tt calc\_new\_currents} are independent because the region arguments that are written
are all disjoint, and the Legion scheduler will run them in parallel, resources permitting.  Each of the following calls to
{\tt distribute\_charge} will be dependent on some of the preceding calls to {\tt calc\_new\_currents} and will be delayed until
all the tasks they depend on have completed.  If {\tt calc\_new\_current} or {\tt distribute\_charge} have tasks in their implementation, the scheduling for those tasks will be handled only considering the local dependences within their function bodies, and
correctness is still guaranteed by Lemma~\ref{lem:scheduling}.






   













