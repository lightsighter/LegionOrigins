
\section{Runtime Implementation}
\label{sec:impl}

There are currently two implementations of our interface:
one that works only on shared memory machines and another that
runs on larges clusters with both CPUs and GPUs.  The shared-memory
implementation uses the POSIX threads library.
The implementation for heterogeneous clusters also uses POSIX
threads\cite{PTHREADS} (commonly referred to as Pthreads) as 
well as the CUDA runtime library for GPUs\cite{CUDA} and the GASNet
API for large clusters\cite{GASNET07}.  

The heterogeneous implementation
models a cluster with CPUs and GPUs as having two types of processors 
and four types of memory.  Every CPU core is presented as a different CPU processor
and every GPU is a GPU processor.  This matches the scheduling granularity available
in the Pthreads and CUDA APIs respectively.  The first type of memory is the 
system memory that is accessible by every CPU core on a given node.  The second is the 
framebuffer memory on each GPU (and accessible only by that GPU).  The third type
of memory is system memory on a node that has been made accessible to the GPU(s) on
that node as well as the CPU cores, commonly referred to as {\em zero-copy memory}.
The final type of memory is the portion of system memory on each node that has been registered with
the GASNet runtime to allow {\em remote memory access} (RMA) by other nodes in the cluster.
%% For every node in the system
%% there is a memory corresponding to the DRAM associated with that node.  For
%% every GPU in the system there are two memories corresponding to the framebuffer
%% memory and the zero-copy memory associated with that GPU \footnote{Zero-copy 
%% memory contains pages that are mapped in both framebuffer and node memory with
%% coherence maintained by the PCI-E protocol.}. The last memory is 
%% a global GASNet memory that represents pages that have been registered 
%% with all nodes in the system for supporting remote memory accesses (RMAs).
%% The GASNet memory provides the illusion of global memory.  CPU processors
%% can directly access their node memory, all zero-copy memories for GPUs on
%% their node, and the GASNet memory.  GPU processors can only access their
%% framebuffer memory and their zero-copy memory.  Copies are permitted
%% between all pairs of memory except between any framebuffer memory and
%% the GASNet memory.

In addition to the RMA capabilities, GASNet provides {\em active messages} for 
inter-node communication.  Active messages consist of a command and payload that
are sent by one node to another without any previous coordination.  Upon arrival
at the destination node, a handler routine is invoked to process the payload.
The handler may issue a response to the sender, but this is optional, and avoided
as much as possible in our implementation due to the latencies involved in waiting
for a response.  As an example, consider the {\tt spawn} call on a {\tt Processor}
object.  If the processor is not on the same node as the requestor, the information
about the task (e.g. the task ID and arguments, the actual target {\tt Processor},
the prerequisite event (if any), and the completion event ID) is sent in an
active message to the node containing the target processor.  The recipient node
unpacks the message, and either places the task on the ready queue for the target
processor or marks it as being deferred until the prerequisite event has triggered.
No response is sent to the original requestor.  The requestor already knows the event
that corresponds to the completion of this task (since it was provided in the active
message) - there is no utility in knowing exactly when the active message itself has
been processed.

%% To support inter-node operations our implementation relies on GASNet active
%% messages for communication.  Active messages are a command and a payload
%% that are sent remotely and cause a handler to be run on the target node.
%% A simple example of our use of active messages can be illustrated by the 
%% {\tt spawn} call on a {\tt Processor} object.  Consider the case where {\tt spawn}
%% is invoked on a processor that is on a remote node.  This operation is converted into
%% an active message that contains all the information about the task launch.
%% When the active message is handled by the remote node, the task and all its
%% metadata are registered with the processor on which the task was launched.
%% While most of the implementation of our heterogeneous runtime is straight
%% forward, the next three sections describe in greater detail the nuances
%% of the implementation of events, locks, and reduction instances.

%% Our implementation uses a hierarchical model, in which data structures are shared by
%% all threads on the same node (taking advantage of the shared address space and low
%% latencies of using Pthreads mutexes and/or lock-free data structures).

Due to space constraints, we will not cover every part of our implementation in
detail, instead limiting our discussion to the three most interesting aspects:
events, locks, and reduction instances.

\subsection{Event Implementation}
\label{subsec:eventimpl}

Events may be created on demand, and are always {\em owned} by the node on which the
creation request was made.  The creation of an event occurs with no inter-node communcation.
The set of event
handles is divided across the nodes at startup time, allowing each node to assign handles
to new events without fear of conflicting with another node's events.  No broadcast of
the event creation is required either, as the static division is sufficient to allow any
node to correctly determine an event's owning node when (and only if) an operation on that
event is performed on the other node.

At event creation time, the owning node allocates a data structure to track the state of the event
(i.e. triggered or not) as well as record a list of the operations that are known to be dependent
on the event (e.g. initiation of a copy operation, placing a task on a processor's ready queue, waking
up a waiting task).  However, the owning node only keeps a list of the dependent operations from the same
node.  Every other node also allocates a corresponding data structure (the first time the event
is referenced) to remember if the event has already triggered and to record its local dependent operations.
Arbitrarily many dependent operations from a
single node are aggregated into a single {\em event subscription} active message that is sent to the
owning node.  The owning node keeps a bitmask of which other nodes have subscribed to the event, and when the
event finally does trigger, a single {\em event trigger} active message is sent to each subscribing node,
which notes that the event has triggered (in case queries come after the trigger) and executes the list
of local operations that were dependent on that event.  (In the case that the triggering of the event has
occurred before the reception of an event subscription, a trigger message is sent immediatel to the 
new subscriber.)

The actual triggering of an event may occur on any node.  If it occurs on a node other than the owning
node, an {\em event trigger} active message is sent from the triggering node back to the owning node, which
then forwards that message to all subscribed nodes, with the exception of the triggering node, which was
able to execute its locally dependent operations (if any) immediately.  Although this can often result in
the latency of a triggering operation including two active messages, it bounds the number of active
messages required per event to $2N$ where $N$ is the number of nodes (which can be much smaller than the
number of dependent operations) interested in the event.  An alternative
would have been to share the subscriber bitmask so that the triggering node can notify all interested
nodes directly, but such an algorithm is much more complicated due to race conditions, and requires $O(N^2)$
active messages.  As we will see in section~\ref{sec:microbenchmarks}, the latency of a single extra
active message is very small, and any cost that is super-linear with the node count either is or will soon
be an scalability issue for large systems.

The data structures used to track an event cannot be freed until all operations on that event have been
performed.  Creation and triggering can happen only once each, but there can be any number of operations
that are dependent on an event, and some of those operations may not be requested until long after the
event has triggered.  Other systems incorporating events address this by reference counting event
handles\cite{Khronos:OpenCL}, but such reference counting adds both programmer and runtime overhead even when
limited to a single node -- further overhead and complexity would be expected for a cluster-level
reference-counting approach.

Instead of focusing on freeing event data structures, our implementation aggressively recycles them, 
needing fewer total event data structures than a referencing counting implementation while also eliminating
the overhead of reference counting.  The key observation is that one {\em generational event} data structure
 can efficiently capture the state of one {\em untriggered} event and a very large number (e.g. $2^{32}-1$)
of already-triggered events.  An event's handle is expanded to include its generation number as well as the
identifier for the underlying generational event.  In addition to the triggered-or-not state and list of
dependent operations
for the most recent generation, the owning node also remembers how many previous generations have already
been triggered.  (There is no need to keep a list of dependent operations on previous generations.  Any
new operation that is dependent on a generation that is known to be already triggered can immediately be
executed.)  When an event is created, any generational event for whom the most recent generation has 
triggered can be reused - the generation is increased by one and the state is returned to ``untriggered.''
As before, this can be done with no inter-node communication.

A remote node's data structure is also efficient - the boolean values for whether the event
has triggered and whether the event has already been subscribed to are replaced with the numbers of the most
recent generation known to have triggered (this is received in the event trigger active message) and of the
most recent generation the remote node has subscribed to (this is sent in the event subscription active
message).  And as with the owner node's data structure, only
one list of dependent operations is needed.  Although the latencies of a large system can delay the
reception of an event trigger active message, if an operation is created that depends on a later generation
of an event than what the current list of events is dependent on, that serves as a roundabout indication that
the previous generation has indeed triggered, allowing the existing list of operations to be executed.


%% Events are created on demand by each node.  The ID of the creating node is encoded
%% in the upper bits of each event's ID and the creating node is said to be the 
%% event's {\em owner}.  If an event is used as an argument to an
%% operation on a remote node then the remote node can determine the owner of 
%% the event and send an active message to become a {\em subscriber} of the event.  When a
%% remote node becomes a subscriber to an event it guarantees that it will receive an active
%% message from the event's owner when the event triggers.  To minimize the amount
%% of communication that occurs between nodes, each node remembers the events to which it
%% is subscribed and guarantees that it only subscribes to an event once.  In the case
%% where there may be many remote waiters on an event 
%% this can dramatically reduce the number of inter-node active messages.

%% When an event triggers, it automatically notifies all of the operations that are waiting
%% on its local node.  The owner node will also send active messages to all of the subscriber nodes
%% telling them that the event has triggered.  Even though the event has now notified all of
%% its waiters, the memory on the owning node for storing the event cannot be reclaimed 
%% because there still may be handles to the event in the system which will request to
%% use this event in the future.  These requests will need to be satisfied saying that 
%% the event has already triggered for the duration of the application.  Rather than waste
%% the resources, we recycle event implementations.

%% A {\em dynamic event} is a logical construct that will only be triggered once and is the level
%% of abstraction at which the programmer reasons about events.  A {\em physical event} is an
%% actual event implementation that consumes resources on its owner node.  Multiple dynamic events
%% can be mapped to a single physical event.  However, for each physical event there can be at most
%% one {\em active} event in the set of dynamic events that are mapped to it.  An active event is a 
%% dynamic event that is yet to trigger.  By only having one active dyanmic event at a time 
%% there is no need to disambiguate event triggers that are sent to the physical event.  Each
%% physical event maintains a count of the number of trigger operations that it has observed.

%% When the runtime needs to return a handle corresponding to a new dynamic event, it finds
%% a physical event which currently has no active events (e.g. the trigger count equals the number
%% of dynamic events mapped to the physical event).  The runtime increases the count
%% of the number of dynamic events mapped to the physical event.  The handle that is returned 
%% corresponds to a new dynamic event which contains an ID that refers to the physical
%% event it is mapped to and a {\em generation} that is the dynamic event number for the particular
%% physical event.  Note that by definition the generation will always be one greater than the
%% trigger count at the time of dynamic event creation.

%% Within this framework it is now very easy to test whether the dynamic event specified by a
%% handle has triggered.  Using the ID that is contained within the handle we find the
%% physical event implementation (which may require an active message if the owner node is
%% remote from where the test is begin performed).  We can then compare the generation of
%% the handle to the number of observed triggers contained by the physical event.  If the
%% number of physical triggers is greater than or equal to the handle's generation, then
%% the dynamic event has triggered.  We show in Section~\ref{sec:apps} that in practice
%% the number of needed physical events is significantly less than the number of dynamic events.

%% We also leverage the mapping of dynamic events onto physical events to improve the efficiency
%% of subscriptions.  Nodes cache the last generation for every physical event for which they
%% have seen a trigger notification.  If the trigger test described above detects a trigger based
%% on the cached information then there would be no need for the subscription which would save an
%% active message.  If the event has not been detected to have
%% triggered locally then an active message is sent to the owner which would always have been necesary
%% without the dynamic to physical event mapping.

\subsection{Lock Implementation}
\label{subsec:lockimpl}

\subsection{Reduction Instances}
\label{subsec:reducimpl}
