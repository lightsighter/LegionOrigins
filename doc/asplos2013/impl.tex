
\section{Runtime Implementation}
\label{sec:impl}

There are currently two implementations of our interface:
one that works only on shared memory machines and another that
runs on larges clusters with both CPUs and GPUs.  The shared-memory
implementation uses the POSIX threads library.
The implementation for heterogeneous clusters also uses POSIX
threads\cite{PTHREADS} (commonly referred to as Pthreads) as 
well as the CUDA runtime library for GPUs\cite{CUDA} and the GASNet
API for large clusters\cite{GASNET07}.  

The heterogeneous implementation
models a cluster with CPUs and GPUs as having two types of processors 
and four types of memory.  Every CPU core is presented as a different CPU processor
and every GPU is a GPU processor.  This matches the scheduling granularity available
in the Pthreads and CUDA APIs respectively.  The first type of memory is the 
system memory that is accessible by every CPU core on a given node.  The second is the 
framebuffer memory on each GPU (and accessible only by that GPU).  The third type
of memory is system memory on a node that has been made accessible to the GPU(s) on
that node as well as the CPU cores, commonly referred to as {\em zero-copy memory}.
The final type of memory is the portion of system memory on each node that has been registered with
the GASNet runtime to allow {\em remote memory access} (RMA) by other nodes in the cluster.
%% For every node in the system
%% there is a memory corresponding to the DRAM associated with that node.  For
%% every GPU in the system there are two memories corresponding to the framebuffer
%% memory and the zero-copy memory associated with that GPU \footnote{Zero-copy 
%% memory contains pages that are mapped in both framebuffer and node memory with
%% coherence maintained by the PCI-E protocol.}. The last memory is 
%% a global GASNet memory that represents pages that have been registered 
%% with all nodes in the system for supporting remote memory accesses (RMAs).
%% The GASNet memory provides the illusion of global memory.  CPU processors
%% can directly access their node memory, all zero-copy memories for GPUs on
%% their node, and the GASNet memory.  GPU processors can only access their
%% framebuffer memory and their zero-copy memory.  Copies are permitted
%% between all pairs of memory except between any framebuffer memory and
%% the GASNet memory.

In addition to the RMA capabilities, GASNet provides {\em active messages} for 
inter-node communication.  Active messages consist of a command and payload that
are sent by one node to another without any previous coordination.  Upon arrival
at the destination node, a handler routine is invoked to process the payload.
The handler may issue a response to the sender, but this is optional, and avoided
as much as possible in our implementation due to the latencies involved in waiting
for a response.  As an example, consider the {\tt spawn} call on a {\tt Processor}
object.  If the processor is not on the same node as the requestor, the information
about the task (e.g. the task ID and arguments, the actual target {\tt Processor},
the prerequisite event (if any), and the completion event ID) is sent in an
active message to the node containing the target processor.  The recipient node
unpacks the message, and either places the task on the ready queue for the target
processor or marks it as being deferred until the prerequisite event has triggered.
No response is sent to the original requestor.  The requestor already knows the event
that corresponds to the completion of this task (since it was provided in the active
message) - there is no utility in knowing exactly when the active message itself has
been processed.

%% To support inter-node operations our implementation relies on GASNet active
%% messages for communication.  Active messages are a command and a payload
%% that are sent remotely and cause a handler to be run on the target node.
%% A simple example of our use of active messages can be illustrated by the 
%% {\tt spawn} call on a {\tt Processor} object.  Consider the case where {\tt spawn}
%% is invoked on a processor that is on a remote node.  This operation is converted into
%% an active message that contains all the information about the task launch.
%% When the active message is handled by the remote node, the task and all its
%% metadata are registered with the processor on which the task was launched.
%% While most of the implementation of our heterogeneous runtime is straight
%% forward, the next three sections describe in greater detail the nuances
%% of the implementation of events, locks, and reduction instances.

%% Our implementation uses a hierarchical model, in which data structures are shared by
%% all threads on the same node (taking advantage of the shared address space and low
%% latencies of using Pthreads mutexes and/or lock-free data structures).

Due to space constraints, we will not cover every part of our implementation in
detail, instead limiting our discussion to the three most interesting aspects:
events, locks, and reduction instances.

\subsection{Event Implementation}
\label{subsec:eventimpl}

Events may be created on demand, and are always {\em owned} by the node on which the
creation request was made.  The creation of an event occurs with no inter-node communcation.
The set of event
handles is divided across the nodes at startup time, allowing each node to assign handles
to new events without fear of conflicting with another node's events.  No broadcast of
the event creation is required either, as the static division is sufficient to allow any
node to correctly determine an event's owning node when (and only if) an operation on that
event is performed on the other node.

At event creation time, the owning node allocates a data structure to track the state of the event
(i.e. triggered or not) as well as record a list of the operations that are known to be dependent
on the event (e.g. initiation of a copy operation, placing a task on a processor's ready queue, waking
up a waiting task).  However, the owning node only keeps a list of the dependent operations from the same
node.  Every other node also allocates a corresponding data structure (the first time the event
is referenced) to remember if the event has already triggered and to record its local dependent operations.
Arbitrarily many dependent operations from a
single node are aggregated into a single {\em event subscription} active message that is sent to the
owning node.  The owning node keeps a bitmask of which other nodes have subscribed to the event, and when the
event finally does trigger, a single {\em event trigger} active message is sent to each subscribing node,
which notes that the event has triggered (in case queries come after the trigger) and executes the list
of local operations that were dependent on that event.  (In the case that the triggering of the event has
occurred before the reception of an event subscription, a trigger message is sent immediatel to the 
new subscriber.)

The actual triggering of an event may occur on any node.  If it occurs on a node other than the owning
node, an {\em event trigger} active message is sent from the triggering node back to the owning node, which
then forwards that message to all subscribed nodes, with the exception of the triggering node, which was
able to execute its locally dependent operations (if any) immediately.  Although this can often result in
the latency of a triggering operation including two active messages, it bounds the number of active
messages required per event to $2N$ where $N$ is the number of nodes (which can be much smaller than the
number of dependent operations) interested in the event.  An alternative
would have been to share the subscriber bitmask so that the triggering node can notify all interested
nodes directly, but such an algorithm is much more complicated due to race conditions, and requires $O(N^2)$
active messages.  As we will see in section~\ref{sec:microbenchmarks}, the latency of a single extra
active message is very small, and any cost that is super-linear with the node count either is or will soon
be an scalability issue for large systems.

The data structures used to track an event cannot be freed until all operations on that event have been
performed.  Creation and triggering can happen only once each, but there can be any number of operations
that are dependent on an event, and some of those operations may not be requested until long after the
event has triggered.  Other systems incorporating events address this by reference counting event
handles\cite{Khronos:OpenCL}, but such reference counting adds both programmer and runtime overhead even when
limited to a single node -- further overhead and complexity would be expected for a cluster-level
reference-counting approach.

Instead of focusing on freeing event data structures, our implementation aggressively recycles them, 
needing fewer total event data structures than a referencing counting implementation while also eliminating
the overhead of reference counting.  The key observation is that one {\em generational event} data structure
 can efficiently capture the state of one {\em untriggered} event and a very large number (e.g. $2^{32}-1$)
of already-triggered events.  An event's handle is expanded to include its generation number as well as the
identifier for the underlying generational event.  In addition to the triggered-or-not state and list of
dependent operations
for the most recent generation, the owning node also remembers how many previous generations have already
been triggered.  (There is no need to keep a list of dependent operations on previous generations.  Any
new operation that is dependent on a generation that is known to be already triggered can immediately be
executed.)  When an event is created, any generational event for whom the most recent generation has 
triggered can be reused - the generation is increased by one and the state is returned to ``untriggered.''
As before, this can be done with no inter-node communication.

A remote node's data structure is also efficient - the boolean values for whether the event
has triggered and whether the event has already been subscribed to are replaced with the numbers of the most
recent generation known to have triggered (this is received in the event trigger active message) and of the
most recent generation the remote node has subscribed to (this is sent in the event subscription active
message).  And as with the owner node's data structure, only
one list of dependent operations is needed.  Although the latencies of a large system can delay the
reception of an event trigger active message, if an operation is created that depends on a later generation
of an event than what the current list of events is dependent on, that serves as a roundabout indication that
the previous generation has indeed triggered, allowing the existing list of operations to be executed.

Barriers are implemented as an extension of events.  In place of an event's boolean triggered-or-not state,
a barrier's owner tracks how many arrivals are still expected for the event.  When the arrival count 
reaches zero, the barrier is considered to have ``triggered''.  Other nodes send {\em barrier update}
active messages containing an adjustment value that is positive for {\tt alter\_arrival\_count} operations 
and negative for {\tt arrive} operations.  A simplified version of vector clocks\cite{Fidge1998} is used
to detect the race condition in which the barrier owner receives an {\tt arrive} operation before the
corresponding {\tt alter\_arrival\_count} and defer the decrement of the arrival count to avoid a spurious
triggering of the barrier.

%% Events are created on demand by each node.  The ID of the creating node is encoded
%% in the upper bits of each event's ID and the creating node is said to be the 
%% event's {\em owner}.  If an event is used as an argument to an
%% operation on a remote node then the remote node can determine the owner of 
%% the event and send an active message to become a {\em subscriber} of the event.  When a
%% remote node becomes a subscriber to an event it guarantees that it will receive an active
%% message from the event's owner when the event triggers.  To minimize the amount
%% of communication that occurs between nodes, each node remembers the events to which it
%% is subscribed and guarantees that it only subscribes to an event once.  In the case
%% where there may be many remote waiters on an event 
%% this can dramatically reduce the number of inter-node active messages.

%% When an event triggers, it automatically notifies all of the operations that are waiting
%% on its local node.  The owner node will also send active messages to all of the subscriber nodes
%% telling them that the event has triggered.  Even though the event has now notified all of
%% its waiters, the memory on the owning node for storing the event cannot be reclaimed 
%% because there still may be handles to the event in the system which will request to
%% use this event in the future.  These requests will need to be satisfied saying that 
%% the event has already triggered for the duration of the application.  Rather than waste
%% the resources, we recycle event implementations.

%% A {\em dynamic event} is a logical construct that will only be triggered once and is the level
%% of abstraction at which the programmer reasons about events.  A {\em physical event} is an
%% actual event implementation that consumes resources on its owner node.  Multiple dynamic events
%% can be mapped to a single physical event.  However, for each physical event there can be at most
%% one {\em active} event in the set of dynamic events that are mapped to it.  An active event is a 
%% dynamic event that is yet to trigger.  By only having one active dyanmic event at a time 
%% there is no need to disambiguate event triggers that are sent to the physical event.  Each
%% physical event maintains a count of the number of trigger operations that it has observed.

%% When the runtime needs to return a handle corresponding to a new dynamic event, it finds
%% a physical event which currently has no active events (e.g. the trigger count equals the number
%% of dynamic events mapped to the physical event).  The runtime increases the count
%% of the number of dynamic events mapped to the physical event.  The handle that is returned 
%% corresponds to a new dynamic event which contains an ID that refers to the physical
%% event it is mapped to and a {\em generation} that is the dynamic event number for the particular
%% physical event.  Note that by definition the generation will always be one greater than the
%% trigger count at the time of dynamic event creation.

%% Within this framework it is now very easy to test whether the dynamic event specified by a
%% handle has triggered.  Using the ID that is contained within the handle we find the
%% physical event implementation (which may require an active message if the owner node is
%% remote from where the test is begin performed).  We can then compare the generation of
%% the handle to the number of observed triggers contained by the physical event.  If the
%% number of physical triggers is greater than or equal to the handle's generation, then
%% the dynamic event has triggered.  We show in Section~\ref{sec:apps} that in practice
%% the number of needed physical events is significantly less than the number of dynamic events.

%% We also leverage the mapping of dynamic events onto physical events to improve the efficiency
%% of subscriptions.  Nodes cache the last generation for every physical event for which they
%% have seen a trigger notification.  If the trigger test described above detects a trigger based
%% on the cached information then there would be no need for the subscription which would save an
%% active message.  If the event has not been detected to have
%% triggered locally then an active message is sent to the owner which would always have been necesary
%% without the dynamic to physical event mapping.

\subsection{Lock Implementation}
\label{subsec:lockimpl}

Like events, locks are created on demand by the node on which the creation request was made.  The lock
handles are also statically divided across the nodes, allowing the creation to be done without inter-node
communication.  Unlike events whose ownership is static, locks may migrate, so although the {\em creating}
node is the initial owner of a lock, that ownership can be transferred to other nodes.

Locks are often used to mediate access to data, and the size of that data is often relatively small.  To
avoid adding the latency of inter-node data copies to critical sections, a lock may also be created with a
{\em payload}, a small (i.e. less than 4KB - the limit for an active message's payload) piece of data that is moved around with the lock and guaranteed to be coherent if
the lock is held.

Since any node may at some point be the owner of a lock, all nodes use the same data structure to track the
state of the lock (they differ only in when the structure is allocated -
nodes other than the creator still wait until the first reference to the lock on that node).  The structure
tracks the following:
\begin{itemize}
\item {\em owner node} - the most recently known owner of the lock.  If the current node is the owner, this
information is always accurate.  If not, this information may be stale, but at least the recorded ``owner''
is sure to have more up-to-date information.
\item {\em lock status} - records whether the lock is currently held.  Only the current lock owner has
guaranteed-current information here.
\item {\em local waiters} - a list of pending local lock requests.  This data is always valid as well.
\item {\em remote waiters} - a bitmask of which other nodes have pending requests.  This bitmask is only
used when the current node is the owner of the lock.
\item {\em local payload pointer and size} - the local node's copy of the lock payload
\end{itemize}

When a lock request is made, an event is created to track when the grant occurs.  The current node then
examines its copy of the lock data structure (creating it if
this the first operation for that lock on that node).  If the current node is the owner and the lock isn't
held, the lock is granted immediately and the event is triggered.  If the lock is held, the event is added
to the list of local waiters.  (This is the only thing that is stored.  The actual ``requestor'' of the lock
is implicitly captured in which operations are dependent on the lock grant event.)  If the current node
isn't the owner, a {\em lock request} active message is
sent to the most recently known owner.  (If that information proves to be stale, the request is forwarded
by the receiving node, eventually catching up to the current owner.)  If the current owner's status shows
the lock is currently held, the requesting node is added to the remote waiters bitmask.  If the lock is
not held, the ownership of the lock is given to the requesting node via a {\em lock transfer} active
message.  A lock transfer message includes the current remote waiters bitmask (in case other nodes are
have also requested the lock) and an up-to-date copy of the lock's payload.

Similarly, an unlock request is first checked against the local node's lock state.  If the local node is
not the owner, an {\em unlock} active message is sent to the most recently known owner, which is forwarded
if necessary.  Once the unlock request is on the lock's current owning node, the local waiter list is
examined.  If the list is non-empty, the lock remains in the ``locked'' state and the first lock grant
event is pulled off the local waiter list and triggered.  If instead the local waiter list is empty, the
lock state is changed to ``unlocked'', and the remote waiter bitmask is examined.  If any bits are set, then
one of them is chosen and the corresponding node becomes the new owner via a lock transfer active message.

The unfairness that results from a lock favoring local waiters over remote waiters is intentional.  When the
contention on a lock is high (the only time when the question of fairness is even relevant), the latencies
involved in transferring a lock between nodes can be the limiter on throughput.  By minimizing the number
of lock transfers, the throughput on the lock is maximized.

A similar issue can arise with the forwarding of lock request active messages.  If two nodes are
transferring a lock very quickly back and forth, a third node's request could be forwarded an arbitrary
number of times as it chases the lock around.  Although this can result in unfairness, it doesn't 
prevent forward progress.  A lock is only transferred to a node with at least one local waiter, so if 
the third node's request ever became the only one left, the lock transferring would stop and the request
would finally catch up.

\subsection{Reduction Instances}
\label{subsec:reducimpl}

In addition to normal physical regions, our implementation supports two different kinds of reduction-only
physical regions, designed to optimize the common pattern in which multiple tasks make commutative 
{\em reductions} into shared data.  Instead of having to move the existing data to a suitable memory and
then applying individual reduction operations to the existing data 
right away (as would happen in a normal physical region), reduction instances allow a large number of
reduction operations to be batched up and transferred efficiently to the original location of the data.
This decoupling from the original location of the data can allow allow greater parallelism
in cases where there are available processors that cannot all access the same memory.

The first kind of reduction-only physical region is a {\em reduction fold instance}, which may only be
used for reduction operations that support a {\em fold} operation.  A reduction fold instance is similar
to a normal physical region in that it is implemented as an array, indexed by the same element indices used
for the normal physical regions.  The difference is that each array element is a value of the reduction's
{\tt RHS} type instead of the actual element type of the region (i.e. the {\tt LHS} type).  When the client
requests a reduction operation to a particular location, the supplied right-hand-side value is folded in
to the corresponding location in the array.  An arbitrary number of reductions can be done to each location.
When the reduction fold instance is copied back to a normal instance, the contents of the reduction instance
(which are generally the same size or smaller than the normal region contents) are transferred in a large
block to the location of the normal instance, where an application of the reduction function is performed
once for each location in the region via a cache-friendly linear sweep over the memory.

The second kind of reduction instance is a {\em reduction list instance}.  Although it generally doesn't
perform as well as a reduction fold instance, it can be better for cases with sparse updates.  It is also
the only option when a reduction operation doesn't support a {\em fold} operation.  Instead of folding
together multiple reductions to the same location, a reduction list instance keeps a log of every
reduction operation requested (i.e. the location and the right-hand-side value).  When the reduction list 
instance is copied back to a normal physical region, the whole list is transferred and then replayed at
the target region's location.  Except in sparse update cases, this list is often larger than the target
region, and any randomness in the sequence of locations in the list can impact the memory access efficiency
of the accesses to the target region.  However, as we will show in section~\ref{sec:microbenchmarks}, 
the benefits of enabling parallelism and hiding latency usually more than make up for these drawbacks.
