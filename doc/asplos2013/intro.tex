
\section{Introduction}
\label{sec:intro}

% Big machines -> latencies are growing
% Processors can't be blocked, need to be able to continue handling work
% Operations need to be composable
% deferred execution model

% Contributions:
% - An event system where every operation can take an event and returns an event
% - Handles are globally valid
% - More flexible synchronization primitive for this environment: deferred locks
% - Physical regions for data movement including special versions for reductions

% Parallel programs have three parts
%  Parallel control
%  Data movement in the memory hierarchy
%  Synchronization

% Bulk-synchronous
% asynchronous components are not composable

A characteristic feature of current and future heterogeneous
supercomputers is the large, variable, and growing latencies between
components.  The well-known mechanism for masking unpredictable or
long latencies is to use {\em asynchronous operations} that do not block,
thereby enabling other useful work to be done while the asynchronous
operations complete. Current implementation primitives for parallel
programming systems, however, rely on blocking constructs that were
designed for machines many orders of magnitude
smaller\cite{MPI,COARRAY_FORTRAN,UPC99}; these primitives expose
latency on today's large distributed, heterogeneous parallel machines.


Most current programming systems for large-scale parallel processing provide some variant
of the {\em bulk-synchronous} execution model\cite{Valiant90}.  Bulk-synchronous models
partition computations into {\em phases} that are either parallel computation, data
communication, or synchronization.  At any given point in time all processors
are engaged in the same kind of phase.  Bulk-synchrony
makes it easy to reason about parallel execution but
hinders attempts to hide the increasingly long-latency communication
or memory movement phases.  
%In older machines this was not a problem because slower
%processors and smaller machines meant computation phases were much larger than
%communication or synchronization phases.  However, today's machines are much
%larger with faster processors, causing an increasing percentage of time
%to be spent in the communication and synchronization phases.

To combat this problem, parallel programming libraries have introduced
asynchronous constructs allowing programmers to overlap phases.  For
example, MPI has asynchronous {\em send} and {\em receive} operations
for sending messages in parallel with computation \cite{MPI}.  There
are two problems with this approach.  First, not all phases support
asynchronous constructs.  Second, the asynchronous operations do not
compose.  There is no way to express that one asynchronous computation
should begin as soon as another asynchronous communication operation
has completed.  The result is that at some point there must be
blocking operations to coordinate synchronous and asynchronous phases.
If blocking calls are not accurately placed in the program they can
lead to processor stalls that expose latency.  Correct placement of
blocking calls can be dependent on input data, algorithmic decisions
like blocking factors, and the underlying hardware.  To hide as much
latency as possible and to minimize processor stalls, 
%regardless of inputs, algorithmic choices, or underlying hardware,
all three major aspects of parallel programs (computation, data movement,
synchronization) should be asynchronous and composable.

We propose a new low-level interface for programming modern
supercomputers that is fully asynchronous. In our system, one can
asynchronously compose computation, data movement, and
synchronization.  The basic mechanism that enables composition of operations
is an underlying {\em event} primitive.  Events provide a mechanism
for naming a point in the future when an asynchronous operation
(computation, data copy, or synchronization) will complete.  Every
call to perform an operation $op$ in our runtime returns immediately
with an event that {\em triggers} when $op$ has completed.
Furthermore, every call to perform an operation $op$ can take as a
precondition an event that must trigger before $op$ can begin.  Using
events programmers can compose arbitrary chains of dependent
operations, which the runtime is free to execute in any way that
respects the event dependences.  With this freedom the runtime can
schedule operations to optimize throughput while hiding the long
latency operations.

The following sections describe our contributions:
\begin{itemize}
\item We present an interface for asynchronously composing
parallel computation, data movement, and synchronization using events (Section~\ref{sec:interface}).  

\item To support synchronization in our event system we introduce {\em deferred locks}, 
a novel synchronization primitive that operates in an asynchronous enviroment (Section~\ref{subsec:locks}).

\item We describe a physical region system for the management of data that abstracts
data layouts.  Abstracted data layouts enable composable reduction operations that can be
deferred to further hide latency (Section~\ref{subsec:phyreg}).

\item Our approach depends on the efficient handling of very large numbers of events.
We describe a distributed implementation of our interface that requires no global coordination
and has modest local storage costs.  We describe optimizations for the implementation of events, 
deferred locks, and reductions (Section~\ref{sec:impl}).

\item We report on micro-benchmarks that are designed to stress
our implementation.  The results indiciate that our implementation approaches the performance of
the underlying hardware (Section~\ref{sec:micro}).

\item We demonstrate that our interface is powerful enough to support Legion \cite{Legion12}, a higher-level
programming system, and that real Legion applications programmed to our interface
are capable of achieving high-performance (Section~\ref{sec:apps}).  Furthermore, we demostrate
that the same applications written in a bulk-synchronous model incur performance penalites between 22-135\%.
\end{itemize}

Section~\ref{sec:related} describes related work and Section~\ref{sec:conclusion}
concludes.

