

\section{Introduction}
\label{sec:intro}

In the past decade machine architecture, particularly at the high
performance end of the spectrum, has undergone a revolution.  The
latest supercomputers are now complex hierarchies of many different
kinds of computing technologies: networks of nodes at the top level,
multiple chips per node, mutiple cores within a chip, and, most
recently, multiple accelerators (usually GPUs) per node, which 
can themselves be decomposed into several levels.   We present the operational and static
semantics of Legion, a programming model targeted at providing an
appropriate level of abstraction for programming such machines, one
that is both sufficiently high-level to be portable while still
exposing aspects that are crucial to performance. 

In Legion data is organized in a hierarchy of {\em regions}
and subregions while computation is organized in a hierarchy of {\em
tasks} and subtasks operating on regions.  Regions and tasks interact
through a static system of {\em region permissions} specifying 
operations a task may perform on a region argument (read,
write, or reduce) and  {\em region coherence} annotations that
express what other tasks may do concurrently with
the region (exclusive, atomic, or simultaneous).  We prove the
soundness of the permissions and coherence system and use these two
results to prove a third result: if two {\em siblings} (tasks that
have the same immediate parent task) $t_1$ and $t_2$ are 
{\em independent} (have no ordering requirements for correctness), 
then the unique ancestors of $t_1$ and $t_2$ in the task hierarchy 
are also independent.  This theorem is the basis for correct distributed
task scheduling in a Legion implementation, which is crucial for
performance; a centralized scheduler would be a bottleneck
because of the huge latencies involved in communicating in the target
class of machines.

% This is a description of how the listings should be formatted.
% It can go anywhere before the listings.
\lstset{
  captionpos=b,
  language=Haskell,
  basicstyle=\scriptsize,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  stepnumber=1,
  escapechar=\#,
  keepspaces=true,
  literate={<}{{$\langle$}}1 {>}{{$\rangle$}}1,
  morekeywords={task,rr,int,bool,isnull,partition,as,downregion,upregion,reads,writes,rdwrs,reduces,read,write,reduce,using,unpack,pack,coloring,color,newcolor,atomic,simultaneous},
  deletekeywords={float,head,min,max}
}

\begin{lstlisting}[float={t},label={lst:code_ex},caption={Circuit simulation.}]
type CircuitNode        = < float, --voltage
                            int >
type CircuitWire<rn,rg>  = < CircuitNode@rn,
                             CircuitNode@(rn,rg),
                             float> -- current

type node_list<rl,rn>       = < CircuitNode@rn, node_list<rl,rn>@rl >
type wire_list<rl,rw,rn,rg> = < CircuitWire<rn,rg>@rw, wire_list<rl,rw,rn,rg>@rl >

type CircuitPiece<rl,rw,rn> = rr[rpw,rpn,rg]
        < wire_list<rl,rpw,rpn,rg>@rl,
          node_list<rl,rpn>@rl >         
          where rpn <= rn and rg <= rn and rpw <= rw and
                rpn * rg and rn * rw and rl * rn and rl * rw

type piece_list<rl,rp,rw,rn> = < CircuitPiece<rl,rw,rn>@rp, piece_list<rl,rp,rw,rn>@rl >

function color_circuit[rn,rw,rl] ( all_nodes : node_list<rl,rn>@rl,
                               all_wires : wire_list<rl,rw,rn>@rl ), reads(rn,rw,rl) : 
                               <coloring(rn), coloring(rn), coloring(rw)> =  
  -- Invoke programmer chosen coloring algorithm (e.g. METIS)

function build_lists[rl,rw,rn,rpw,rpn,rg] ( all_nodes : node_list<rl,rn>@rl, all_wires : wire_list<rl,rw,rn>@rl,
                                 owned_coloring : coloring(rn), ghost_coloring : coloring(rn),
                                 wires_coloring : coloring(rw), c : int), reads(rn,rw,rl), writes(rl) :
                                 < wire_list<rl,rpw,rpn,rg>@rl, node_list<rl,rpn>@rl > = 
  -- Construct node and wire lists for each piece based on coloring

function build_piece_list[rl,rp,rw,rn] ( one: CircuitPiece<rl,rw,rn>, two : CircuitPiece<rl,rw,rn>,
                                 three: CircuitPiece<rl,rw,rn>, four : CircuitPiece<rl,rw,rn> ),
                                 reads(rl,rp,rw,rn), writes(rl,rp) : piece_list<rl,rp,rw,rn>@rl =
  -- Build a list of the pieces

function simulate_circuit[rl,rw,rn] ( all_nodes : node_list<rl,rn>@rl,
                                  all_wires : wire_list<rl,rw,rn,rn>@rl, steps : int ),
                          reads(rn,rw,rl), writes(rn,rw,rl) : bool = 
  let pc : <coloring(rn),<coloring(rn),coloring(rw)> >
                      = color_circuit[rn,rw,rl](all_nodes,all_wires) in
  partition rn using pc.1 as rn0,rn1,rn2,rn3 in
  partition rn using pc.2 as rg0,rg1,rg2,rg3 in
  partition rw using pc.3 as rw0,rw1,rw2,rw3 in
  let piece0_lists : < wire_list<rl,rw0,rn0,rg0>@rl, node_list<rl,rn0>@rl > = 
           build_lists[rl,rw,rn,rw0,rn0,rg0](all_nodes,all_wires,pc.1,pc.2.1,pc.2.2,0) in
  let piece0 : CircuitPiece<rl,rw,rn> = pack piece0_lists as CircuitPiece<rl,rw,rn>[rw0,rn0,rg0] in
  let piece1_lists : < wire_list<rl,rw1,rn1,rg1>@rl, node_list<rl,rn1>@rl > = 
           build_lists[rl,rw,rn,rw1,rn1,rg1](all_nodes,all_wires,pc.1,pc.2.1,pc.2.2,1) in
  let piece1 : CircuitPiece<rl,rw,rn> = pack piece1_lists as CircuitPiece<rl,rw,rn>[rw1,rn1,rg1] in
  let piece2_lists : < wire_list<rl,rw2,rn2,rg2>@rl, node_list<rl,rn2>@rl > = 
           build_lists[rl,rw,rn,rw2,rn2,rg2](all_nodes,all_wires,pc.1,pc.2.1,pc.2.2,2) in
  let piece2 : CircuitPiece<rl,rw,rn> = pack piece2_lists as CircuitPiece<rl,rw,rn>[rw2,rn2,rg2] in
  let piece3_lists : < wire_list<rl,rw3,rn3,rg3>@rl, node_list<rl,rn3>@rl > = 
           build_lists[rl,rw,rn,rw3,rn3,rg3](all_nodes,all_wires,pc.1,pc.2.1,pc.2.2,3) in
  let piece3 : CircuitPiece<rl,rw,rn> = pack piece3_lists as CircuitPiece<rl,rw,rn>[rw3,rn3,rg3] in
  let all_pieces : pieces_list<rl,rl,rw,rn> = build_piece_list[rl,rl,rw,rn](piece0,piece1,piece2,piece3) in
      execute_time_step[rl,rl,rw,rn](all_pieces,steps)

function execute_time_step[rl,rp,rw,rn] ( all_pieces : pieces_list<rl,rp,rw,rn>@rl ),
                                          reads(rl,rp,rw,rn), writes(rw,rn) : bool =
  if steps < 1 then true else
  let all_pieces' : pieces_list<rl,rp,rw,rn>@rl = map calc_new_currents[rl,rw,rn] all_pieces in
  let all_pieces'' : pieces_list<rl,rp,rw,rn>@rl = map distribute_charge[rl,rw,rn] all_pieces' in
  let all_pieces''' : pieces_list<rl,rp,rw,rn>@rl = map update_voltage[rl,rw,rn] all_pieces'' in
      execute_time_step[rl,rp,rw,rn](all_pieces''',steps-1)

function calc_new_currents[rl,rw,rn] ( p : CircuitPiece<rl,rw,rn> ), 
                                      reads(rl,rw,rn), writes(rw) : CircuitPiece<rl,rw,rn> =
  -- Calculate new currents for each wire

function distribute_charge[rl,rw,rn] ( p : CircuitPiece<rl,rw,rn> ), 
                      reads(rl,rw,rn), reduces(reduce_charge,rn), atomic(rn) : CircuitPiece<rl,rw,rn> =
  -- Distribute charge from each wire to all nodes

function update_voltage[rl,rw,rn] ( piece : CircuitPiece<rl,rw,rn> ), reads(rl,rw,rn), writes(rn) : bool = 
  -- Update voltage for all our owned nodes

\end{lstlisting}

Legion programs organize data into a forest of {\em logical regions}.  Logical
regions express {\em locality} (data that will be used together, and therefore colocated if possible in the same
physical memory) and {\em independence} (disjoint data that may be operated on in parallel).
Researchers have previously explored language designs with hierarchical decomposition of data to express
locality and independence for high performance programming. Two recent examples, Sequoia \cite{Fatahalion06} and 
Deterministic Parallel Java (DPJ) \cite{Bocchino09}, each provide a mechanism to partition the heap recursively
into a tree of collections of data.  The two designs are different in many aspects, but agree that there is
a single tree-shaped partitioning of data that can be checked fully statically (see Section~\ref{sec:related} for more
discussion of related work).

Our own experience with Sequoia and many high-performance applications
written in the current industry standard mix of MPI, shared-memory
threads, and CUDA, has taught us that for many practical situations a
single, fairly static partitioning is insufficient.  In practice, the
best way to partition data is often a function of the data
itself---i.e., the partitions need to be dynamically computed.
Furthermore, it is useful to allow mutliple partitions simultaneously of
the same data, providing different views on to that data. Thus, in Legion the 
handling of regions and partitioning of regions is much more dynamic than in 
previous designs.  In particular,
\begin{itemize}
\item  logical regions are first-class values in Legion
and may be dynamically allocated and stored in data structures;

\item logical regions can be {\em partitioned} into {\em subregions}; the programmer can express arbitrary partitions of
regions;

\item  a logical region may be dynamically partitioned in multiple different ways.
\end{itemize}
A consequence of these decisions is that, through partitioning, a
given datum may belong to multiple different regions.  For example, if
a region $R$ participates in three separate disjoint partition
operations and each partition allocates every element of $R$ to a
subregion (partitions need not be total in Legion), then every element
of $R$ is included in three different subregions.  This is why we use
the term {\em logical} regions: language-level regions serve to name
sets of data but do not imply anything about physical layout.  There
is a separate system of {\em physical regions} at runtime that hold
copies of the data in the language-level logical regions.  In fact,
the Legion implementation routinely maintains multiple distinct
physical regions, each with a copy of the data in the corresponding
logical region, for performance reasons.

A further consequence of placing the same data in multiple logical
regions as a result of runtime computation is that Legion has a very
hard static alias analysis problem on regions, which poses a problem
for parallel computation because knowing regions do not alias is the
basis for safe parallel execution.  This is why languages such as
Sequoia and DPJ allow only a single static partition of each
region---it simplifies static analysis of regions to the point that it
is practical.  

Legion's execution model is that by default, a program's semantics is
its meaning as a sequential program, which we refer to as the {\em
program order} execution.  While the
implementation is not our focus here, if two functions use
disjoint data (because all of the regions they use are disjoint), then
Legion may execute them simultaneously.  Also like DPJ,
Legion has a system of {\em permissions} ({\em read}, {\em write}, and
{\em reduce}) on regions and an orthogonal system of region {\em
coherence} modes ({\em excl}, {\em atomic}, and {\em simultaneous})
that increase the number of situations in which functions can be
executed in parallel.

The important difference bewteen Legion and the more static approaches
is that the hardest part of the tests to justify parallel execution is
done dynamically instead of statically.  The key observation is that
alias analysis is hard statically but very easy and relatively cheap
dynamically.  Thus, Legion falls between fully static systems, such as
Sequoia and DPJ, and fully dynamic approaches such as transactional
memory.  Legion still has a significant static component; in
particular, the safety of permissions is checked statically and region
pointer dereferences are checked statically.  The dynamic alias tests
are done at the granularity of regions and one check on a region
itself often serves to prove the safety of many subsequent region
accesses.  In contrast, because transactional memory has no mechanism
for grouping heap data into larger collections, it much test the
overlap between two sets of data on each individual element, which is
much more expensive.


We begin in Section~\ref{sec:example} with an example program that illustrates
a typical style of programming in Legion as well as motivating the need for multiple, dynamically computed  partitions of
a region.  We define Core Legion, a small language suitable for our formal development, in Section~\ref{sec:legioncore}.
The next four sections each state and prove one of our main results and contributions:
\begin{itemize}
\item We prove the soundness of Legion's permissions system (Section~\ref{sec:soundness}).

\item We use the soundness of permissions to prove the soundness of Legion's region coherence modes (Section~\ref{sec:coherence}).

\item We show that if expressions $e_1$ and $e_2$ are {\em non-interfering} (can be executed in parallel), then subexpressions
$e_1'$ of $e_1$ and $e_2'$ of $e_2$ are also non-interfering (Section~\ref{sec:scheduling}).  This result is the basis
for a hierarchical, distributed scheduler in the Legion implementation; on the target class of machines, any centralized
scheduler would be a serious bottleneck.

\item We give experimental evidence for the Legion design.  On three real-world applications, we show that 
dynamic region pointer checks would be expensive, justifying checking this aspect of the type system statically.
We also show that the cost of checking aliasing on region permissions is low, showing that a much more expressive
and dynamic langauge is not incompatible with high performance.

\end{itemize}
Finally, Section~\ref{sec:related} discusses the most closely related work and
Section~\ref{sect:conclusion} concludes.


  










