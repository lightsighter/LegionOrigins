
\section{Evaluation}
\label{sec:evaluation}

To determine the benefits of the Legion type system we evaluate
it on three criteria:
\begin{itemize}
\item Expressivity: can real applications be expressed (Section \ref{subsec:expressivity})
\item Overhead: can it reduce checking overhead (Section \ref{subsec:overhead})
\item Scalability: can it leverage hierarchical scheduling to scale (Section \ref{subsec:scalability})
\end{itemize}
Our evaluation uses a prototype implementation of the Legion langauge and programming model.
The prototype consists of two components: an implementation of a type checker
for the langauge introduced in Section \ref{subsec:langdef} and a C++ runtime library
for executing programs written in the Legion programming model\cite{Legion12}.  All experiments
are conducted on the Keeneland supercomputer\cite{Keeneland}.  Each node of the Keeneland
machine consists of two Xeon 5660 processors, three Tesla M2090 GPUs, and 24 GB of DRAM.  Nodes
are connected by a QDR Infiniband interconnect.

\subsection{Expressivity}
\label{subsec:expressivity}
We evaluate Legion on three real-world applications.  To qualitatively gauge the 
expressivity of the Legion type system, we introduce each of these applications in
the context of which features of the Legion type system are used in their implementations.

\subsubsection{Circuit Simulation}
\label{subsec:circuit}
The circuit application simulates the wires of a large integrated circuit using an RLC
model.  The computation consists of three phases for each time step: compute the current in each wire using
an iterative model, updated the charge in each node, and compute the voltage of each node.
The primary data structure in the circuit graph is a large irregular graph.  To execute
this computation in parallel the graph is dynamically partitioned into pieces to be
performed in parallel.  Our implementation creates separate regions for the wires and
nodes in the graph.  The wire and node regions are recusrively partitioned up into
subregions for pieces of the graph.  The node region is also partitioned an additional way to 
describe the sets of ghost nodes required for each piece providing multiple views onto the
same data.  The information about each piece of the graph is stored
in a region relationship that remembers the disjointness information for each piece
from other pieces.  Launching the tasks for the three phases of the computation make 
use of read, write, and reduce privileges as well as exclusive and simultaneous coherence.

\subsubsection{Fluid Simulation}
\label{subsec:fluid}
Our fluid application is based on the fluidanimate benchmark for the PARSEC benchmark
suite\cite{bienia11benchmarking}.  Fluid simulates the flow of an incompressible fluid
using particles that move between a regular grid of cells.  To perform operations in 
parallel the cells are partitioned into grids that can be executed in parallel.  Unlike
the Circuit application, the Fluid first creates regions and partitions them before
allocating cells in Regions.  Another difference between Fluid and Circuit is that Fluid
maintains separate regions for ghost cells rather using multiple views of
the regions containing shared data.  Region relationships are again used to remember
the disjointness of information for the regions of each grid.

\subsubsection{Adaptive Mesh Refinement}
\label{subsec:amr}
The third application is an adaptive mesh refinement (AMR) benchmark based on the third heat flow
example from the Berkeley Labs BoxLib project\cite{BoxLib}.  The algorithm solves the two
dimensional heat equation on a grid of cells using three levels of refinement with subrefinements
randomly placed on the surface.  For each time step in the application cells at the boundary of
a refinement are interpolated from cells at a coarser level of refinement, energy fluxes between
cells are computed, energy is transferred, and cells at a coarser level of refinement are restricted
to the values of refined cells.  The AMR application uses separate regions for every level of
refinement.  The regions at each level are partitioned several ways to provide multiple views of
the cells.  One partitioning separates cells into pieces that can be updated in
parallel.  Additional partitions are created for viewing data from coarser and finer levels of
the simulation.  Two types of region relationships are created: one for describing the pieces of
each level of refinement, and another for describing the relationship between pieces at different
levels of refinement.  These region relationships capture both intra- and inter-level disjointness
information.  The dynamic nature of AMR requires that regions can be created, partitioned,
and destroyed at runtime.  The many ways in which cells are used also mandates that we multiple
partitions of regions can be created.

These applications illustrate the expressivity of the Legion type system.  Legion is capable of 
expressing applications with both regular (Fluid,AMR) and irregular (Circuit) pointer data structres.
Being able to dynamically create, partition, and destroy regions at runtime is crucial to Legion's
ability to handle applications that make runtime decisions about how to partition data (AMR).  Legion
is able to capture both allocate-then-partition (Circuit) and partition-then-allocate (Fluid) ways 
of loading data.  Having multiple partitions of regions is necessary for describing the many ways 
that data can be accessed (Circuit,AMR).  All types of privileges and coherence are necessary for 
expressing the various applications presented in this paper (Circuit,Fluid,AMR,Histogram).  In
all applications region relationships allow for disjointness information to be conveyed despite
regions being a first-class feature.

\subsection{Checking Overhead}
\label{subsec:overhead}
Our initial implementation of Legion consisted only of a C++ library for Legion 
programs\cite{Legion12} which contained no checking of region memory accesses.  In the process
of implementing applications in Legion we frequently encountered memory corruption due to
illegal region accesses caused by bugs and a lack of checking.  Many times these corruptions 
occurred on remote nodes or on GPUs.  Finding these bugs required either multiple {\tt gdb} sessions 
for observing execution on multiple nodes simultaneously or using {\tt printf} for debugging on
GPUs.  To alleviate this problem we added a feature to our runtime system that dynamically checked all region 
accesses for both CPUs and GPUs to guarantee safety.  However, this feature
added considerable runtime overhead.  

To avoid the overhead of dynamic checks for all memory accesses we implemented a type checker of the
language in Section \ref{subsec:langdef}.  Using the type checker we were able to statically verify
the correctness of all region accesses and elide all the dynamic checks.  

%\begin{figure}
%\subfigure[48 Piece Data Set]
%{
%\includegraphics[scale=0.4]{figs/circuit_48_popl.pdf}
%\label{fig:ckt48}
%}
%\subfigure[96 Piece Data Set]
%{
%\includegraphics[scale=0.4]{figs/circuit_96_popl.pdf}
%\label{fig:ckt96}
%}
%\caption{Checking overhead of the Circuit simulation.\label{fig:ckt_overhead}}
%\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{figs/circuit_96_popl.pdf}
\end{center}
\vspace{-2mm}
\caption{Overhead for the Circuit simulation with 96 total pieces.\label{fig:ckt_overhead}}
\vspace{-6mm}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{figs/fluid_19200_popl.pdf}
\end{center}
\vspace{-2mm}
\caption{Overhead for the Fluid simulation with 19200 cells.\label{fig:fluid_overhead}}
\vspace{-6mm}
\end{figure}

Figures \ref{fig:ckt_overhead}, \ref{fig:fluid_overhead}, and \ref{fig:amr_overhead} show 
the total time spent by all CPUs and GPUs in each phase of the application.  The topmost
component of each bar shows the overhead of the dynamic checks.  In 
each figure the problem size stays the same as the number of processors are increased
(strong scaling).  Figure \ref{fig:amr_overhead} includes multiple problem sizes to show
how overhead is affected by changing problem size (weak scaling).  For cases where there
is an existing implementation to compare against we have included a dotted line indicating
baseline performance.  In a few cases (e.g. Figure \ref{fig:amr4096}) checking overhead is the difference
between better and worse performance than the baseline.  For an explanation of overall
performance relative to the baseline implementations please refer to \cite{Legion12}.

\begin{figure}
\begin{center}
\subfigure[4096 cells per dimension]
{
\includegraphics[scale=0.35]{figs/amr_4096_popl.pdf}
\label{fig:amr4096}
}
\subfigure[8192 cells per dimension]
{
\includegraphics[scale=0.35]{figs/amr_8192_popl.pdf}
\label{fig:amr8192}
}
\subfigure[16384 cells per dimension]
{
\includegraphics[scale=0.35]{figs/amr_16384_popl.pdf}
\label{fig:amr16384}
}
\end{center}
\vspace{-2mm}
\caption{Overhead for the AMR application for three different problem sizes.\label{fig:amr_overhead}}
\vspace{-6mm}
\end{figure}

In addition to total processor overhead, we also measured performance gain in terms of wall-clock
time.  Note that since most region accesses occur in leaf tasks they often parallelize
very well.  Wall-clock performance gains from eliding memory checks ranged from 1-10\%, 1-15\%, 
and 2-71\% for Circuit, Fluid, and AMR respectively.  Performance gains for AMR were larger than
the other applications because AMR was already memory bound and the additional checks intensified
memory pressure.  We also note that for GPU kernels, checking required up to 8 additional registers.  
While the GPU kernels in Circuit where not bound by 
available on-chip memory, kernels that are would be susceptible to extreme performance 
degradation due to the extra registers required for checking.  Finally, we measured the overhead
of the dynamic checks associated with unpack operations but found them to be negligible relative
to the overhead of region access checks.

\subsection{Scalability}
\label{subsec:scalability}

